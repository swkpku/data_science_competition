{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from dataset.dataset import get_cdiscount_dataset\n",
    "from model.model import assemble_model, assemble_model_with_classifier\n",
    "from trainer.trainer import get_trainer\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# redirect print to file\n",
    "# import sys\n",
    "# sys.stdout = open(\"PyTorch-resnet34-log.txt\", \"w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuration\n",
    "config = {\n",
    "    'train_batch_size': 124, 'val_batch_size': 124,\n",
    "    'arch': 'resnet50', 'pretrained': True,\n",
    "    'optimizer': 'Adam', 'learning_rate': 1e-4, 'decay_lr_freq': 4e4, 'weight_decay': 1e-5,\n",
    "    'resume': None,\n",
    "    'start_epoch': 0, 'epochs': 10,\n",
    "    'print_freq': 10, 'validate_freq': 7e4, 'save_freq': 1e3,\n",
    "    'best_val_prec1': 0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting dataset...\n",
      "getting data loader...\n",
      "=> using pre-trained model 'resnet50'\n",
      "DataParallel (\n",
      "  (module): AssembledModel (\n",
      "    (model): Sequential (\n",
      "      (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "      (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (2): ReLU (inplace)\n",
      "      (3): MaxPool2d (size=(3, 3), stride=(2, 2), padding=(1, 1), dilation=(1, 1))\n",
      "      (4): Sequential (\n",
      "        (0): Bottleneck (\n",
      "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu): ReLU (inplace)\n",
      "          (downsample): Sequential (\n",
      "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
      "          )\n",
      "        )\n",
      "        (1): Bottleneck (\n",
      "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu): ReLU (inplace)\n",
      "        )\n",
      "        (2): Bottleneck (\n",
      "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu): ReLU (inplace)\n",
      "        )\n",
      "      )\n",
      "      (5): Sequential (\n",
      "        (0): Bottleneck (\n",
      "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu): ReLU (inplace)\n",
      "          (downsample): Sequential (\n",
      "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
      "          )\n",
      "        )\n",
      "        (1): Bottleneck (\n",
      "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu): ReLU (inplace)\n",
      "        )\n",
      "        (2): Bottleneck (\n",
      "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu): ReLU (inplace)\n",
      "        )\n",
      "        (3): Bottleneck (\n",
      "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu): ReLU (inplace)\n",
      "        )\n",
      "      )\n",
      "      (6): Sequential (\n",
      "        (0): Bottleneck (\n",
      "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu): ReLU (inplace)\n",
      "          (downsample): Sequential (\n",
      "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)\n",
      "          )\n",
      "        )\n",
      "        (1): Bottleneck (\n",
      "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu): ReLU (inplace)\n",
      "        )\n",
      "        (2): Bottleneck (\n",
      "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu): ReLU (inplace)\n",
      "        )\n",
      "        (3): Bottleneck (\n",
      "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu): ReLU (inplace)\n",
      "        )\n",
      "        (4): Bottleneck (\n",
      "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu): ReLU (inplace)\n",
      "        )\n",
      "        (5): Bottleneck (\n",
      "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu): ReLU (inplace)\n",
      "        )\n",
      "      )\n",
      "      (7): Sequential (\n",
      "        (0): Bottleneck (\n",
      "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu): ReLU (inplace)\n",
      "          (downsample): Sequential (\n",
      "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "            (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True)\n",
      "          )\n",
      "        )\n",
      "        (1): Bottleneck (\n",
      "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu): ReLU (inplace)\n",
      "        )\n",
      "        (2): Bottleneck (\n",
      "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu): ReLU (inplace)\n",
      "        )\n",
      "      )\n",
      "      (8): AvgPool2d (size=7, stride=7, padding=0, ceil_mode=False, count_include_pad=True)\n",
      "    )\n",
      "    (classifier): Sequential (\n",
      "      (0): Linear (2048 -> 5270)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "start training\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0][0/79839]\tTime 26.422 (26.422)\tData 5.764 (5.764)\tLoss 8.5794 (8.5794)\tPrec@1 0.000 (0.000)\tPrec@5 0.000 (0.000)\n",
      "Epoch: [0][10/79839]\tTime 2.569 (4.814)\tData 0.011 (0.533)\tLoss 7.8500 (8.2494)\tPrec@1 5.645 (2.493)\tPrec@5 8.871 (5.059)\n",
      "Epoch: [0][20/79839]\tTime 2.579 (3.745)\tData 0.010 (0.284)\tLoss 7.1453 (7.8771)\tPrec@1 8.065 (4.800)\tPrec@5 11.290 (8.372)\n",
      "Epoch: [0][30/79839]\tTime 2.585 (3.370)\tData 0.008 (0.195)\tLoss 6.9028 (7.5931)\tPrec@1 4.839 (5.957)\tPrec@5 17.742 (10.458)\n",
      "Epoch: [0][40/79839]\tTime 2.586 (3.179)\tData 0.008 (0.150)\tLoss 6.8441 (7.3576)\tPrec@1 13.710 (7.199)\tPrec@5 21.774 (12.903)\n",
      "Epoch: [0][50/79839]\tTime 2.630 (3.066)\tData 0.015 (0.123)\tLoss 6.2689 (7.1704)\tPrec@1 11.290 (8.049)\tPrec@5 25.000 (14.801)\n",
      "Epoch: [0][60/79839]\tTime 2.590 (2.990)\tData 0.010 (0.104)\tLoss 5.9732 (7.0019)\tPrec@1 12.903 (9.122)\tPrec@5 29.839 (16.856)\n",
      "Epoch: [0][70/79839]\tTime 2.557 (2.935)\tData 0.010 (0.091)\tLoss 5.8693 (6.8872)\tPrec@1 18.548 (10.177)\tPrec@5 29.839 (18.458)\n",
      "Epoch: [0][80/79839]\tTime 2.600 (2.894)\tData 0.009 (0.081)\tLoss 6.3483 (6.7795)\tPrec@1 8.065 (11.002)\tPrec@5 18.548 (19.753)\n",
      "Epoch: [0][90/79839]\tTime 2.608 (2.862)\tData 0.009 (0.073)\tLoss 5.8894 (6.6955)\tPrec@1 24.194 (11.689)\tPrec@5 30.645 (20.782)\n",
      "Epoch: [0][100/79839]\tTime 2.616 (2.837)\tData 0.015 (0.067)\tLoss 5.9695 (6.6101)\tPrec@1 16.129 (12.352)\tPrec@5 28.226 (21.750)\n",
      "Epoch: [0][110/79839]\tTime 2.617 (2.816)\tData 0.017 (0.062)\tLoss 4.7911 (6.5173)\tPrec@1 30.645 (13.019)\tPrec@5 42.742 (22.828)\n",
      "Epoch: [0][120/79839]\tTime 2.609 (2.801)\tData 0.009 (0.058)\tLoss 5.9772 (6.4464)\tPrec@1 12.097 (13.410)\tPrec@5 25.000 (23.660)\n",
      "Epoch: [0][130/79839]\tTime 2.637 (2.786)\tData 0.008 (0.054)\tLoss 6.0048 (6.3843)\tPrec@1 17.742 (13.919)\tPrec@5 32.258 (24.520)\n",
      "Epoch: [0][140/79839]\tTime 2.597 (2.773)\tData 0.009 (0.051)\tLoss 5.7112 (6.3158)\tPrec@1 17.742 (14.396)\tPrec@5 33.871 (25.395)\n",
      "Epoch: [0][150/79839]\tTime 2.604 (2.763)\tData 0.008 (0.048)\tLoss 5.1184 (6.2576)\tPrec@1 29.032 (14.933)\tPrec@5 41.129 (26.132)\n",
      "Epoch: [0][160/79839]\tTime 2.636 (2.754)\tData 0.013 (0.046)\tLoss 5.8232 (6.2073)\tPrec@1 19.355 (15.308)\tPrec@5 30.645 (26.668)\n",
      "Epoch: [0][170/79839]\tTime 2.623 (2.745)\tData 0.010 (0.044)\tLoss 5.3667 (6.1509)\tPrec@1 17.742 (15.771)\tPrec@5 37.903 (27.339)\n",
      "Epoch: [0][180/79839]\tTime 2.599 (2.738)\tData 0.008 (0.042)\tLoss 5.3810 (6.1020)\tPrec@1 25.806 (16.182)\tPrec@5 36.290 (27.945)\n",
      "Epoch: [0][190/79839]\tTime 2.606 (2.731)\tData 0.017 (0.041)\tLoss 5.0710 (6.0556)\tPrec@1 22.581 (16.496)\tPrec@5 44.355 (28.475)\n",
      "Epoch: [0][200/79839]\tTime 2.638 (2.726)\tData 0.010 (0.039)\tLoss 5.1804 (6.0142)\tPrec@1 25.000 (16.919)\tPrec@5 37.903 (28.956)\n",
      "Epoch: [0][210/79839]\tTime 2.607 (2.721)\tData 0.009 (0.038)\tLoss 5.4341 (5.9800)\tPrec@1 21.774 (17.234)\tPrec@5 32.258 (29.307)\n",
      "Epoch: [0][220/79839]\tTime 2.668 (2.716)\tData 0.011 (0.036)\tLoss 4.7940 (5.9418)\tPrec@1 30.645 (17.556)\tPrec@5 42.742 (29.744)\n",
      "Epoch: [0][230/79839]\tTime 2.609 (2.712)\tData 0.010 (0.035)\tLoss 4.9391 (5.9036)\tPrec@1 25.000 (17.882)\tPrec@5 36.290 (30.184)\n",
      "Epoch: [0][240/79839]\tTime 2.615 (2.708)\tData 0.009 (0.034)\tLoss 4.3856 (5.8604)\tPrec@1 37.903 (18.287)\tPrec@5 50.806 (30.685)\n",
      "Epoch: [0][250/79839]\tTime 2.609 (2.704)\tData 0.008 (0.033)\tLoss 5.2010 (5.8278)\tPrec@1 25.806 (18.632)\tPrec@5 39.516 (31.114)\n",
      "Epoch: [0][260/79839]\tTime 2.626 (2.700)\tData 0.009 (0.032)\tLoss 4.8351 (5.7962)\tPrec@1 29.839 (18.876)\tPrec@5 41.935 (31.449)\n",
      "Epoch: [0][270/79839]\tTime 2.614 (2.697)\tData 0.011 (0.032)\tLoss 4.7310 (5.7665)\tPrec@1 28.226 (19.155)\tPrec@5 45.968 (31.785)\n",
      "Epoch: [0][280/79839]\tTime 2.612 (2.694)\tData 0.011 (0.031)\tLoss 5.1782 (5.7383)\tPrec@1 22.581 (19.389)\tPrec@5 36.290 (32.077)\n",
      "Epoch: [0][290/79839]\tTime 2.617 (2.691)\tData 0.010 (0.030)\tLoss 4.7869 (5.7025)\tPrec@1 29.839 (19.685)\tPrec@5 47.581 (32.527)\n",
      "Epoch: [0][300/79839]\tTime 2.589 (2.689)\tData 0.009 (0.029)\tLoss 4.3871 (5.6800)\tPrec@1 36.290 (19.907)\tPrec@5 50.000 (32.815)\n",
      "Epoch: [0][310/79839]\tTime 2.611 (2.686)\tData 0.011 (0.029)\tLoss 4.8622 (5.6544)\tPrec@1 29.839 (20.151)\tPrec@5 41.935 (33.103)\n",
      "Epoch: [0][320/79839]\tTime 2.617 (2.684)\tData 0.010 (0.028)\tLoss 4.8237 (5.6263)\tPrec@1 28.226 (20.370)\tPrec@5 46.774 (33.431)\n",
      "Epoch: [0][330/79839]\tTime 2.589 (2.681)\tData 0.010 (0.028)\tLoss 4.8039 (5.5963)\tPrec@1 28.226 (20.619)\tPrec@5 45.968 (33.798)\n",
      "Epoch: [0][340/79839]\tTime 2.601 (2.679)\tData 0.010 (0.027)\tLoss 4.7485 (5.5705)\tPrec@1 27.419 (20.842)\tPrec@5 37.903 (34.103)\n",
      "Epoch: [0][350/79839]\tTime 2.593 (2.678)\tData 0.010 (0.027)\tLoss 4.6917 (5.5429)\tPrec@1 27.419 (21.085)\tPrec@5 45.968 (34.427)\n",
      "Epoch: [0][360/79839]\tTime 2.602 (2.676)\tData 0.010 (0.026)\tLoss 4.8164 (5.5237)\tPrec@1 23.387 (21.216)\tPrec@5 41.129 (34.644)\n",
      "Epoch: [0][370/79839]\tTime 2.598 (2.674)\tData 0.011 (0.026)\tLoss 4.5657 (5.4940)\tPrec@1 28.226 (21.489)\tPrec@5 47.581 (35.014)\n",
      "Epoch: [0][380/79839]\tTime 2.600 (2.672)\tData 0.009 (0.026)\tLoss 4.6340 (5.4658)\tPrec@1 29.839 (21.768)\tPrec@5 45.161 (35.374)\n",
      "Epoch: [0][390/79839]\tTime 2.588 (2.670)\tData 0.012 (0.025)\tLoss 4.9439 (5.4461)\tPrec@1 23.387 (21.931)\tPrec@5 41.129 (35.603)\n",
      "Epoch: [0][400/79839]\tTime 2.607 (2.669)\tData 0.010 (0.025)\tLoss 4.0527 (5.4241)\tPrec@1 34.677 (22.124)\tPrec@5 50.806 (35.856)\n",
      "Epoch: [0][410/79839]\tTime 2.576 (2.667)\tData 0.010 (0.024)\tLoss 4.5429 (5.4000)\tPrec@1 26.613 (22.277)\tPrec@5 43.548 (36.108)\n",
      "Epoch: [0][420/79839]\tTime 2.614 (2.666)\tData 0.011 (0.024)\tLoss 4.4301 (5.3768)\tPrec@1 32.258 (22.504)\tPrec@5 48.387 (36.363)\n",
      "Epoch: [0][430/79839]\tTime 2.610 (2.665)\tData 0.012 (0.024)\tLoss 4.4546 (5.3540)\tPrec@1 31.452 (22.715)\tPrec@5 45.161 (36.625)\n",
      "Epoch: [0][440/79839]\tTime 2.619 (2.663)\tData 0.009 (0.024)\tLoss 4.3582 (5.3315)\tPrec@1 36.290 (22.932)\tPrec@5 48.387 (36.876)\n",
      "Epoch: [0][450/79839]\tTime 2.609 (2.662)\tData 0.011 (0.023)\tLoss 4.6977 (5.3094)\tPrec@1 28.226 (23.137)\tPrec@5 45.161 (37.134)\n",
      "Epoch: [0][460/79839]\tTime 2.633 (2.661)\tData 0.012 (0.023)\tLoss 4.5926 (5.2904)\tPrec@1 29.839 (23.317)\tPrec@5 47.581 (37.368)\n",
      "Epoch: [0][470/79839]\tTime 2.590 (2.660)\tData 0.009 (0.023)\tLoss 4.0061 (5.2719)\tPrec@1 33.065 (23.474)\tPrec@5 50.806 (37.571)\n",
      "Epoch: [0][480/79839]\tTime 2.623 (2.659)\tData 0.010 (0.022)\tLoss 4.4750 (5.2511)\tPrec@1 24.194 (23.662)\tPrec@5 42.742 (37.794)\n",
      "Epoch: [0][490/79839]\tTime 2.615 (2.658)\tData 0.009 (0.022)\tLoss 4.4312 (5.2366)\tPrec@1 33.065 (23.796)\tPrec@5 48.387 (37.964)\n",
      "Epoch: [0][500/79839]\tTime 2.613 (2.657)\tData 0.012 (0.022)\tLoss 4.0557 (5.2215)\tPrec@1 37.097 (23.968)\tPrec@5 51.613 (38.117)\n",
      "Epoch: [0][510/79839]\tTime 2.615 (2.656)\tData 0.009 (0.022)\tLoss 4.4697 (5.2054)\tPrec@1 31.452 (24.105)\tPrec@5 45.161 (38.323)\n",
      "Epoch: [0][520/79839]\tTime 2.601 (2.655)\tData 0.012 (0.022)\tLoss 4.4253 (5.1899)\tPrec@1 25.806 (24.190)\tPrec@5 46.774 (38.504)\n",
      "Epoch: [0][530/79839]\tTime 2.538 (2.654)\tData 0.009 (0.021)\tLoss 3.7323 (5.1714)\tPrec@1 34.677 (24.347)\tPrec@5 57.258 (38.719)\n",
      "Epoch: [0][540/79839]\tTime 2.610 (2.654)\tData 0.015 (0.021)\tLoss 4.0686 (5.1573)\tPrec@1 34.677 (24.478)\tPrec@5 50.000 (38.895)\n",
      "Epoch: [0][550/79839]\tTime 2.595 (2.653)\tData 0.009 (0.021)\tLoss 4.0572 (5.1385)\tPrec@1 35.484 (24.644)\tPrec@5 52.419 (39.131)\n",
      "Epoch: [0][560/79839]\tTime 2.611 (2.652)\tData 0.013 (0.021)\tLoss 3.9479 (5.1206)\tPrec@1 41.129 (24.803)\tPrec@5 58.871 (39.339)\n",
      "Epoch: [0][570/79839]\tTime 2.749 (2.651)\tData 0.014 (0.021)\tLoss 3.8657 (5.1054)\tPrec@1 39.516 (24.959)\tPrec@5 58.871 (39.522)\n",
      "Epoch: [0][580/79839]\tTime 2.611 (2.651)\tData 0.012 (0.020)\tLoss 4.3922 (5.0920)\tPrec@1 33.871 (25.074)\tPrec@5 48.387 (39.684)\n",
      "Epoch: [0][590/79839]\tTime 2.605 (2.650)\tData 0.011 (0.020)\tLoss 3.9618 (5.0743)\tPrec@1 34.677 (25.239)\tPrec@5 51.613 (39.867)\n",
      "Epoch: [0][600/79839]\tTime 2.603 (2.650)\tData 0.009 (0.020)\tLoss 3.9940 (5.0593)\tPrec@1 34.677 (25.370)\tPrec@5 50.000 (40.050)\n",
      "Epoch: [0][610/79839]\tTime 2.675 (2.649)\tData 0.011 (0.020)\tLoss 4.0554 (5.0426)\tPrec@1 37.097 (25.519)\tPrec@5 50.000 (40.245)\n",
      "Epoch: [0][620/79839]\tTime 2.579 (2.648)\tData 0.009 (0.020)\tLoss 3.6589 (5.0257)\tPrec@1 36.290 (25.660)\tPrec@5 56.452 (40.430)\n",
      "Epoch: [0][630/79839]\tTime 2.619 (2.648)\tData 0.011 (0.020)\tLoss 3.8666 (5.0120)\tPrec@1 37.903 (25.786)\tPrec@5 54.032 (40.585)\n",
      "Epoch: [0][640/79839]\tTime 2.605 (2.647)\tData 0.012 (0.020)\tLoss 3.8572 (4.9986)\tPrec@1 32.258 (25.886)\tPrec@5 51.613 (40.711)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0][650/79839]\tTime 2.629 (2.647)\tData 0.010 (0.019)\tLoss 3.5960 (4.9838)\tPrec@1 40.323 (26.023)\tPrec@5 56.452 (40.891)\n",
      "Epoch: [0][660/79839]\tTime 2.574 (2.646)\tData 0.011 (0.019)\tLoss 3.8096 (4.9691)\tPrec@1 35.484 (26.124)\tPrec@5 52.419 (41.031)\n",
      "Epoch: [0][670/79839]\tTime 2.614 (2.646)\tData 0.016 (0.019)\tLoss 4.5350 (4.9559)\tPrec@1 25.806 (26.238)\tPrec@5 35.484 (41.158)\n",
      "Epoch: [0][680/79839]\tTime 2.631 (2.645)\tData 0.011 (0.019)\tLoss 3.8440 (4.9428)\tPrec@1 35.484 (26.361)\tPrec@5 48.387 (41.292)\n",
      "Epoch: [0][690/79839]\tTime 2.618 (2.645)\tData 0.013 (0.019)\tLoss 3.5308 (4.9294)\tPrec@1 38.710 (26.487)\tPrec@5 58.065 (41.451)\n",
      "Epoch: [0][700/79839]\tTime 2.602 (2.644)\tData 0.010 (0.019)\tLoss 4.2384 (4.9182)\tPrec@1 34.677 (26.590)\tPrec@5 45.968 (41.582)\n",
      "Epoch: [0][710/79839]\tTime 2.615 (2.644)\tData 0.012 (0.019)\tLoss 3.8509 (4.9062)\tPrec@1 33.871 (26.690)\tPrec@5 50.806 (41.731)\n",
      "Epoch: [0][720/79839]\tTime 2.618 (2.644)\tData 0.014 (0.019)\tLoss 4.2388 (4.8951)\tPrec@1 31.452 (26.785)\tPrec@5 46.774 (41.855)\n",
      "Epoch: [0][730/79839]\tTime 2.624 (2.643)\tData 0.011 (0.019)\tLoss 4.1119 (4.8815)\tPrec@1 31.452 (26.902)\tPrec@5 51.613 (42.040)\n",
      "Epoch: [0][740/79839]\tTime 2.609 (2.643)\tData 0.014 (0.019)\tLoss 4.4011 (4.8697)\tPrec@1 28.226 (27.012)\tPrec@5 51.613 (42.179)\n",
      "Epoch: [0][750/79839]\tTime 2.634 (2.642)\tData 0.011 (0.018)\tLoss 4.4344 (4.8592)\tPrec@1 30.645 (27.125)\tPrec@5 45.968 (42.288)\n",
      "Epoch: [0][760/79839]\tTime 2.603 (2.642)\tData 0.011 (0.018)\tLoss 3.4498 (4.8489)\tPrec@1 43.548 (27.204)\tPrec@5 62.097 (42.417)\n",
      "Epoch: [0][770/79839]\tTime 2.618 (2.641)\tData 0.012 (0.018)\tLoss 3.9274 (4.8372)\tPrec@1 29.839 (27.270)\tPrec@5 53.226 (42.539)\n",
      "Epoch: [0][780/79839]\tTime 2.611 (2.641)\tData 0.012 (0.018)\tLoss 4.4491 (4.8270)\tPrec@1 25.806 (27.354)\tPrec@5 38.710 (42.655)\n",
      "Epoch: [0][790/79839]\tTime 2.614 (2.641)\tData 0.012 (0.018)\tLoss 3.8100 (4.8158)\tPrec@1 35.484 (27.469)\tPrec@5 53.226 (42.802)\n",
      "Epoch: [0][800/79839]\tTime 2.612 (2.641)\tData 0.013 (0.018)\tLoss 3.4225 (4.8049)\tPrec@1 38.710 (27.564)\tPrec@5 59.677 (42.938)\n",
      "Epoch: [0][810/79839]\tTime 2.663 (2.640)\tData 0.011 (0.018)\tLoss 3.3589 (4.7923)\tPrec@1 49.194 (27.694)\tPrec@5 60.484 (43.071)\n",
      "Epoch: [0][820/79839]\tTime 2.614 (2.640)\tData 0.009 (0.018)\tLoss 3.9235 (4.7821)\tPrec@1 35.484 (27.778)\tPrec@5 53.226 (43.188)\n",
      "Epoch: [0][830/79839]\tTime 2.610 (2.640)\tData 0.011 (0.018)\tLoss 4.1171 (4.7723)\tPrec@1 29.839 (27.862)\tPrec@5 47.581 (43.295)\n",
      "Epoch: [0][840/79839]\tTime 2.602 (2.639)\tData 0.009 (0.018)\tLoss 4.2421 (4.7617)\tPrec@1 29.032 (27.962)\tPrec@5 52.419 (43.413)\n",
      "Epoch: [0][850/79839]\tTime 2.601 (2.639)\tData 0.012 (0.018)\tLoss 4.0987 (4.7517)\tPrec@1 33.871 (28.029)\tPrec@5 50.806 (43.528)\n",
      "Epoch: [0][860/79839]\tTime 2.614 (2.639)\tData 0.009 (0.018)\tLoss 4.3215 (4.7402)\tPrec@1 33.065 (28.122)\tPrec@5 48.387 (43.664)\n",
      "Epoch: [0][870/79839]\tTime 2.615 (2.638)\tData 0.012 (0.017)\tLoss 3.6929 (4.7298)\tPrec@1 37.097 (28.210)\tPrec@5 51.613 (43.783)\n",
      "Epoch: [0][880/79839]\tTime 2.600 (2.638)\tData 0.011 (0.017)\tLoss 4.2483 (4.7198)\tPrec@1 33.065 (28.300)\tPrec@5 50.806 (43.904)\n",
      "Epoch: [0][890/79839]\tTime 2.595 (2.637)\tData 0.009 (0.017)\tLoss 4.3043 (4.7090)\tPrec@1 30.645 (28.398)\tPrec@5 45.161 (44.029)\n",
      "Epoch: [0][900/79839]\tTime 2.607 (2.637)\tData 0.011 (0.017)\tLoss 3.6365 (4.6997)\tPrec@1 39.516 (28.487)\tPrec@5 60.484 (44.142)\n",
      "Epoch: [0][910/79839]\tTime 2.607 (2.637)\tData 0.011 (0.017)\tLoss 3.5393 (4.6887)\tPrec@1 41.935 (28.604)\tPrec@5 57.258 (44.266)\n",
      "Epoch: [0][920/79839]\tTime 2.589 (2.637)\tData 0.009 (0.017)\tLoss 3.7766 (4.6799)\tPrec@1 34.677 (28.694)\tPrec@5 54.032 (44.370)\n",
      "Epoch: [0][930/79839]\tTime 2.605 (2.636)\tData 0.011 (0.017)\tLoss 3.4279 (4.6716)\tPrec@1 41.935 (28.759)\tPrec@5 60.484 (44.465)\n",
      "Epoch: [0][940/79839]\tTime 2.605 (2.636)\tData 0.014 (0.017)\tLoss 3.8781 (4.6614)\tPrec@1 33.871 (28.845)\tPrec@5 57.258 (44.579)\n",
      "Epoch: [0][950/79839]\tTime 2.629 (2.636)\tData 0.014 (0.017)\tLoss 3.7892 (4.6523)\tPrec@1 26.613 (28.925)\tPrec@5 54.032 (44.681)\n",
      "Epoch: [0][960/79839]\tTime 2.609 (2.636)\tData 0.011 (0.017)\tLoss 3.5043 (4.6431)\tPrec@1 37.097 (29.005)\tPrec@5 58.871 (44.788)\n",
      "Epoch: [0][970/79839]\tTime 2.617 (2.635)\tData 0.013 (0.017)\tLoss 4.1637 (4.6336)\tPrec@1 32.258 (29.088)\tPrec@5 50.000 (44.905)\n",
      "Epoch: [0][980/79839]\tTime 2.601 (2.635)\tData 0.010 (0.017)\tLoss 3.9288 (4.6264)\tPrec@1 40.323 (29.157)\tPrec@5 57.258 (44.980)\n",
      "Epoch: [0][990/79839]\tTime 2.613 (2.635)\tData 0.012 (0.017)\tLoss 4.2008 (4.6187)\tPrec@1 31.452 (29.215)\tPrec@5 50.806 (45.068)\n"
     ]
    }
   ],
   "source": [
    "import torchvision.models as models\n",
    "\n",
    "# get dataset\n",
    "print('getting dataset...')\n",
    "train_dataset = get_cdiscount_dataset(offsets_csv=\"train_offsets.csv\",\n",
    "                                      images_csv=\"train_images.csv\",\n",
    "                                      bson_file_path=\"/mnt/data/cdiscount/train.bson\",\n",
    "                                      with_label=True,\n",
    "                                      resize=224)\n",
    "val_dataset = get_cdiscount_dataset(offsets_csv=\"train_offsets.csv\",\n",
    "                                    images_csv=\"val_images.csv\",\n",
    "                                    bson_file_path=\"/mnt/data/cdiscount/train.bson\",\n",
    "                                    with_label=True,\n",
    "                                    resize=224)\n",
    "\n",
    "# get data loader\n",
    "print('getting data loader...')\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=config['train_batch_size'], shuffle=True, num_workers=6)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=config['val_batch_size'], shuffle=True, num_workers=6)\n",
    "\n",
    "# define model\n",
    "print(\"=> using pre-trained model '{}'\".format(config['arch']))\n",
    "model = models.__dict__[config['arch']](pretrained=config['pretrained'])\n",
    "\n",
    "# model = torch.nn.Sequential(*list(model.children())[:-1])\n",
    "# model.add_module('classifier', torch.nn.Linear(in_features=2048, out_features=5270))\n",
    "# model = torch.nn.DataParallel(model).cuda()\n",
    "# print(model)\n",
    "\n",
    "classifier_layer = [\n",
    "    torch.nn.Linear(in_features=2048, out_features=5270)\n",
    "]\n",
    "\n",
    "# classifier_layer = [\n",
    "#     torch.nn.Linear(in_features=2048, out_features=5270),\n",
    "# ]\n",
    "\n",
    "classifier = torch.nn.Sequential(*classifier_layer)\n",
    "\n",
    "model = assemble_model_with_classifier(model, -1, classifier)\n",
    "model = torch.nn.DataParallel(model).cuda()\n",
    "print(model)\n",
    "\n",
    "# define loss function (criterion) and optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss().cuda()\n",
    "\n",
    "# get trainer\n",
    "Trainer = get_trainer(train_dataloader, val_dataloader, model, criterion, config)\n",
    "\n",
    "# Run!\n",
    "Trainer.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
