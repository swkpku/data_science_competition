{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import io\n",
    "import bson\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm_notebook\n",
    "import concurrent.futures\n",
    "from multiprocessing import cpu_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_images = 1000000\n",
    "im_size = 16\n",
    "num_cpus = cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Widget Javascript not detected.  It may not be installed or enabled properly.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5de8f0e833054d89a82a2a46c06d01ac"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def imread(buf):\n",
    "    return cv2.imdecode(np.frombuffer(buf, np.uint8), cv2.IMREAD_ANYCOLOR)\n",
    "\n",
    "def img2feat(im):\n",
    "    x = cv2.resize(im, (im_size, im_size), interpolation=cv2.INTER_AREA)\n",
    "    return np.float32(x) / 255\n",
    "\n",
    "images = np.empty((num_images, im_size, im_size, 3), dtype=np.float32)\n",
    "labels = []\n",
    "\n",
    "def load_image(pic, target, bar):\n",
    "    picture = imread(pic)\n",
    "    x = img2feat(picture)\n",
    "    bar.update()\n",
    "    \n",
    "    return x, target\n",
    "\n",
    "bar = tqdm_notebook(total=num_images)\n",
    "with open('/mnt/data/cdiscount/train.bson', 'rb') as f, \\\n",
    "        concurrent.futures.ThreadPoolExecutor(num_cpus) as executor:\n",
    "\n",
    "    data = bson.decode_file_iter(f)\n",
    "    delayed_load = []\n",
    "\n",
    "    i = 0\n",
    "    try:\n",
    "        for c, d in enumerate(data):\n",
    "            target = d['category_id']\n",
    "            for e, pic in enumerate(d['imgs']):\n",
    "                delayed_load.append(executor.submit(load_image, pic['picture'], target, bar))\n",
    "                \n",
    "                i = i + 1\n",
    "\n",
    "                if i >= num_images:\n",
    "                    raise IndexError()\n",
    "\n",
    "    except IndexError:\n",
    "        pass;\n",
    "    \n",
    "    for i, future in enumerate(concurrent.futures.as_completed(delayed_load)):\n",
    "        x, target = future.result()\n",
    "        \n",
    "        images[i] = x\n",
    "        labels.append(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((1000000, 16, 16, 3), 1000000)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images.shape, len(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.885605\n"
     ]
    }
   ],
   "source": [
    "labels = pd.Series(labels)\n",
    "\n",
    "num_classes = 1000 \n",
    "valid_targets = set(labels.value_counts().index[:num_classes-1].tolist())\n",
    "valid_labels = labels.isin(valid_targets)\n",
    "\n",
    "labels[~valid_labels] = -1\n",
    "\n",
    "max_acc = valid_labels.mean()\n",
    "print(max_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels, rev_labels = pd.factorize(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    }
   ],
   "source": [
    "labels.shape\n",
    "print(labels[600])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train image shape:  (900000, 16, 16, 3)\n",
      "train label shape:  (900000,)\n",
      "val image shape:  (100000, 16, 16, 3)\n",
      "val label shape:  (100000,)\n"
     ]
    }
   ],
   "source": [
    "num_train = 900000\n",
    "num_val   = 100000\n",
    "\n",
    "indicies = np.arange(num_images)\n",
    "np.random.shuffle(indicies)\n",
    "train_mask = indicies[range(num_train)]\n",
    "val_mask = indicies[range(num_train, num_train + num_val)]\n",
    "\n",
    "train_image = images[train_mask]\n",
    "train_label = labels[train_mask]\n",
    "val_image = images[val_mask]\n",
    "val_label = labels[val_mask]\n",
    "\n",
    "print('train image shape: ', train_image.shape)\n",
    "print('train label shape: ', train_label.shape)\n",
    "print('val image shape: ', val_image.shape)\n",
    "print('val label shape: ', val_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training with batch_size: 128, learning_rate: 0.000050\n",
      "epoch 0:\n",
      "iter: 0, loss: 7.553686, acc: 0.000000\n",
      "1000000/|/100%|| 1000000/1000000 [03:48<00:00, 5356.13it/s]iter: 1000, loss: 4.855631, acc: 0.195312\n",
      "iter: 2000, loss: 3.744300, acc: 0.351562\n",
      "iter: 3000, loss: 3.664635, acc: 0.320312\n",
      "iter: 4000, loss: 3.505937, acc: 0.406250\n",
      "iter: 5000, loss: 3.295086, acc: 0.421875\n",
      "iter: 6000, loss: 3.453911, acc: 0.382812\n",
      "iter: 7000, loss: 3.770164, acc: 0.375000\n",
      "val set acc: 0.392600\n",
      "val set acc: 0.387400\n",
      "val set acc: 0.391000\n",
      "val set acc: 0.386200\n",
      "val set acc: 0.390800\n",
      "val set acc: 0.398000\n",
      "val set acc: 0.391300\n",
      "val set acc: 0.389700\n",
      "val set acc: 0.386000\n",
      "val set acc: 0.391500\n",
      "epoch 1:\n",
      "iter: 0, loss: 2.773903, acc: 0.484375\n",
      "iter: 1000, loss: 3.831333, acc: 0.328125\n",
      "iter: 2000, loss: 2.891819, acc: 0.445312\n",
      "iter: 3000, loss: 3.022703, acc: 0.390625\n",
      "iter: 4000, loss: 2.954076, acc: 0.453125\n",
      "iter: 5000, loss: 2.845863, acc: 0.468750\n",
      "iter: 6000, loss: 3.021114, acc: 0.414062\n",
      "iter: 7000, loss: 3.373402, acc: 0.406250\n",
      "val set acc: 0.430500\n",
      "val set acc: 0.426900\n",
      "val set acc: 0.430200\n",
      "val set acc: 0.424700\n",
      "val set acc: 0.427900\n",
      "val set acc: 0.437200\n",
      "val set acc: 0.431200\n",
      "val set acc: 0.428600\n",
      "val set acc: 0.425400\n",
      "val set acc: 0.428800\n",
      "epoch 2:\n",
      "iter: 0, loss: 2.384086, acc: 0.578125\n",
      "iter: 1000, loss: 3.500801, acc: 0.343750\n",
      "iter: 2000, loss: 2.584972, acc: 0.492188\n",
      "iter: 3000, loss: 2.721951, acc: 0.453125\n",
      "iter: 4000, loss: 2.636394, acc: 0.500000\n",
      "iter: 5000, loss: 2.619953, acc: 0.484375\n",
      "iter: 6000, loss: 2.773092, acc: 0.414062\n",
      "iter: 7000, loss: 3.089044, acc: 0.437500\n",
      "val set acc: 0.446500\n",
      "val set acc: 0.443200\n",
      "val set acc: 0.446400\n",
      "val set acc: 0.441400\n",
      "val set acc: 0.448200\n",
      "val set acc: 0.452900\n",
      "val set acc: 0.454000\n",
      "val set acc: 0.445600\n",
      "val set acc: 0.443000\n",
      "val set acc: 0.444900\n",
      "epoch 3:\n",
      "iter: 0, loss: 2.217016, acc: 0.609375\n",
      "iter: 1000, loss: 3.268076, acc: 0.390625\n",
      "iter: 2000, loss: 2.367179, acc: 0.531250\n",
      "iter: 3000, loss: 2.532729, acc: 0.468750\n",
      "iter: 4000, loss: 2.442589, acc: 0.515625\n",
      "iter: 5000, loss: 2.434254, acc: 0.507812\n",
      "iter: 6000, loss: 2.580756, acc: 0.445312\n",
      "iter: 7000, loss: 2.843856, acc: 0.476562\n",
      "val set acc: 0.456700\n",
      "val set acc: 0.456900\n",
      "val set acc: 0.460900\n",
      "val set acc: 0.453300\n",
      "val set acc: 0.460000\n",
      "val set acc: 0.464100\n",
      "val set acc: 0.464100\n",
      "val set acc: 0.456300\n",
      "val set acc: 0.455000\n",
      "val set acc: 0.459200\n",
      "epoch 4:\n",
      "iter: 0, loss: 2.084514, acc: 0.601562\n",
      "iter: 1000, loss: 3.127478, acc: 0.414062\n",
      "iter: 2000, loss: 2.233165, acc: 0.562500\n",
      "iter: 3000, loss: 2.369620, acc: 0.500000\n",
      "iter: 4000, loss: 2.287315, acc: 0.562500\n",
      "iter: 5000, loss: 2.323874, acc: 0.500000\n",
      "iter: 6000, loss: 2.418337, acc: 0.484375\n",
      "iter: 7000, loss: 2.652281, acc: 0.492188\n",
      "val set acc: 0.463700\n",
      "val set acc: 0.463400\n",
      "val set acc: 0.466400\n",
      "val set acc: 0.460500\n",
      "val set acc: 0.465900\n",
      "val set acc: 0.469400\n",
      "val set acc: 0.470200\n",
      "val set acc: 0.463300\n",
      "val set acc: 0.464200\n",
      "val set acc: 0.464300\n",
      "epoch 5:\n",
      "iter: 0, loss: 1.974752, acc: 0.617188\n",
      "iter: 1000, loss: 2.944946, acc: 0.437500\n",
      "iter: 2000, loss: 2.126412, acc: 0.570312\n",
      "iter: 3000, loss: 2.215149, acc: 0.531250\n",
      "iter: 4000, loss: 2.154969, acc: 0.562500\n",
      "iter: 5000, loss: 2.216608, acc: 0.546875\n",
      "iter: 6000, loss: 2.273808, acc: 0.507812\n",
      "iter: 7000, loss: 2.520317, acc: 0.492188\n",
      "val set acc: 0.466600\n",
      "val set acc: 0.467900\n",
      "val set acc: 0.469400\n",
      "val set acc: 0.467900\n",
      "val set acc: 0.473200\n",
      "val set acc: 0.471300\n",
      "val set acc: 0.470700\n",
      "val set acc: 0.470000\n",
      "val set acc: 0.469600\n",
      "val set acc: 0.466700\n",
      "epoch 6:\n",
      "iter: 0, loss: 1.910045, acc: 0.617188\n",
      "iter: 1000, loss: 2.817742, acc: 0.453125\n",
      "iter: 2000, loss: 2.038685, acc: 0.570312\n",
      "iter: 3000, loss: 2.074308, acc: 0.546875\n",
      "iter: 4000, loss: 2.049857, acc: 0.570312\n",
      "iter: 5000, loss: 2.117556, acc: 0.554688\n",
      "iter: 6000, loss: 2.154281, acc: 0.515625\n",
      "iter: 7000, loss: 2.401878, acc: 0.523438\n",
      "val set acc: 0.468800\n",
      "val set acc: 0.468800\n",
      "val set acc: 0.472600\n",
      "val set acc: 0.470000\n",
      "val set acc: 0.477700\n",
      "val set acc: 0.475500\n",
      "val set acc: 0.474800\n",
      "val set acc: 0.475100\n",
      "val set acc: 0.473400\n",
      "val set acc: 0.471300\n",
      "epoch 7:\n",
      "iter: 0, loss: 1.830251, acc: 0.617188\n",
      "iter: 1000, loss: 2.644471, acc: 0.437500\n",
      "iter: 2000, loss: 1.978132, acc: 0.578125\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "num_epochs = 10\n",
    "batch_size = 128\n",
    "val_batch_size = 10000\n",
    "learning_rate = 1e-4\n",
    "\n",
    "num_test = 60000#test_images.shape[0]\n",
    "\n",
    "def model(x):\n",
    "    x = tf.reshape(x, [-1, 16, 16, 3])\n",
    "    x = tf.layers.conv2d(x, filters=32, kernel_size=[3, 3], strides=[1, 1], padding='same', activation=tf.nn.relu)\n",
    "    x = tf.layers.batch_normalization(x, training=True)\n",
    "    x = tf.layers.conv2d(x, filters=32, kernel_size=[3, 3], strides=[1, 1], padding='same', activation=tf.nn.relu)\n",
    "    x = tf.layers.batch_normalization(x, training=True)\n",
    "    x = tf.layers.conv2d(x, filters=32, kernel_size=[3, 3], strides=[1, 1], padding='same', activation=tf.nn.relu)\n",
    "    x = tf.layers.batch_normalization(x, training=True)\n",
    "    \n",
    "    x = tf.layers.conv2d(x, filters=64, kernel_size=[3, 3], strides=[2, 2], padding='same', activation=tf.nn.relu)\n",
    "    x = tf.layers.batch_normalization(x, training=True)    \n",
    "    \n",
    "    x = tf.layers.conv2d(x, filters=64, kernel_size=[3, 3], strides=[1, 1], padding='same', activation=tf.nn.relu)\n",
    "    x = tf.layers.batch_normalization(x, training=True)\n",
    "    x = tf.layers.conv2d(x, filters=64, kernel_size=[3, 3], strides=[1, 1], padding='same', activation=tf.nn.relu)\n",
    "    x = tf.layers.batch_normalization(x, training=True)\n",
    "    \n",
    "    x = tf.layers.conv2d(x, filters=128, kernel_size=[3, 3], strides=[2, 2], padding='same', activation=tf.nn.relu)\n",
    "    x = tf.layers.batch_normalization(x, training=True)\n",
    "    \n",
    "    x = tf.layers.conv2d(x, filters=128, kernel_size=[3, 3], strides=[1, 1], padding='same', activation=tf.nn.relu)\n",
    "    x = tf.layers.batch_normalization(x, training=True)\n",
    "    x = tf.layers.conv2d(x, filters=128, kernel_size=[3, 3], strides=[1, 1], padding='same', activation=tf.nn.relu)\n",
    "    x = tf.layers.batch_normalization(x, training=True) \n",
    "\n",
    "    x = tf.contrib.layers.flatten(x)\n",
    "    logits = tf.layers.dense(x, num_classes)\n",
    "    return logits\n",
    "\n",
    "tf.reset_default_graph()\n",
    "X = tf.placeholder(tf.float32, [None, 16, 16, 3])\n",
    "y = tf.placeholder(tf.int64, [None])\n",
    "\n",
    "logits = model(X)\n",
    "\n",
    "loss = tf.reduce_mean(tf.losses.softmax_cross_entropy(tf.one_hot(y, num_classes), logits=logits))\n",
    "\n",
    "predictions = tf.equal(tf.argmax(logits, 1), y)\n",
    "acc = tf.reduce_mean(tf.cast(predictions, tf.float32))\n",
    "\n",
    "train_indicies = np.arange(num_train)\n",
    "np.random.shuffle(train_indicies)\n",
    "\n",
    "val_indicies = np.arange(num_val)\n",
    "test_indicies = np.arange(num_test)\n",
    "\n",
    "test_predictions = tf.argmax(logits, 1)\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "def run_model(sess, train_step, batch_size, learning_rate, show_every):\n",
    "    print('training with batch_size: %d, learning_rate: %f'%(batch_size, learning_rate))\n",
    "    max_val_acc = .0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print('epoch %d:' % epoch)\n",
    "        for iter_i in range(num_train//batch_size):  \n",
    "            start_idx = (iter_i*batch_size)%num_train\n",
    "            idx = train_indicies[start_idx:start_idx+batch_size]\n",
    "            \n",
    "            _loss, _acc, _ = sess.run([loss, acc, train_step], feed_dict={\n",
    "                X: train_image[idx, :], y: train_label[idx]\n",
    "            })\n",
    "            if iter_i % show_every == 0:\n",
    "                print('iter: %d, loss: %f, acc: %f' % (iter_i, _loss, _acc))\n",
    "\n",
    "        #total_correct = []\n",
    "        for iter_i in range(num_val//val_batch_size):\n",
    "            start_idx = (iter_i * val_batch_size) % num_val\n",
    "            idx = val_indicies[start_idx:start_idx+val_batch_size]\n",
    "            val_acc = sess.run(acc, feed_dict={\n",
    "                X:val_image[idx, :], y: val_label[idx]\n",
    "            })\n",
    "            print('val set acc: %f'% val_acc)\n",
    "            #if (val_acc > max_val_acc):\n",
    "            #    max_val_acc = val_acc\n",
    "            #total_correct = total_correct + val_correct_predictions\n",
    "            \n",
    "#         if (val_acc >= 0.995):\n",
    "#             print('bingo!')\n",
    "#             total_test_predictions = []\n",
    "#             test_batch_size = 7000\n",
    "#             for test_iter in range(num_test//test_batch_size):\n",
    "#                 start_test_idx = (test_iter * test_batch_size)%num_test\n",
    "#                 test_idx = test_indicies[start_test_idx:start_test_idx+test_batch_size]\n",
    "#                 _test_predictions = sess.run(test_predictions, feed_dict={X:test_images[test_idx, :]})\n",
    "#                 total_test_predictions = np.concatenate((total_test_predictions,_test_predictions))\n",
    "            \n",
    "#             result = total_test_predictions\n",
    "#             with open('submission_%d.csv'%(epoch), 'w', newline='') as csvfile:\n",
    "#                 datawriter = csv.writer(csvfile, delimiter=',')\n",
    "#                 datawriter.writerow(['ImageId', 'Label'])\n",
    "#                 for i, predict_label in enumerate(result):\n",
    "#                     datawriter.writerow([i+1, predict_label.astype(np.uint8)])\n",
    "        \n",
    "#         save_path = \"../ckpt/epoch%d/model.ckpt\"%epoch\n",
    "#         saver.save(sess, save_path)\n",
    "    return max_val_acc\n",
    "#     print('predicting:')\n",
    "#     total_test_predictions = []\n",
    "#     test_batch_size = 7000\n",
    "#     for test_iter in range(num_test//test_batch_size):\n",
    "#         start_test_idx = (test_iter * test_batch_size)%num_test\n",
    "#         test_idx = test_indicies[start_test_idx:start_test_idx+test_batch_size]\n",
    "#         _test_predictions = sess.run(test_predictions, feed_dict={X:test_images[test_idx, :]})\n",
    "#         total_test_predictions = np.concatenate((total_test_predictions,_test_predictions))\n",
    "#     return total_test_predictions\n",
    "\n",
    "def test(sess):\n",
    "    submission = pd.read_csv('/mnt/data/cdiscount/sample_submission.csv', index_col='_id')\n",
    "    most_frequent_guess =1000018296\n",
    "    submission['category_id'] = most_frequent_guess \n",
    "\n",
    "    num_images_test = 1768182\n",
    "    with open('/mnt/data/cdiscount/test.bson', 'rb') as f, \\\n",
    "             concurrent.futures.ThreadPoolExecutor(num_cpus) as executor:\n",
    "\n",
    "        data = bson.decode_file_iter(f)\n",
    "        future_load = []\n",
    "\n",
    "        for i,d in enumerate(data):\n",
    "            if i >= num_images_test:\n",
    "                  break\n",
    "            future_load.append(executor.submit(load_image, d['imgs'][0]['picture'], d['_id'], bar))\n",
    "        \n",
    "            #print(\"Starting future processing\")\n",
    "        for future in concurrent.futures.as_completed(future_load):\n",
    "            x, _id = future.result()\n",
    "            x = np.reshape(x, [-1, 16, 16, 3])\n",
    "            _test_predictions = sess.run(test_predictions, feed_dict={X:x})\n",
    "            y_cat = rev_labels[np.argmax(_test_predictions)]\n",
    "            if y_cat == -1:\n",
    "                y_cat = most_frequent_guess\n",
    "\n",
    "            bar.update()\n",
    "            submission.loc[_id, 'category_id'] = y_cat\n",
    "    submission.to_csv('new_submission.csv.gz', compression='gzip')\n",
    "\n",
    "batch_size_arr = [128]\n",
    "learning_rate_arr = [5e-5]\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    for batch_size in batch_size_arr:\n",
    "        for learning_rate in learning_rate_arr:\n",
    "            train_step = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "            \n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            val_acc = run_model(sess, train_step, batch_size, learning_rate, show_every=1000)\n",
    "            #print('best_val_acc: %f'%val_acc)\n",
    "    del images\n",
    "    test(sess)\n",
    "print('done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
