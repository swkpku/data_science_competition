{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from torch.utils.data import DataLoader\n",
    "from dataset.dataset import get_cdiscount_dataset\n",
    "from model.model import assemble_model, assemble_model_with_classifier\n",
    "from predictor.predictor import get_predictor\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'test_batch_size': 128,\n",
    "    'arch': 'vgg19_bn',\n",
    "    'checkpoint': 'vgg19_bn_checkpoint_epoch_0_iter_63000.pth.tar',\n",
    "    'print_freq': 10,\n",
    "    'pred_filename': \"vgg19_bn-Epoch_0-iter-63000.csv\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting dataset...\n",
      "getting data loader...\n",
      "=> using pre-trained model 'vgg19_bn'\n",
      "DataParallel (\n",
      "  (module): AssembledModel (\n",
      "    (model): Sequential (\n",
      "      (0): Sequential (\n",
      "        (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
      "        (2): ReLU (inplace)\n",
      "        (3): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (4): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True)\n",
      "        (5): ReLU (inplace)\n",
      "        (6): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
      "        (7): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (8): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "        (9): ReLU (inplace)\n",
      "        (10): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (11): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True)\n",
      "        (12): ReLU (inplace)\n",
      "        (13): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
      "        (14): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (15): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
      "        (16): ReLU (inplace)\n",
      "        (17): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (18): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
      "        (19): ReLU (inplace)\n",
      "        (20): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (21): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
      "        (22): ReLU (inplace)\n",
      "        (23): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (24): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True)\n",
      "        (25): ReLU (inplace)\n",
      "        (26): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
      "        (27): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (28): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
      "        (29): ReLU (inplace)\n",
      "        (30): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (31): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
      "        (32): ReLU (inplace)\n",
      "        (33): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (34): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
      "        (35): ReLU (inplace)\n",
      "        (36): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (37): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
      "        (38): ReLU (inplace)\n",
      "        (39): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
      "        (40): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (41): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
      "        (42): ReLU (inplace)\n",
      "        (43): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (44): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
      "        (45): ReLU (inplace)\n",
      "        (46): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (47): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
      "        (48): ReLU (inplace)\n",
      "        (49): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "        (50): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True)\n",
      "        (51): ReLU (inplace)\n",
      "        (52): MaxPool2d (size=(2, 2), stride=(2, 2), dilation=(1, 1))\n",
      "      )\n",
      "    )\n",
      "    (classifier): Sequential (\n",
      "      (0): Linear (12800 -> 5270)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "=> loading checkpoint 'vgg19_bn_checkpoint_epoch_0_iter_63000.pth.tar'\n",
      "=> loaded checkpoint:\n",
      "Epoch: [0][63000]\tLoss 1.7659 (1.6365)\tPrec@1 63.281 (65.933)\tPrec@5 78.125 (82.735)\n",
      "start prediction\n",
      "Iter: [0/24181]\tTime 16.764\tData 1.972\t\n",
      "Iter: [10/24181]\tTime 0.733\tData 0.007\t\n",
      "Iter: [20/24181]\tTime 0.733\tData 0.005\t\n",
      "Iter: [30/24181]\tTime 0.736\tData 0.006\t\n",
      "Iter: [40/24181]\tTime 0.754\tData 0.009\t\n",
      "Iter: [50/24181]\tTime 0.733\tData 0.005\t\n",
      "Iter: [60/24181]\tTime 0.737\tData 0.005\t\n",
      "Iter: [70/24181]\tTime 0.751\tData 0.006\t\n",
      "Iter: [80/24181]\tTime 0.755\tData 0.017\t\n",
      "Iter: [90/24181]\tTime 0.738\tData 0.007\t\n",
      "Iter: [100/24181]\tTime 0.741\tData 0.006\t\n",
      "Iter: [110/24181]\tTime 0.738\tData 0.007\t\n",
      "Iter: [120/24181]\tTime 0.751\tData 0.006\t\n",
      "Iter: [130/24181]\tTime 0.740\tData 0.005\t\n",
      "Iter: [140/24181]\tTime 0.738\tData 0.006\t\n",
      "Iter: [150/24181]\tTime 0.742\tData 0.005\t\n",
      "Iter: [160/24181]\tTime 0.736\tData 0.005\t\n",
      "Iter: [170/24181]\tTime 0.754\tData 0.008\t\n",
      "Iter: [180/24181]\tTime 0.751\tData 0.006\t\n",
      "Iter: [190/24181]\tTime 0.739\tData 0.005\t\n",
      "Iter: [200/24181]\tTime 0.744\tData 0.007\t\n",
      "Iter: [210/24181]\tTime 0.743\tData 0.008\t\n",
      "Iter: [220/24181]\tTime 0.741\tData 0.005\t\n",
      "Iter: [230/24181]\tTime 0.752\tData 0.008\t\n",
      "Iter: [240/24181]\tTime 0.757\tData 0.016\t\n",
      "Iter: [250/24181]\tTime 0.744\tData 0.006\t\n",
      "Iter: [260/24181]\tTime 0.750\tData 0.007\t\n",
      "Iter: [270/24181]\tTime 0.751\tData 0.010\t\n",
      "Iter: [280/24181]\tTime 0.771\tData 0.019\t\n",
      "Iter: [290/24181]\tTime 0.747\tData 0.007\t\n",
      "Iter: [300/24181]\tTime 0.736\tData 0.009\t\n",
      "Iter: [310/24181]\tTime 0.759\tData 0.008\t\n",
      "Iter: [320/24181]\tTime 0.747\tData 0.006\t\n",
      "Iter: [330/24181]\tTime 0.744\tData 0.007\t\n",
      "Iter: [340/24181]\tTime 0.751\tData 0.008\t\n",
      "Iter: [350/24181]\tTime 0.742\tData 0.005\t\n",
      "Iter: [360/24181]\tTime 0.755\tData 0.007\t\n",
      "Iter: [370/24181]\tTime 0.747\tData 0.006\t\n",
      "Iter: [380/24181]\tTime 0.760\tData 0.006\t\n",
      "Iter: [390/24181]\tTime 0.743\tData 0.006\t\n",
      "Iter: [400/24181]\tTime 0.736\tData 0.006\t\n",
      "Iter: [410/24181]\tTime 0.758\tData 0.007\t\n",
      "Iter: [420/24181]\tTime 0.769\tData 0.009\t\n",
      "Iter: [430/24181]\tTime 0.763\tData 0.019\t\n",
      "Iter: [440/24181]\tTime 0.748\tData 0.008\t\n",
      "Iter: [450/24181]\tTime 0.745\tData 0.007\t\n",
      "Iter: [460/24181]\tTime 0.737\tData 0.005\t\n",
      "Iter: [470/24181]\tTime 0.752\tData 0.008\t\n",
      "Iter: [480/24181]\tTime 0.757\tData 0.010\t\n",
      "Iter: [490/24181]\tTime 0.757\tData 0.007\t\n",
      "Iter: [500/24181]\tTime 0.750\tData 0.006\t\n",
      "Iter: [510/24181]\tTime 0.752\tData 0.006\t\n",
      "Iter: [520/24181]\tTime 0.742\tData 0.006\t\n",
      "Iter: [530/24181]\tTime 0.776\tData 0.016\t\n",
      "Iter: [540/24181]\tTime 0.747\tData 0.005\t\n",
      "Iter: [550/24181]\tTime 0.748\tData 0.006\t\n",
      "Iter: [560/24181]\tTime 0.754\tData 0.007\t\n",
      "Iter: [570/24181]\tTime 0.743\tData 0.005\t\n",
      "Iter: [580/24181]\tTime 0.741\tData 0.011\t\n",
      "Iter: [590/24181]\tTime 0.751\tData 0.009\t\n",
      "Iter: [600/24181]\tTime 0.750\tData 0.007\t\n",
      "Iter: [610/24181]\tTime 0.764\tData 0.006\t\n",
      "Iter: [620/24181]\tTime 0.758\tData 0.008\t\n",
      "Iter: [630/24181]\tTime 0.740\tData 0.007\t\n",
      "Iter: [640/24181]\tTime 0.756\tData 0.007\t\n",
      "Iter: [650/24181]\tTime 0.752\tData 0.006\t\n",
      "Iter: [660/24181]\tTime 0.758\tData 0.009\t\n",
      "Iter: [670/24181]\tTime 0.755\tData 0.007\t\n",
      "Iter: [680/24181]\tTime 0.749\tData 0.010\t\n",
      "Iter: [690/24181]\tTime 0.741\tData 0.005\t\n",
      "Iter: [700/24181]\tTime 0.751\tData 0.012\t\n",
      "Iter: [710/24181]\tTime 0.740\tData 0.007\t\n",
      "Iter: [720/24181]\tTime 0.743\tData 0.005\t\n",
      "Iter: [730/24181]\tTime 0.739\tData 0.010\t\n",
      "Iter: [740/24181]\tTime 0.739\tData 0.005\t\n",
      "Iter: [750/24181]\tTime 0.747\tData 0.007\t\n"
     ]
    }
   ],
   "source": [
    "import torchvision.models as models\n",
    "\n",
    "# get dataset\n",
    "print('getting dataset...')\n",
    "test_dataset = get_cdiscount_dataset(offsets_csv=\"test_offsets.csv\",\n",
    "                                     images_csv=\"test_images.csv\",\n",
    "                                     bson_file_path=\"/mnt/data/cdiscount/test.bson\",\n",
    "                                     with_label=False,\n",
    "                                     resize=160)\n",
    "\n",
    "# get data loader\n",
    "print('getting data loader...')\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=config['test_batch_size'], shuffle=False, num_workers=4)\n",
    "\n",
    "# define model\n",
    "print(\"=> using pre-trained model '{}'\".format(config['arch']))\n",
    "model = models.__dict__[config['arch']](pretrained=False)\n",
    "\n",
    "classifier_layer = [\n",
    "    torch.nn.Linear(in_features=12800, out_features=5270)\n",
    "]\n",
    "\n",
    "classifier = torch.nn.Sequential(*classifier_layer)\n",
    "\n",
    "model = assemble_model_with_classifier(model, -1, classifier)\n",
    "model = torch.nn.DataParallel(model).cuda()\n",
    "print(model)\n",
    "\n",
    "\n",
    "# load checkpoint\n",
    "if not os.path.isfile(config['checkpoint']):\n",
    "    print(\"=> no checkpoint found at '{}'\".format(config['checkpoint']))\n",
    "    \n",
    "print(\"=> loading checkpoint '{}'\".format(config['checkpoint']))\n",
    "checkpoint = torch.load(config['checkpoint'])\n",
    "model.load_state_dict(checkpoint['state_dict'])\n",
    "print(\"=> loaded checkpoint:\")\n",
    "\n",
    "print('Epoch: [{0}][{1}]\\t'\n",
    "      'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "      'Prec@1 {top1.val:.3f} ({top1.avg:.3f})\\t'\n",
    "      'Prec@5 {top5.val:.3f} ({top5.avg:.3f})'.format(\n",
    "       checkpoint['epoch'], checkpoint['iter'], loss=checkpoint['loss'], top1=checkpoint['top1'], top5=checkpoint['top5']))\n",
    "\n",
    "del checkpoint # save some GPU memory\n",
    "\n",
    "# get trainer\n",
    "Predictor = get_predictor(test_dataloader, model, config)\n",
    "\n",
    "# Run!\n",
    "Predictor.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
