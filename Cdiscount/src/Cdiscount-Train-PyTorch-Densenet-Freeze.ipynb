{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "import torchvision.models as models\n",
    "\n",
    "from dataset.dataset import get_cdiscount_dataset\n",
    "from model.model import assemble_model, assemble_model_with_classifier, load_model_merged\n",
    "from model.utils import freeze_layers\n",
    "from model.densenet import *\n",
    "from trainer.trainer import get_trainer\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# redirect print to file\n",
    "# import sys\n",
    "# sys.stdout = open(\"PyTorch-resnet34-log.txt\", \"w\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# configuration\n",
    "config = {\n",
    "    'train_batch_size': 512, 'val_batch_size': 512,\n",
    "    'arch': 'densenet161', 'pretrained': True,\n",
    "    'optimizer': 'Adam', 'learning_rate': 1e-3, 'momentum': 0.9, 'decay_lr_freq': 2e4, 'weight_decay': 1e-4,\n",
    "    'resume': None,\n",
    "    'start_epoch': 0, 'epochs': 10,\n",
    "    'print_freq': 10, 'validate_freq': 2e4, 'save_freq': 1e3,\n",
    "    'best_val_prec1': 0\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "getting dataset...\n",
      "getting data loader...\n",
      "=> using pre-trained model 'densenet161'\n",
      "0 freezing Conv2d(3, 96, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "1 freezing BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True)\n",
      "2 freezing ReLU (inplace)\n",
      "3 freezing MaxPool2d (size=(3, 3), stride=(2, 2), padding=(1, 1), dilation=(1, 1))\n",
      "4 freezing _DenseBlock (\n",
      "  (denselayer1): _DenseLayer (\n",
      "    (norm.1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (relu.1): ReLU (inplace)\n",
      "    (conv.1): Conv2d(96, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (norm.2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (relu.2): ReLU (inplace)\n",
      "    (conv.2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  )\n",
      "  (denselayer2): _DenseLayer (\n",
      "    (norm.1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (relu.1): ReLU (inplace)\n",
      "    (conv.1): Conv2d(144, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (norm.2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (relu.2): ReLU (inplace)\n",
      "    (conv.2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  )\n",
      "  (denselayer3): _DenseLayer (\n",
      "    (norm.1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (relu.1): ReLU (inplace)\n",
      "    (conv.1): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (norm.2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (relu.2): ReLU (inplace)\n",
      "    (conv.2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  )\n",
      "  (denselayer4): _DenseLayer (\n",
      "    (norm.1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (relu.1): ReLU (inplace)\n",
      "    (conv.1): Conv2d(240, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (norm.2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (relu.2): ReLU (inplace)\n",
      "    (conv.2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  )\n",
      "  (denselayer5): _DenseLayer (\n",
      "    (norm.1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (relu.1): ReLU (inplace)\n",
      "    (conv.1): Conv2d(288, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (norm.2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (relu.2): ReLU (inplace)\n",
      "    (conv.2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  )\n",
      "  (denselayer6): _DenseLayer (\n",
      "    (norm.1): BatchNorm2d(336, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (relu.1): ReLU (inplace)\n",
      "    (conv.1): Conv2d(336, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (norm.2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (relu.2): ReLU (inplace)\n",
      "    (conv.2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  )\n",
      ")\n",
      "5 freezing _Transition (\n",
      "  (norm): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True)\n",
      "  (relu): ReLU (inplace)\n",
      "  (conv): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (pool): AvgPool2d (size=2, stride=2, padding=0, ceil_mode=False, count_include_pad=True)\n",
      ")\n",
      "6 freezing _DenseBlock (\n",
      "  (denselayer1): _DenseLayer (\n",
      "    (norm.1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (relu.1): ReLU (inplace)\n",
      "    (conv.1): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (norm.2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (relu.2): ReLU (inplace)\n",
      "    (conv.2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  )\n",
      "  (denselayer2): _DenseLayer (\n",
      "    (norm.1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (relu.1): ReLU (inplace)\n",
      "    (conv.1): Conv2d(240, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (norm.2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (relu.2): ReLU (inplace)\n",
      "    (conv.2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  )\n",
      "  (denselayer3): _DenseLayer (\n",
      "    (norm.1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (relu.1): ReLU (inplace)\n",
      "    (conv.1): Conv2d(288, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (norm.2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (relu.2): ReLU (inplace)\n",
      "    (conv.2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  )\n",
      "  (denselayer4): _DenseLayer (\n",
      "    (norm.1): BatchNorm2d(336, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (relu.1): ReLU (inplace)\n",
      "    (conv.1): Conv2d(336, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (norm.2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (relu.2): ReLU (inplace)\n",
      "    (conv.2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  )\n",
      "  (denselayer5): _DenseLayer (\n",
      "    (norm.1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (relu.1): ReLU (inplace)\n",
      "    (conv.1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (norm.2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (relu.2): ReLU (inplace)\n",
      "    (conv.2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  )\n",
      "  (denselayer6): _DenseLayer (\n",
      "    (norm.1): BatchNorm2d(432, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (relu.1): ReLU (inplace)\n",
      "    (conv.1): Conv2d(432, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (norm.2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (relu.2): ReLU (inplace)\n",
      "    (conv.2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  )\n",
      "  (denselayer7): _DenseLayer (\n",
      "    (norm.1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (relu.1): ReLU (inplace)\n",
      "    (conv.1): Conv2d(480, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (norm.2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (relu.2): ReLU (inplace)\n",
      "    (conv.2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  )\n",
      "  (denselayer8): _DenseLayer (\n",
      "    (norm.1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (relu.1): ReLU (inplace)\n",
      "    (conv.1): Conv2d(528, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (norm.2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (relu.2): ReLU (inplace)\n",
      "    (conv.2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  )\n",
      "  (denselayer9): _DenseLayer (\n",
      "    (norm.1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (relu.1): ReLU (inplace)\n",
      "    (conv.1): Conv2d(576, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (norm.2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (relu.2): ReLU (inplace)\n",
      "    (conv.2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  )\n",
      "  (denselayer10): _DenseLayer (\n",
      "    (norm.1): BatchNorm2d(624, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (relu.1): ReLU (inplace)\n",
      "    (conv.1): Conv2d(624, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (norm.2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (relu.2): ReLU (inplace)\n",
      "    (conv.2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  )\n",
      "  (denselayer11): _DenseLayer (\n",
      "    (norm.1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (relu.1): ReLU (inplace)\n",
      "    (conv.1): Conv2d(672, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (norm.2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (relu.2): ReLU (inplace)\n",
      "    (conv.2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  )\n",
      "  (denselayer12): _DenseLayer (\n",
      "    (norm.1): BatchNorm2d(720, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (relu.1): ReLU (inplace)\n",
      "    (conv.1): Conv2d(720, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (norm.2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (relu.2): ReLU (inplace)\n",
      "    (conv.2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  )\n",
      ")\n",
      "7 freezing _Transition (\n",
      "  (norm): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True)\n",
      "  (relu): ReLU (inplace)\n",
      "  (conv): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "  (pool): AvgPool2d (size=2, stride=2, padding=0, ceil_mode=False, count_include_pad=True)\n",
      ")\n",
      "8 freezing _DenseBlock (\n",
      "  (denselayer1): _DenseLayer (\n",
      "    (norm.1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (relu.1): ReLU (inplace)\n",
      "    (conv.1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (norm.2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (relu.2): ReLU (inplace)\n",
      "    (conv.2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  )\n",
      "  (denselayer2): _DenseLayer (\n",
      "    (norm.1): BatchNorm2d(432, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (relu.1): ReLU (inplace)\n",
      "    (conv.1): Conv2d(432, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (norm.2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (relu.2): ReLU (inplace)\n",
      "    (conv.2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  )\n",
      "  (denselayer3): _DenseLayer (\n",
      "    (norm.1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (relu.1): ReLU (inplace)\n",
      "    (conv.1): Conv2d(480, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (norm.2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (relu.2): ReLU (inplace)\n",
      "    (conv.2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  )\n",
      "  (denselayer4): _DenseLayer (\n",
      "    (norm.1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (relu.1): ReLU (inplace)\n",
      "    (conv.1): Conv2d(528, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (norm.2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (relu.2): ReLU (inplace)\n",
      "    (conv.2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  )\n",
      "  (denselayer5): _DenseLayer (\n",
      "    (norm.1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (relu.1): ReLU (inplace)\n",
      "    (conv.1): Conv2d(576, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (norm.2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (relu.2): ReLU (inplace)\n",
      "    (conv.2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  )\n",
      "  (denselayer6): _DenseLayer (\n",
      "    (norm.1): BatchNorm2d(624, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (relu.1): ReLU (inplace)\n",
      "    (conv.1): Conv2d(624, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (norm.2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (relu.2): ReLU (inplace)\n",
      "    (conv.2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  )\n",
      "  (denselayer7): _DenseLayer (\n",
      "    (norm.1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (relu.1): ReLU (inplace)\n",
      "    (conv.1): Conv2d(672, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (norm.2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (relu.2): ReLU (inplace)\n",
      "    (conv.2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  )\n",
      "  (denselayer8): _DenseLayer (\n",
      "    (norm.1): BatchNorm2d(720, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (relu.1): ReLU (inplace)\n",
      "    (conv.1): Conv2d(720, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (norm.2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (relu.2): ReLU (inplace)\n",
      "    (conv.2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  )\n",
      "  (denselayer9): _DenseLayer (\n",
      "    (norm.1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (relu.1): ReLU (inplace)\n",
      "    (conv.1): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (norm.2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (relu.2): ReLU (inplace)\n",
      "    (conv.2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  )\n",
      "  (denselayer10): _DenseLayer (\n",
      "    (norm.1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (relu.1): ReLU (inplace)\n",
      "    (conv.1): Conv2d(816, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (norm.2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (relu.2): ReLU (inplace)\n",
      "    (conv.2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  )\n",
      "  (denselayer11): _DenseLayer (\n",
      "    (norm.1): BatchNorm2d(864, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (relu.1): ReLU (inplace)\n",
      "    (conv.1): Conv2d(864, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (norm.2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (relu.2): ReLU (inplace)\n",
      "    (conv.2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  )\n",
      "  (denselayer12): _DenseLayer (\n",
      "    (norm.1): BatchNorm2d(912, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (relu.1): ReLU (inplace)\n",
      "    (conv.1): Conv2d(912, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (norm.2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (relu.2): ReLU (inplace)\n",
      "    (conv.2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  )\n",
      "  (denselayer13): _DenseLayer (\n",
      "    (norm.1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (relu.1): ReLU (inplace)\n",
      "    (conv.1): Conv2d(960, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (norm.2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (relu.2): ReLU (inplace)\n",
      "    (conv.2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  )\n",
      "  (denselayer14): _DenseLayer (\n",
      "    (norm.1): BatchNorm2d(1008, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (relu.1): ReLU (inplace)\n",
      "    (conv.1): Conv2d(1008, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (norm.2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (relu.2): ReLU (inplace)\n",
      "    (conv.2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  )\n",
      "  (denselayer15): _DenseLayer (\n",
      "    (norm.1): BatchNorm2d(1056, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (relu.1): ReLU (inplace)\n",
      "    (conv.1): Conv2d(1056, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (norm.2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (relu.2): ReLU (inplace)\n",
      "    (conv.2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  )\n",
      "  (denselayer16): _DenseLayer (\n",
      "    (norm.1): BatchNorm2d(1104, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (relu.1): ReLU (inplace)\n",
      "    (conv.1): Conv2d(1104, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (norm.2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (relu.2): ReLU (inplace)\n",
      "    (conv.2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  )\n",
      "  (denselayer17): _DenseLayer (\n",
      "    (norm.1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (relu.1): ReLU (inplace)\n",
      "    (conv.1): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (norm.2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (relu.2): ReLU (inplace)\n",
      "    (conv.2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  )\n",
      "  (denselayer18): _DenseLayer (\n",
      "    (norm.1): BatchNorm2d(1200, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (relu.1): ReLU (inplace)\n",
      "    (conv.1): Conv2d(1200, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (norm.2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (relu.2): ReLU (inplace)\n",
      "    (conv.2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  )\n",
      "  (denselayer19): _DenseLayer (\n",
      "    (norm.1): BatchNorm2d(1248, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (relu.1): ReLU (inplace)\n",
      "    (conv.1): Conv2d(1248, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (norm.2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (relu.2): ReLU (inplace)\n",
      "    (conv.2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  )\n",
      "  (denselayer20): _DenseLayer (\n",
      "    (norm.1): BatchNorm2d(1296, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (relu.1): ReLU (inplace)\n",
      "    (conv.1): Conv2d(1296, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (norm.2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (relu.2): ReLU (inplace)\n",
      "    (conv.2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  )\n",
      "  (denselayer21): _DenseLayer (\n",
      "    (norm.1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (relu.1): ReLU (inplace)\n",
      "    (conv.1): Conv2d(1344, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (norm.2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (relu.2): ReLU (inplace)\n",
      "    (conv.2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  )\n",
      "  (denselayer22): _DenseLayer (\n",
      "    (norm.1): BatchNorm2d(1392, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (relu.1): ReLU (inplace)\n",
      "    (conv.1): Conv2d(1392, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (norm.2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (relu.2): ReLU (inplace)\n",
      "    (conv.2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  )\n",
      "  (denselayer23): _DenseLayer (\n",
      "    (norm.1): BatchNorm2d(1440, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (relu.1): ReLU (inplace)\n",
      "    (conv.1): Conv2d(1440, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (norm.2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (relu.2): ReLU (inplace)\n",
      "    (conv.2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  )\n",
      "  (denselayer24): _DenseLayer (\n",
      "    (norm.1): BatchNorm2d(1488, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (relu.1): ReLU (inplace)\n",
      "    (conv.1): Conv2d(1488, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (norm.2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (relu.2): ReLU (inplace)\n",
      "    (conv.2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  )\n",
      "  (denselayer25): _DenseLayer (\n",
      "    (norm.1): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (relu.1): ReLU (inplace)\n",
      "    (conv.1): Conv2d(1536, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (norm.2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (relu.2): ReLU (inplace)\n",
      "    (conv.2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  )\n",
      "  (denselayer26): _DenseLayer (\n",
      "    (norm.1): BatchNorm2d(1584, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (relu.1): ReLU (inplace)\n",
      "    (conv.1): Conv2d(1584, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (norm.2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (relu.2): ReLU (inplace)\n",
      "    (conv.2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  )\n",
      "  (denselayer27): _DenseLayer (\n",
      "    (norm.1): BatchNorm2d(1632, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (relu.1): ReLU (inplace)\n",
      "    (conv.1): Conv2d(1632, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (norm.2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (relu.2): ReLU (inplace)\n",
      "    (conv.2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  )\n",
      "  (denselayer28): _DenseLayer (\n",
      "    (norm.1): BatchNorm2d(1680, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (relu.1): ReLU (inplace)\n",
      "    (conv.1): Conv2d(1680, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (norm.2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (relu.2): ReLU (inplace)\n",
      "    (conv.2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  )\n",
      "  (denselayer29): _DenseLayer (\n",
      "    (norm.1): BatchNorm2d(1728, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (relu.1): ReLU (inplace)\n",
      "    (conv.1): Conv2d(1728, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (norm.2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (relu.2): ReLU (inplace)\n",
      "    (conv.2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  )\n",
      "  (denselayer30): _DenseLayer (\n",
      "    (norm.1): BatchNorm2d(1776, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (relu.1): ReLU (inplace)\n",
      "    (conv.1): Conv2d(1776, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (norm.2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (relu.2): ReLU (inplace)\n",
      "    (conv.2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  )\n",
      "  (denselayer31): _DenseLayer (\n",
      "    (norm.1): BatchNorm2d(1824, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (relu.1): ReLU (inplace)\n",
      "    (conv.1): Conv2d(1824, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (norm.2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (relu.2): ReLU (inplace)\n",
      "    (conv.2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  )\n",
      "  (denselayer32): _DenseLayer (\n",
      "    (norm.1): BatchNorm2d(1872, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (relu.1): ReLU (inplace)\n",
      "    (conv.1): Conv2d(1872, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (norm.2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (relu.2): ReLU (inplace)\n",
      "    (conv.2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  )\n",
      "  (denselayer33): _DenseLayer (\n",
      "    (norm.1): BatchNorm2d(1920, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (relu.1): ReLU (inplace)\n",
      "    (conv.1): Conv2d(1920, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (norm.2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (relu.2): ReLU (inplace)\n",
      "    (conv.2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  )\n",
      "  (denselayer34): _DenseLayer (\n",
      "    (norm.1): BatchNorm2d(1968, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (relu.1): ReLU (inplace)\n",
      "    (conv.1): Conv2d(1968, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (norm.2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (relu.2): ReLU (inplace)\n",
      "    (conv.2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  )\n",
      "  (denselayer35): _DenseLayer (\n",
      "    (norm.1): BatchNorm2d(2016, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (relu.1): ReLU (inplace)\n",
      "    (conv.1): Conv2d(2016, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (norm.2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (relu.2): ReLU (inplace)\n",
      "    (conv.2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  )\n",
      "  (denselayer36): _DenseLayer (\n",
      "    (norm.1): BatchNorm2d(2064, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (relu.1): ReLU (inplace)\n",
      "    (conv.1): Conv2d(2064, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "    (norm.2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True)\n",
      "    (relu.2): ReLU (inplace)\n",
      "    (conv.2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "  )\n",
      ")\n",
      "DataParallel (\n",
      "  (module): DenseNet (\n",
      "    (features): Sequential (\n",
      "      (conv0): Conv2d(3, 96, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "      (norm0): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True)\n",
      "      (relu0): ReLU (inplace)\n",
      "      (pool0): MaxPool2d (size=(3, 3), stride=(2, 2), padding=(1, 1), dilation=(1, 1))\n",
      "      (denseblock1): _DenseBlock (\n",
      "        (denselayer1): _DenseLayer (\n",
      "          (norm.1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu.1): ReLU (inplace)\n",
      "          (conv.1): Conv2d(96, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm.2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu.2): ReLU (inplace)\n",
      "          (conv.2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (denselayer2): _DenseLayer (\n",
      "          (norm.1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu.1): ReLU (inplace)\n",
      "          (conv.1): Conv2d(144, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm.2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu.2): ReLU (inplace)\n",
      "          (conv.2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (denselayer3): _DenseLayer (\n",
      "          (norm.1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu.1): ReLU (inplace)\n",
      "          (conv.1): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm.2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu.2): ReLU (inplace)\n",
      "          (conv.2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (denselayer4): _DenseLayer (\n",
      "          (norm.1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu.1): ReLU (inplace)\n",
      "          (conv.1): Conv2d(240, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm.2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu.2): ReLU (inplace)\n",
      "          (conv.2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (denselayer5): _DenseLayer (\n",
      "          (norm.1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu.1): ReLU (inplace)\n",
      "          (conv.1): Conv2d(288, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm.2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu.2): ReLU (inplace)\n",
      "          (conv.2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (denselayer6): _DenseLayer (\n",
      "          (norm.1): BatchNorm2d(336, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu.1): ReLU (inplace)\n",
      "          (conv.1): Conv2d(336, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm.2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu.2): ReLU (inplace)\n",
      "          (conv.2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "      )\n",
      "      (transition1): _Transition (\n",
      "        (norm): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True)\n",
      "        (relu): ReLU (inplace)\n",
      "        (conv): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (pool): AvgPool2d (size=2, stride=2, padding=0, ceil_mode=False, count_include_pad=True)\n",
      "      )\n",
      "      (denseblock2): _DenseBlock (\n",
      "        (denselayer1): _DenseLayer (\n",
      "          (norm.1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu.1): ReLU (inplace)\n",
      "          (conv.1): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm.2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu.2): ReLU (inplace)\n",
      "          (conv.2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (denselayer2): _DenseLayer (\n",
      "          (norm.1): BatchNorm2d(240, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu.1): ReLU (inplace)\n",
      "          (conv.1): Conv2d(240, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm.2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu.2): ReLU (inplace)\n",
      "          (conv.2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (denselayer3): _DenseLayer (\n",
      "          (norm.1): BatchNorm2d(288, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu.1): ReLU (inplace)\n",
      "          (conv.1): Conv2d(288, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm.2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu.2): ReLU (inplace)\n",
      "          (conv.2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (denselayer4): _DenseLayer (\n",
      "          (norm.1): BatchNorm2d(336, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu.1): ReLU (inplace)\n",
      "          (conv.1): Conv2d(336, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm.2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu.2): ReLU (inplace)\n",
      "          (conv.2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (denselayer5): _DenseLayer (\n",
      "          (norm.1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu.1): ReLU (inplace)\n",
      "          (conv.1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm.2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu.2): ReLU (inplace)\n",
      "          (conv.2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (denselayer6): _DenseLayer (\n",
      "          (norm.1): BatchNorm2d(432, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu.1): ReLU (inplace)\n",
      "          (conv.1): Conv2d(432, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm.2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu.2): ReLU (inplace)\n",
      "          (conv.2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (denselayer7): _DenseLayer (\n",
      "          (norm.1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu.1): ReLU (inplace)\n",
      "          (conv.1): Conv2d(480, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm.2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu.2): ReLU (inplace)\n",
      "          (conv.2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (denselayer8): _DenseLayer (\n",
      "          (norm.1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu.1): ReLU (inplace)\n",
      "          (conv.1): Conv2d(528, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm.2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu.2): ReLU (inplace)\n",
      "          (conv.2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (denselayer9): _DenseLayer (\n",
      "          (norm.1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu.1): ReLU (inplace)\n",
      "          (conv.1): Conv2d(576, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm.2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu.2): ReLU (inplace)\n",
      "          (conv.2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (denselayer10): _DenseLayer (\n",
      "          (norm.1): BatchNorm2d(624, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu.1): ReLU (inplace)\n",
      "          (conv.1): Conv2d(624, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm.2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu.2): ReLU (inplace)\n",
      "          (conv.2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (denselayer11): _DenseLayer (\n",
      "          (norm.1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu.1): ReLU (inplace)\n",
      "          (conv.1): Conv2d(672, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm.2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu.2): ReLU (inplace)\n",
      "          (conv.2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (denselayer12): _DenseLayer (\n",
      "          (norm.1): BatchNorm2d(720, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu.1): ReLU (inplace)\n",
      "          (conv.1): Conv2d(720, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm.2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu.2): ReLU (inplace)\n",
      "          (conv.2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "      )\n",
      "      (transition2): _Transition (\n",
      "        (norm): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True)\n",
      "        (relu): ReLU (inplace)\n",
      "        (conv): Conv2d(768, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (pool): AvgPool2d (size=2, stride=2, padding=0, ceil_mode=False, count_include_pad=True)\n",
      "      )\n",
      "      (denseblock3): _DenseBlock (\n",
      "        (denselayer1): _DenseLayer (\n",
      "          (norm.1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu.1): ReLU (inplace)\n",
      "          (conv.1): Conv2d(384, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm.2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu.2): ReLU (inplace)\n",
      "          (conv.2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (denselayer2): _DenseLayer (\n",
      "          (norm.1): BatchNorm2d(432, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu.1): ReLU (inplace)\n",
      "          (conv.1): Conv2d(432, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm.2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu.2): ReLU (inplace)\n",
      "          (conv.2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (denselayer3): _DenseLayer (\n",
      "          (norm.1): BatchNorm2d(480, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu.1): ReLU (inplace)\n",
      "          (conv.1): Conv2d(480, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm.2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu.2): ReLU (inplace)\n",
      "          (conv.2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (denselayer4): _DenseLayer (\n",
      "          (norm.1): BatchNorm2d(528, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu.1): ReLU (inplace)\n",
      "          (conv.1): Conv2d(528, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm.2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu.2): ReLU (inplace)\n",
      "          (conv.2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (denselayer5): _DenseLayer (\n",
      "          (norm.1): BatchNorm2d(576, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu.1): ReLU (inplace)\n",
      "          (conv.1): Conv2d(576, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm.2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu.2): ReLU (inplace)\n",
      "          (conv.2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (denselayer6): _DenseLayer (\n",
      "          (norm.1): BatchNorm2d(624, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu.1): ReLU (inplace)\n",
      "          (conv.1): Conv2d(624, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm.2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu.2): ReLU (inplace)\n",
      "          (conv.2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (denselayer7): _DenseLayer (\n",
      "          (norm.1): BatchNorm2d(672, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu.1): ReLU (inplace)\n",
      "          (conv.1): Conv2d(672, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm.2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu.2): ReLU (inplace)\n",
      "          (conv.2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (denselayer8): _DenseLayer (\n",
      "          (norm.1): BatchNorm2d(720, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu.1): ReLU (inplace)\n",
      "          (conv.1): Conv2d(720, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm.2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu.2): ReLU (inplace)\n",
      "          (conv.2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (denselayer9): _DenseLayer (\n",
      "          (norm.1): BatchNorm2d(768, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu.1): ReLU (inplace)\n",
      "          (conv.1): Conv2d(768, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm.2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu.2): ReLU (inplace)\n",
      "          (conv.2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (denselayer10): _DenseLayer (\n",
      "          (norm.1): BatchNorm2d(816, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu.1): ReLU (inplace)\n",
      "          (conv.1): Conv2d(816, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm.2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu.2): ReLU (inplace)\n",
      "          (conv.2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (denselayer11): _DenseLayer (\n",
      "          (norm.1): BatchNorm2d(864, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu.1): ReLU (inplace)\n",
      "          (conv.1): Conv2d(864, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm.2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu.2): ReLU (inplace)\n",
      "          (conv.2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (denselayer12): _DenseLayer (\n",
      "          (norm.1): BatchNorm2d(912, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu.1): ReLU (inplace)\n",
      "          (conv.1): Conv2d(912, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm.2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu.2): ReLU (inplace)\n",
      "          (conv.2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (denselayer13): _DenseLayer (\n",
      "          (norm.1): BatchNorm2d(960, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu.1): ReLU (inplace)\n",
      "          (conv.1): Conv2d(960, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm.2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu.2): ReLU (inplace)\n",
      "          (conv.2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (denselayer14): _DenseLayer (\n",
      "          (norm.1): BatchNorm2d(1008, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu.1): ReLU (inplace)\n",
      "          (conv.1): Conv2d(1008, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm.2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu.2): ReLU (inplace)\n",
      "          (conv.2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (denselayer15): _DenseLayer (\n",
      "          (norm.1): BatchNorm2d(1056, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu.1): ReLU (inplace)\n",
      "          (conv.1): Conv2d(1056, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm.2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu.2): ReLU (inplace)\n",
      "          (conv.2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (denselayer16): _DenseLayer (\n",
      "          (norm.1): BatchNorm2d(1104, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu.1): ReLU (inplace)\n",
      "          (conv.1): Conv2d(1104, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm.2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu.2): ReLU (inplace)\n",
      "          (conv.2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (denselayer17): _DenseLayer (\n",
      "          (norm.1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu.1): ReLU (inplace)\n",
      "          (conv.1): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm.2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu.2): ReLU (inplace)\n",
      "          (conv.2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (denselayer18): _DenseLayer (\n",
      "          (norm.1): BatchNorm2d(1200, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu.1): ReLU (inplace)\n",
      "          (conv.1): Conv2d(1200, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm.2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu.2): ReLU (inplace)\n",
      "          (conv.2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (denselayer19): _DenseLayer (\n",
      "          (norm.1): BatchNorm2d(1248, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu.1): ReLU (inplace)\n",
      "          (conv.1): Conv2d(1248, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm.2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu.2): ReLU (inplace)\n",
      "          (conv.2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (denselayer20): _DenseLayer (\n",
      "          (norm.1): BatchNorm2d(1296, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu.1): ReLU (inplace)\n",
      "          (conv.1): Conv2d(1296, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm.2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu.2): ReLU (inplace)\n",
      "          (conv.2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (denselayer21): _DenseLayer (\n",
      "          (norm.1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu.1): ReLU (inplace)\n",
      "          (conv.1): Conv2d(1344, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm.2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu.2): ReLU (inplace)\n",
      "          (conv.2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (denselayer22): _DenseLayer (\n",
      "          (norm.1): BatchNorm2d(1392, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu.1): ReLU (inplace)\n",
      "          (conv.1): Conv2d(1392, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm.2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu.2): ReLU (inplace)\n",
      "          (conv.2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (denselayer23): _DenseLayer (\n",
      "          (norm.1): BatchNorm2d(1440, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu.1): ReLU (inplace)\n",
      "          (conv.1): Conv2d(1440, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm.2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu.2): ReLU (inplace)\n",
      "          (conv.2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (denselayer24): _DenseLayer (\n",
      "          (norm.1): BatchNorm2d(1488, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu.1): ReLU (inplace)\n",
      "          (conv.1): Conv2d(1488, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm.2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu.2): ReLU (inplace)\n",
      "          (conv.2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (denselayer25): _DenseLayer (\n",
      "          (norm.1): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu.1): ReLU (inplace)\n",
      "          (conv.1): Conv2d(1536, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm.2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu.2): ReLU (inplace)\n",
      "          (conv.2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (denselayer26): _DenseLayer (\n",
      "          (norm.1): BatchNorm2d(1584, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu.1): ReLU (inplace)\n",
      "          (conv.1): Conv2d(1584, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm.2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu.2): ReLU (inplace)\n",
      "          (conv.2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (denselayer27): _DenseLayer (\n",
      "          (norm.1): BatchNorm2d(1632, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu.1): ReLU (inplace)\n",
      "          (conv.1): Conv2d(1632, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm.2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu.2): ReLU (inplace)\n",
      "          (conv.2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (denselayer28): _DenseLayer (\n",
      "          (norm.1): BatchNorm2d(1680, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu.1): ReLU (inplace)\n",
      "          (conv.1): Conv2d(1680, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm.2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu.2): ReLU (inplace)\n",
      "          (conv.2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (denselayer29): _DenseLayer (\n",
      "          (norm.1): BatchNorm2d(1728, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu.1): ReLU (inplace)\n",
      "          (conv.1): Conv2d(1728, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm.2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu.2): ReLU (inplace)\n",
      "          (conv.2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (denselayer30): _DenseLayer (\n",
      "          (norm.1): BatchNorm2d(1776, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu.1): ReLU (inplace)\n",
      "          (conv.1): Conv2d(1776, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm.2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu.2): ReLU (inplace)\n",
      "          (conv.2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (denselayer31): _DenseLayer (\n",
      "          (norm.1): BatchNorm2d(1824, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu.1): ReLU (inplace)\n",
      "          (conv.1): Conv2d(1824, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm.2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu.2): ReLU (inplace)\n",
      "          (conv.2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (denselayer32): _DenseLayer (\n",
      "          (norm.1): BatchNorm2d(1872, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu.1): ReLU (inplace)\n",
      "          (conv.1): Conv2d(1872, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm.2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu.2): ReLU (inplace)\n",
      "          (conv.2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (denselayer33): _DenseLayer (\n",
      "          (norm.1): BatchNorm2d(1920, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu.1): ReLU (inplace)\n",
      "          (conv.1): Conv2d(1920, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm.2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu.2): ReLU (inplace)\n",
      "          (conv.2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (denselayer34): _DenseLayer (\n",
      "          (norm.1): BatchNorm2d(1968, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu.1): ReLU (inplace)\n",
      "          (conv.1): Conv2d(1968, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm.2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu.2): ReLU (inplace)\n",
      "          (conv.2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (denselayer35): _DenseLayer (\n",
      "          (norm.1): BatchNorm2d(2016, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu.1): ReLU (inplace)\n",
      "          (conv.1): Conv2d(2016, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm.2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu.2): ReLU (inplace)\n",
      "          (conv.2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (denselayer36): _DenseLayer (\n",
      "          (norm.1): BatchNorm2d(2064, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu.1): ReLU (inplace)\n",
      "          (conv.1): Conv2d(2064, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm.2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu.2): ReLU (inplace)\n",
      "          (conv.2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "      )\n",
      "      (transition3): _Transition (\n",
      "        (norm): BatchNorm2d(2112, eps=1e-05, momentum=0.1, affine=True)\n",
      "        (relu): ReLU (inplace)\n",
      "        (conv): Conv2d(2112, 1056, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (pool): AvgPool2d (size=2, stride=2, padding=0, ceil_mode=False, count_include_pad=True)\n",
      "      )\n",
      "      (denseblock4): _DenseBlock (\n",
      "        (denselayer1): _DenseLayer (\n",
      "          (norm.1): BatchNorm2d(1056, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu.1): ReLU (inplace)\n",
      "          (conv.1): Conv2d(1056, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm.2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu.2): ReLU (inplace)\n",
      "          (conv.2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (denselayer2): _DenseLayer (\n",
      "          (norm.1): BatchNorm2d(1104, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu.1): ReLU (inplace)\n",
      "          (conv.1): Conv2d(1104, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm.2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu.2): ReLU (inplace)\n",
      "          (conv.2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (denselayer3): _DenseLayer (\n",
      "          (norm.1): BatchNorm2d(1152, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu.1): ReLU (inplace)\n",
      "          (conv.1): Conv2d(1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm.2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu.2): ReLU (inplace)\n",
      "          (conv.2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (denselayer4): _DenseLayer (\n",
      "          (norm.1): BatchNorm2d(1200, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu.1): ReLU (inplace)\n",
      "          (conv.1): Conv2d(1200, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm.2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu.2): ReLU (inplace)\n",
      "          (conv.2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (denselayer5): _DenseLayer (\n",
      "          (norm.1): BatchNorm2d(1248, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu.1): ReLU (inplace)\n",
      "          (conv.1): Conv2d(1248, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm.2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu.2): ReLU (inplace)\n",
      "          (conv.2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (denselayer6): _DenseLayer (\n",
      "          (norm.1): BatchNorm2d(1296, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu.1): ReLU (inplace)\n",
      "          (conv.1): Conv2d(1296, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm.2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu.2): ReLU (inplace)\n",
      "          (conv.2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (denselayer7): _DenseLayer (\n",
      "          (norm.1): BatchNorm2d(1344, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu.1): ReLU (inplace)\n",
      "          (conv.1): Conv2d(1344, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm.2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu.2): ReLU (inplace)\n",
      "          (conv.2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (denselayer8): _DenseLayer (\n",
      "          (norm.1): BatchNorm2d(1392, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu.1): ReLU (inplace)\n",
      "          (conv.1): Conv2d(1392, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm.2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu.2): ReLU (inplace)\n",
      "          (conv.2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (denselayer9): _DenseLayer (\n",
      "          (norm.1): BatchNorm2d(1440, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu.1): ReLU (inplace)\n",
      "          (conv.1): Conv2d(1440, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm.2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu.2): ReLU (inplace)\n",
      "          (conv.2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (denselayer10): _DenseLayer (\n",
      "          (norm.1): BatchNorm2d(1488, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu.1): ReLU (inplace)\n",
      "          (conv.1): Conv2d(1488, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm.2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu.2): ReLU (inplace)\n",
      "          (conv.2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (denselayer11): _DenseLayer (\n",
      "          (norm.1): BatchNorm2d(1536, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu.1): ReLU (inplace)\n",
      "          (conv.1): Conv2d(1536, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm.2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu.2): ReLU (inplace)\n",
      "          (conv.2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (denselayer12): _DenseLayer (\n",
      "          (norm.1): BatchNorm2d(1584, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu.1): ReLU (inplace)\n",
      "          (conv.1): Conv2d(1584, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm.2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu.2): ReLU (inplace)\n",
      "          (conv.2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (denselayer13): _DenseLayer (\n",
      "          (norm.1): BatchNorm2d(1632, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu.1): ReLU (inplace)\n",
      "          (conv.1): Conv2d(1632, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm.2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu.2): ReLU (inplace)\n",
      "          (conv.2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (denselayer14): _DenseLayer (\n",
      "          (norm.1): BatchNorm2d(1680, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu.1): ReLU (inplace)\n",
      "          (conv.1): Conv2d(1680, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm.2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu.2): ReLU (inplace)\n",
      "          (conv.2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (denselayer15): _DenseLayer (\n",
      "          (norm.1): BatchNorm2d(1728, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu.1): ReLU (inplace)\n",
      "          (conv.1): Conv2d(1728, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm.2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu.2): ReLU (inplace)\n",
      "          (conv.2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (denselayer16): _DenseLayer (\n",
      "          (norm.1): BatchNorm2d(1776, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu.1): ReLU (inplace)\n",
      "          (conv.1): Conv2d(1776, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm.2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu.2): ReLU (inplace)\n",
      "          (conv.2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (denselayer17): _DenseLayer (\n",
      "          (norm.1): BatchNorm2d(1824, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu.1): ReLU (inplace)\n",
      "          (conv.1): Conv2d(1824, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm.2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu.2): ReLU (inplace)\n",
      "          (conv.2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (denselayer18): _DenseLayer (\n",
      "          (norm.1): BatchNorm2d(1872, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu.1): ReLU (inplace)\n",
      "          (conv.1): Conv2d(1872, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm.2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu.2): ReLU (inplace)\n",
      "          (conv.2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (denselayer19): _DenseLayer (\n",
      "          (norm.1): BatchNorm2d(1920, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu.1): ReLU (inplace)\n",
      "          (conv.1): Conv2d(1920, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm.2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu.2): ReLU (inplace)\n",
      "          (conv.2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (denselayer20): _DenseLayer (\n",
      "          (norm.1): BatchNorm2d(1968, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu.1): ReLU (inplace)\n",
      "          (conv.1): Conv2d(1968, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm.2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu.2): ReLU (inplace)\n",
      "          (conv.2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (denselayer21): _DenseLayer (\n",
      "          (norm.1): BatchNorm2d(2016, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu.1): ReLU (inplace)\n",
      "          (conv.1): Conv2d(2016, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm.2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu.2): ReLU (inplace)\n",
      "          (conv.2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (denselayer22): _DenseLayer (\n",
      "          (norm.1): BatchNorm2d(2064, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu.1): ReLU (inplace)\n",
      "          (conv.1): Conv2d(2064, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm.2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu.2): ReLU (inplace)\n",
      "          (conv.2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (denselayer23): _DenseLayer (\n",
      "          (norm.1): BatchNorm2d(2112, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu.1): ReLU (inplace)\n",
      "          (conv.1): Conv2d(2112, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm.2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu.2): ReLU (inplace)\n",
      "          (conv.2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "        (denselayer24): _DenseLayer (\n",
      "          (norm.1): BatchNorm2d(2160, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu.1): ReLU (inplace)\n",
      "          (conv.1): Conv2d(2160, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "          (norm.2): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True)\n",
      "          (relu.2): ReLU (inplace)\n",
      "          (conv.2): Conv2d(192, 48, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        )\n",
      "      )\n",
      "      (norm5): BatchNorm2d(2208, eps=1e-05, momentum=0.1, affine=True)\n",
      "    )\n",
      "    (classifier): Linear (8832 -> 5270)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# get dataset\n",
    "print('getting dataset...')\n",
    "train_dataset = get_cdiscount_dataset(offsets_csv=\"train_offsets.csv\",\n",
    "                                      images_csv=\"train_images.csv\",\n",
    "                                      bson_file_path=\"/mnt/data/cdiscount/train.bson\",\n",
    "                                      with_label=True,\n",
    "                                      resize=160)\n",
    "val_dataset = get_cdiscount_dataset(offsets_csv=\"train_offsets.csv\",\n",
    "                                    images_csv=\"val_images.csv\",\n",
    "                                    bson_file_path=\"/mnt/data/cdiscount/train.bson\",\n",
    "                                    with_label=True,\n",
    "                                    resize=160)\n",
    "\n",
    "# get data loader\n",
    "print('getting data loader...')\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=config['train_batch_size'], shuffle=True, num_workers=1)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=config['val_batch_size'], shuffle=True, num_workers=1)\n",
    "\n",
    "# define model\n",
    "print(\"=> using pre-trained model '{}'\".format(config['arch']))\n",
    "pretrained_model = models.__dict__[config['arch']](pretrained=config['pretrained'])\n",
    "\n",
    "# model = torch.nn.Sequential(*list(model.children())[:-1])\n",
    "# model.add_module('classifier', torch.nn.Linear(in_features=2048, out_features=5270))\n",
    "# model = torch.nn.DataParallel(model).cuda()\n",
    "# print(model)\n",
    "\n",
    "#model = load_model_merged(name=config['arch'], num_classes=5270)\n",
    "model = densenet161(num_classes=5270)\n",
    "\n",
    "pretrained_state = pretrained_model.state_dict()\n",
    "model_state = model.state_dict()\n",
    "\n",
    "for name, state in pretrained_state.items():\n",
    "    if not name.startswith('classifier'):\n",
    "        model_state[name].copy_(state)\n",
    "\n",
    "# freeze_layers(model, 6)\n",
    "n_layers = 9\n",
    "i = 0\n",
    "for child in model.children():\n",
    "    for subchild in child.children():\n",
    "        if i >= n_layers:\n",
    "            break\n",
    "        print(i, \"freezing\", subchild)\n",
    "        for param in subchild.parameters():\n",
    "            param.requires_grad = False\n",
    "        i += 1\n",
    "        \n",
    "\n",
    "# classifier_layer = [\n",
    "#     torch.nn.Linear(in_features=2208, out_features=5270)\n",
    "# ]\n",
    "\n",
    "# # classifier_layer = [\n",
    "# #     torch.nn.Linear(in_features=2048, out_features=5270),\n",
    "# # ]\n",
    "\n",
    "# classifier = torch.nn.Sequential(*classifier_layer)\n",
    "\n",
    "# model = assemble_model_with_classifier(model, -1, torch.nn.Linear(in_features=2208, out_features=5270))\n",
    "model = torch.nn.DataParallel(model).cuda()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training\n",
      "Epoch: [0][0/19336]\tTime 97.360 (97.360)\tData 2.998 (2.998)\tLoss 8.6644 (8.6644)\tPrec@1 0.000 (0.000)\tPrec@5 0.195 (0.195)\n",
      "Epoch: [0][10/19336]\tTime 6.075 (14.363)\tData 0.012 (0.285)\tLoss 7.1972 (7.6138)\tPrec@1 9.961 (7.120)\tPrec@5 18.945 (14.062)\n",
      "Epoch: [0][20/19336]\tTime 6.113 (10.426)\tData 0.013 (0.156)\tLoss 6.2440 (7.0476)\tPrec@1 15.625 (10.128)\tPrec@5 27.539 (19.996)\n",
      "Epoch: [0][30/19336]\tTime 6.103 (9.033)\tData 0.013 (0.110)\tLoss 5.8004 (6.6869)\tPrec@1 15.820 (11.864)\tPrec@5 30.273 (23.173)\n",
      "Epoch: [0][40/19336]\tTime 6.114 (8.318)\tData 0.014 (0.087)\tLoss 5.4047 (6.4069)\tPrec@1 19.531 (13.686)\tPrec@5 34.570 (25.953)\n",
      "Epoch: [0][50/19336]\tTime 6.124 (7.882)\tData 0.016 (0.073)\tLoss 5.0729 (6.1727)\tPrec@1 23.047 (15.219)\tPrec@5 42.188 (28.389)\n",
      "Epoch: [0][60/19336]\tTime 6.089 (7.591)\tData 0.013 (0.064)\tLoss 5.1486 (6.0053)\tPrec@1 23.047 (16.531)\tPrec@5 37.891 (30.197)\n",
      "Epoch: [0][70/19336]\tTime 6.123 (7.383)\tData 0.020 (0.057)\tLoss 4.7306 (5.8571)\tPrec@1 25.000 (17.564)\tPrec@5 42.383 (31.558)\n",
      "Epoch: [0][80/19336]\tTime 6.123 (7.226)\tData 0.018 (0.052)\tLoss 4.5892 (5.7378)\tPrec@1 25.781 (18.388)\tPrec@5 43.164 (32.769)\n",
      "Epoch: [0][90/19336]\tTime 6.128 (7.105)\tData 0.023 (0.048)\tLoss 4.6738 (5.6234)\tPrec@1 27.148 (19.276)\tPrec@5 40.820 (33.944)\n",
      "Epoch: [0][100/19336]\tTime 6.182 (7.007)\tData 0.014 (0.045)\tLoss 4.8461 (5.5365)\tPrec@1 26.562 (19.999)\tPrec@5 44.922 (34.795)\n",
      "Epoch: [0][110/19336]\tTime 6.071 (6.927)\tData 0.017 (0.042)\tLoss 4.5971 (5.4547)\tPrec@1 29.688 (20.592)\tPrec@5 46.289 (35.689)\n",
      "Epoch: [0][120/19336]\tTime 6.075 (6.859)\tData 0.013 (0.040)\tLoss 4.3431 (5.3713)\tPrec@1 28.516 (21.266)\tPrec@5 47.070 (36.527)\n",
      "Epoch: [0][130/19336]\tTime 6.100 (6.801)\tData 0.019 (0.038)\tLoss 4.4219 (5.3046)\tPrec@1 28.125 (21.777)\tPrec@5 46.875 (37.227)\n",
      "Epoch: [0][140/19336]\tTime 6.098 (6.752)\tData 0.014 (0.037)\tLoss 4.5315 (5.2474)\tPrec@1 30.469 (22.261)\tPrec@5 45.898 (37.856)\n",
      "Epoch: [0][150/19336]\tTime 6.103 (6.709)\tData 0.018 (0.035)\tLoss 4.6234 (5.1889)\tPrec@1 26.953 (22.673)\tPrec@5 44.922 (38.483)\n",
      "Epoch: [0][160/19336]\tTime 6.124 (6.672)\tData 0.015 (0.034)\tLoss 4.1368 (5.1327)\tPrec@1 33.203 (23.094)\tPrec@5 51.367 (39.120)\n",
      "Epoch: [0][170/19336]\tTime 6.090 (6.639)\tData 0.015 (0.033)\tLoss 4.1687 (5.0804)\tPrec@1 31.250 (23.492)\tPrec@5 48.828 (39.646)\n",
      "Epoch: [0][180/19336]\tTime 6.150 (6.610)\tData 0.013 (0.032)\tLoss 4.4952 (5.0381)\tPrec@1 29.883 (23.790)\tPrec@5 46.094 (40.115)\n",
      "Epoch: [0][190/19336]\tTime 6.093 (6.584)\tData 0.013 (0.031)\tLoss 4.2103 (4.9981)\tPrec@1 29.883 (24.080)\tPrec@5 47.656 (40.541)\n",
      "Epoch: [0][200/19336]\tTime 6.162 (6.560)\tData 0.013 (0.030)\tLoss 4.3424 (4.9614)\tPrec@1 30.859 (24.374)\tPrec@5 48.828 (40.931)\n",
      "Epoch: [0][210/19336]\tTime 6.052 (6.537)\tData 0.018 (0.030)\tLoss 4.3456 (4.9268)\tPrec@1 26.172 (24.630)\tPrec@5 48.242 (41.304)\n",
      "Epoch: [0][220/19336]\tTime 6.075 (6.518)\tData 0.013 (0.029)\tLoss 4.4014 (4.8935)\tPrec@1 26.953 (24.935)\tPrec@5 44.922 (41.703)\n",
      "Epoch: [0][230/19336]\tTime 6.096 (6.500)\tData 0.014 (0.028)\tLoss 4.0871 (4.8642)\tPrec@1 34.570 (25.194)\tPrec@5 52.148 (42.043)\n",
      "Epoch: [0][240/19336]\tTime 6.118 (6.483)\tData 0.018 (0.028)\tLoss 3.8607 (4.8331)\tPrec@1 33.008 (25.462)\tPrec@5 53.125 (42.417)\n",
      "Epoch: [0][250/19336]\tTime 6.121 (6.468)\tData 0.019 (0.027)\tLoss 4.1201 (4.8086)\tPrec@1 28.906 (25.671)\tPrec@5 48.047 (42.683)\n",
      "Epoch: [0][260/19336]\tTime 6.090 (6.454)\tData 0.021 (0.027)\tLoss 3.8640 (4.7836)\tPrec@1 34.961 (25.867)\tPrec@5 56.836 (42.964)\n",
      "Epoch: [0][270/19336]\tTime 6.086 (6.441)\tData 0.015 (0.027)\tLoss 3.9642 (4.7570)\tPrec@1 32.812 (26.069)\tPrec@5 50.977 (43.223)\n",
      "Epoch: [0][280/19336]\tTime 6.088 (6.429)\tData 0.019 (0.026)\tLoss 4.0207 (4.7313)\tPrec@1 30.859 (26.266)\tPrec@5 50.000 (43.475)\n",
      "Epoch: [0][290/19336]\tTime 6.125 (6.418)\tData 0.017 (0.026)\tLoss 3.9685 (4.7071)\tPrec@1 30.273 (26.434)\tPrec@5 54.102 (43.751)\n",
      "Epoch: [0][300/19336]\tTime 6.088 (6.408)\tData 0.014 (0.026)\tLoss 3.8244 (4.6817)\tPrec@1 36.328 (26.637)\tPrec@5 55.273 (44.002)\n",
      "Epoch: [0][310/19336]\tTime 6.119 (6.398)\tData 0.018 (0.025)\tLoss 3.9118 (4.6589)\tPrec@1 31.445 (26.828)\tPrec@5 50.391 (44.224)\n",
      "Epoch: [0][320/19336]\tTime 6.095 (6.389)\tData 0.015 (0.025)\tLoss 3.9622 (4.6391)\tPrec@1 34.180 (27.007)\tPrec@5 52.539 (44.455)\n",
      "Epoch: [0][330/19336]\tTime 6.102 (6.380)\tData 0.020 (0.025)\tLoss 3.8796 (4.6191)\tPrec@1 33.203 (27.183)\tPrec@5 52.930 (44.680)\n",
      "Epoch: [0][340/19336]\tTime 6.098 (6.372)\tData 0.013 (0.025)\tLoss 3.7199 (4.5984)\tPrec@1 36.328 (27.367)\tPrec@5 54.688 (44.894)\n",
      "Epoch: [0][350/19336]\tTime 6.091 (6.364)\tData 0.013 (0.024)\tLoss 3.9323 (4.5781)\tPrec@1 33.984 (27.544)\tPrec@5 54.102 (45.131)\n",
      "Epoch: [0][360/19336]\tTime 6.016 (6.357)\tData 0.015 (0.024)\tLoss 3.7917 (4.5581)\tPrec@1 33.398 (27.696)\tPrec@5 52.344 (45.327)\n",
      "Epoch: [0][370/19336]\tTime 6.119 (6.350)\tData 0.018 (0.024)\tLoss 3.9023 (4.5390)\tPrec@1 34.570 (27.853)\tPrec@5 51.758 (45.527)\n",
      "Epoch: [0][380/19336]\tTime 6.093 (6.344)\tData 0.015 (0.024)\tLoss 3.8757 (4.5212)\tPrec@1 33.789 (28.007)\tPrec@5 53.320 (45.732)\n",
      "Epoch: [0][390/19336]\tTime 6.120 (6.338)\tData 0.020 (0.024)\tLoss 3.7742 (4.5024)\tPrec@1 33.398 (28.199)\tPrec@5 56.055 (45.940)\n",
      "Epoch: [0][400/19336]\tTime 6.091 (6.332)\tData 0.017 (0.023)\tLoss 3.8836 (4.4858)\tPrec@1 35.156 (28.374)\tPrec@5 54.688 (46.127)\n",
      "Epoch: [0][410/19336]\tTime 6.133 (6.326)\tData 0.018 (0.023)\tLoss 4.0315 (4.4698)\tPrec@1 33.203 (28.528)\tPrec@5 52.344 (46.313)\n",
      "Epoch: [0][420/19336]\tTime 6.137 (6.321)\tData 0.014 (0.023)\tLoss 3.8502 (4.4531)\tPrec@1 35.156 (28.675)\tPrec@5 52.148 (46.493)\n",
      "Epoch: [0][430/19336]\tTime 6.094 (6.316)\tData 0.018 (0.023)\tLoss 3.8519 (4.4399)\tPrec@1 32.812 (28.775)\tPrec@5 52.539 (46.649)\n",
      "Epoch: [0][440/19336]\tTime 6.103 (6.311)\tData 0.022 (0.023)\tLoss 3.8012 (4.4310)\tPrec@1 36.328 (28.869)\tPrec@5 54.883 (46.785)\n",
      "Epoch: [0][450/19336]\tTime 6.105 (6.306)\tData 0.020 (0.023)\tLoss 3.7829 (4.4199)\tPrec@1 33.203 (28.962)\tPrec@5 52.930 (46.923)\n",
      "Epoch: [0][460/19336]\tTime 6.090 (6.302)\tData 0.023 (0.023)\tLoss 3.7548 (4.4099)\tPrec@1 35.938 (29.083)\tPrec@5 54.102 (47.077)\n",
      "Epoch: [0][470/19336]\tTime 6.102 (6.297)\tData 0.018 (0.023)\tLoss 3.7294 (4.4001)\tPrec@1 36.523 (29.177)\tPrec@5 54.688 (47.190)\n",
      "Epoch: [0][480/19336]\tTime 6.106 (6.293)\tData 0.018 (0.022)\tLoss 3.7567 (4.3907)\tPrec@1 33.008 (29.262)\tPrec@5 52.344 (47.283)\n",
      "Epoch: [0][490/19336]\tTime 6.092 (6.289)\tData 0.014 (0.022)\tLoss 3.9924 (4.3817)\tPrec@1 32.227 (29.339)\tPrec@5 54.102 (47.389)\n",
      "Epoch: [0][500/19336]\tTime 6.101 (6.286)\tData 0.018 (0.022)\tLoss 3.7651 (4.3718)\tPrec@1 34.375 (29.418)\tPrec@5 53.516 (47.500)\n",
      "Epoch: [0][510/19336]\tTime 6.098 (6.282)\tData 0.015 (0.022)\tLoss 3.7948 (4.3632)\tPrec@1 34.570 (29.494)\tPrec@5 54.883 (47.609)\n",
      "Epoch: [0][520/19336]\tTime 6.077 (6.278)\tData 0.014 (0.022)\tLoss 3.9682 (4.3535)\tPrec@1 34.180 (29.578)\tPrec@5 51.758 (47.718)\n",
      "Epoch: [0][530/19336]\tTime 6.067 (6.275)\tData 0.014 (0.022)\tLoss 4.0237 (4.3489)\tPrec@1 33.789 (29.643)\tPrec@5 53.711 (47.813)\n",
      "Epoch: [0][540/19336]\tTime 6.102 (6.272)\tData 0.016 (0.022)\tLoss 4.0294 (4.3405)\tPrec@1 32.422 (29.725)\tPrec@5 50.977 (47.907)\n",
      "Epoch: [0][550/19336]\tTime 6.105 (6.269)\tData 0.017 (0.022)\tLoss 3.7798 (4.3333)\tPrec@1 33.984 (29.782)\tPrec@5 54.883 (47.987)\n",
      "Epoch: [0][560/19336]\tTime 6.132 (6.266)\tData 0.021 (0.022)\tLoss 3.8961 (4.3252)\tPrec@1 31.641 (29.866)\tPrec@5 52.930 (48.074)\n",
      "Epoch: [0][570/19336]\tTime 6.113 (6.263)\tData 0.014 (0.022)\tLoss 3.6648 (4.3165)\tPrec@1 37.695 (29.942)\tPrec@5 55.469 (48.182)\n",
      "Epoch: [0][580/19336]\tTime 6.105 (6.260)\tData 0.017 (0.021)\tLoss 3.8018 (4.3082)\tPrec@1 34.375 (30.022)\tPrec@5 52.734 (48.265)\n",
      "Epoch: [0][590/19336]\tTime 6.092 (6.258)\tData 0.016 (0.021)\tLoss 4.1407 (4.3000)\tPrec@1 30.469 (30.090)\tPrec@5 48.828 (48.359)\n",
      "Epoch: [0][600/19336]\tTime 6.084 (6.255)\tData 0.022 (0.021)\tLoss 3.6574 (4.2901)\tPrec@1 36.719 (30.190)\tPrec@5 55.078 (48.467)\n",
      "Epoch: [0][610/19336]\tTime 6.081 (6.252)\tData 0.015 (0.021)\tLoss 3.7717 (4.2820)\tPrec@1 32.031 (30.248)\tPrec@5 54.883 (48.557)\n",
      "Epoch: [0][620/19336]\tTime 6.109 (6.250)\tData 0.018 (0.021)\tLoss 3.8041 (4.2729)\tPrec@1 33.203 (30.331)\tPrec@5 53.906 (48.657)\n",
      "Epoch: [0][630/19336]\tTime 6.072 (6.247)\tData 0.016 (0.021)\tLoss 3.6569 (4.2645)\tPrec@1 39.062 (30.398)\tPrec@5 58.594 (48.743)\n",
      "Epoch: [0][640/19336]\tTime 6.090 (6.245)\tData 0.017 (0.021)\tLoss 3.7928 (4.2569)\tPrec@1 33.789 (30.476)\tPrec@5 53.320 (48.827)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0][650/19336]\tTime 6.095 (6.243)\tData 0.019 (0.021)\tLoss 3.7374 (4.2490)\tPrec@1 31.445 (30.539)\tPrec@5 56.055 (48.919)\n",
      "Epoch: [0][660/19336]\tTime 6.126 (6.241)\tData 0.052 (0.021)\tLoss 3.6472 (4.2393)\tPrec@1 38.477 (30.635)\tPrec@5 56.641 (49.032)\n",
      "Epoch: [0][670/19336]\tTime 6.118 (6.239)\tData 0.015 (0.021)\tLoss 3.9780 (4.2313)\tPrec@1 32.227 (30.709)\tPrec@5 51.172 (49.124)\n",
      "Epoch: [0][680/19336]\tTime 6.109 (6.237)\tData 0.019 (0.021)\tLoss 3.6463 (4.2227)\tPrec@1 35.156 (30.786)\tPrec@5 57.031 (49.226)\n",
      "Epoch: [0][690/19336]\tTime 6.106 (6.235)\tData 0.019 (0.021)\tLoss 3.3565 (4.2139)\tPrec@1 37.891 (30.855)\tPrec@5 58.398 (49.319)\n",
      "Epoch: [0][700/19336]\tTime 6.097 (6.233)\tData 0.015 (0.021)\tLoss 3.5193 (4.2048)\tPrec@1 36.133 (30.941)\tPrec@5 55.273 (49.428)\n",
      "Epoch: [0][710/19336]\tTime 6.115 (6.231)\tData 0.015 (0.021)\tLoss 3.6187 (4.1965)\tPrec@1 35.156 (31.024)\tPrec@5 54.883 (49.527)\n",
      "Epoch: [0][720/19336]\tTime 6.081 (6.229)\tData 0.015 (0.021)\tLoss 3.4950 (4.1881)\tPrec@1 39.258 (31.116)\tPrec@5 58.008 (49.628)\n",
      "Epoch: [0][730/19336]\tTime 6.101 (6.228)\tData 0.015 (0.021)\tLoss 3.5227 (4.1796)\tPrec@1 39.062 (31.199)\tPrec@5 59.375 (49.718)\n",
      "Epoch: [0][740/19336]\tTime 6.182 (6.226)\tData 0.015 (0.021)\tLoss 3.8852 (4.1725)\tPrec@1 33.789 (31.260)\tPrec@5 53.516 (49.803)\n",
      "Epoch: [0][750/19336]\tTime 6.097 (6.225)\tData 0.021 (0.021)\tLoss 3.5206 (4.1641)\tPrec@1 38.672 (31.338)\tPrec@5 56.641 (49.892)\n",
      "Epoch: [0][760/19336]\tTime 6.113 (6.223)\tData 0.019 (0.021)\tLoss 3.3857 (4.1560)\tPrec@1 37.695 (31.427)\tPrec@5 58.008 (49.980)\n",
      "Epoch: [0][770/19336]\tTime 6.096 (6.222)\tData 0.017 (0.021)\tLoss 3.6996 (4.1490)\tPrec@1 33.984 (31.488)\tPrec@5 54.688 (50.053)\n",
      "Epoch: [0][780/19336]\tTime 6.099 (6.220)\tData 0.019 (0.021)\tLoss 3.4983 (4.1414)\tPrec@1 38.086 (31.550)\tPrec@5 56.836 (50.138)\n",
      "Epoch: [0][790/19336]\tTime 6.127 (6.219)\tData 0.018 (0.021)\tLoss 3.2866 (4.1336)\tPrec@1 41.211 (31.632)\tPrec@5 62.109 (50.240)\n",
      "Epoch: [0][800/19336]\tTime 6.106 (6.217)\tData 0.022 (0.021)\tLoss 3.3760 (4.1256)\tPrec@1 39.648 (31.711)\tPrec@5 58.984 (50.344)\n",
      "Epoch: [0][810/19336]\tTime 6.118 (6.216)\tData 0.023 (0.021)\tLoss 3.6139 (4.1180)\tPrec@1 36.719 (31.783)\tPrec@5 55.859 (50.427)\n",
      "Epoch: [0][820/19336]\tTime 6.091 (6.215)\tData 0.022 (0.021)\tLoss 3.4797 (4.1111)\tPrec@1 36.719 (31.855)\tPrec@5 57.812 (50.499)\n",
      "Epoch: [0][830/19336]\tTime 6.118 (6.214)\tData 0.019 (0.021)\tLoss 3.7333 (4.1040)\tPrec@1 35.938 (31.935)\tPrec@5 54.883 (50.581)\n",
      "Epoch: [0][840/19336]\tTime 6.101 (6.213)\tData 0.026 (0.021)\tLoss 3.6925 (4.0976)\tPrec@1 36.328 (32.003)\tPrec@5 53.516 (50.656)\n",
      "Epoch: [0][850/19336]\tTime 6.122 (6.211)\tData 0.018 (0.021)\tLoss 3.6224 (4.0908)\tPrec@1 38.281 (32.077)\tPrec@5 55.469 (50.741)\n",
      "Epoch: [0][860/19336]\tTime 6.098 (6.210)\tData 0.022 (0.021)\tLoss 3.2958 (4.0848)\tPrec@1 38.477 (32.133)\tPrec@5 59.375 (50.811)\n",
      "Epoch: [0][870/19336]\tTime 6.069 (6.209)\tData 0.022 (0.021)\tLoss 3.5850 (4.0783)\tPrec@1 36.719 (32.201)\tPrec@5 58.008 (50.887)\n",
      "Epoch: [0][880/19336]\tTime 6.091 (6.208)\tData 0.021 (0.021)\tLoss 3.5330 (4.0720)\tPrec@1 38.672 (32.268)\tPrec@5 58.203 (50.957)\n",
      "Epoch: [0][890/19336]\tTime 6.128 (6.207)\tData 0.029 (0.021)\tLoss 3.6251 (4.0666)\tPrec@1 37.500 (32.317)\tPrec@5 55.664 (51.021)\n",
      "Epoch: [0][900/19336]\tTime 6.104 (6.206)\tData 0.022 (0.021)\tLoss 3.4865 (4.0609)\tPrec@1 38.281 (32.359)\tPrec@5 58.594 (51.085)\n",
      "Epoch: [0][910/19336]\tTime 6.125 (6.205)\tData 0.025 (0.021)\tLoss 3.4459 (4.0551)\tPrec@1 40.039 (32.417)\tPrec@5 59.180 (51.155)\n",
      "Epoch: [0][920/19336]\tTime 6.109 (6.204)\tData 0.019 (0.021)\tLoss 3.6183 (4.0485)\tPrec@1 36.914 (32.488)\tPrec@5 54.883 (51.225)\n",
      "Epoch: [0][930/19336]\tTime 6.120 (6.203)\tData 0.025 (0.021)\tLoss 3.1365 (4.0422)\tPrec@1 41.406 (32.545)\tPrec@5 62.305 (51.299)\n",
      "Epoch: [0][940/19336]\tTime 6.114 (6.202)\tData 0.018 (0.021)\tLoss 3.3308 (4.0362)\tPrec@1 40.430 (32.605)\tPrec@5 58.203 (51.367)\n",
      "Epoch: [0][950/19336]\tTime 6.118 (6.201)\tData 0.023 (0.021)\tLoss 3.5289 (4.0308)\tPrec@1 39.258 (32.665)\tPrec@5 55.664 (51.429)\n",
      "Epoch: [0][960/19336]\tTime 6.129 (6.200)\tData 0.018 (0.021)\tLoss 3.4679 (4.0252)\tPrec@1 39.844 (32.714)\tPrec@5 57.617 (51.492)\n",
      "Epoch: [0][970/19336]\tTime 6.130 (6.199)\tData 0.023 (0.021)\tLoss 3.5436 (4.0198)\tPrec@1 36.523 (32.766)\tPrec@5 56.641 (51.556)\n",
      "Epoch: [0][980/19336]\tTime 6.129 (6.199)\tData 0.018 (0.021)\tLoss 3.6615 (4.0142)\tPrec@1 37.109 (32.825)\tPrec@5 56.836 (51.621)\n",
      "Epoch: [0][990/19336]\tTime 6.098 (6.198)\tData 0.025 (0.021)\tLoss 3.5427 (4.0090)\tPrec@1 39.453 (32.878)\tPrec@5 57.812 (51.679)\n",
      "Epoch: [0][1000/19336]\tTime 6.104 (6.197)\tData 0.021 (0.021)\tLoss 3.4767 (4.0038)\tPrec@1 38.867 (32.930)\tPrec@5 56.445 (51.735)\n",
      "Epoch: [0][1010/19336]\tTime 6.104 (6.197)\tData 0.021 (0.022)\tLoss 3.5305 (3.9986)\tPrec@1 39.062 (32.978)\tPrec@5 57.031 (51.793)\n",
      "Epoch: [0][1020/19336]\tTime 6.120 (6.197)\tData 0.020 (0.022)\tLoss 3.2494 (3.9936)\tPrec@1 39.062 (33.032)\tPrec@5 60.742 (51.849)\n",
      "Epoch: [0][1030/19336]\tTime 6.113 (6.196)\tData 0.019 (0.022)\tLoss 3.5678 (3.9886)\tPrec@1 37.891 (33.080)\tPrec@5 58.398 (51.908)\n",
      "Epoch: [0][1040/19336]\tTime 6.121 (6.195)\tData 0.020 (0.022)\tLoss 3.5347 (3.9830)\tPrec@1 37.891 (33.134)\tPrec@5 56.836 (51.969)\n",
      "Epoch: [0][1050/19336]\tTime 6.121 (6.194)\tData 0.022 (0.022)\tLoss 3.2160 (3.9785)\tPrec@1 41.602 (33.176)\tPrec@5 61.133 (52.022)\n",
      "Epoch: [0][1060/19336]\tTime 6.116 (6.194)\tData 0.026 (0.022)\tLoss 3.3250 (3.9736)\tPrec@1 39.844 (33.225)\tPrec@5 57.422 (52.074)\n",
      "Epoch: [0][1070/19336]\tTime 6.129 (6.193)\tData 0.023 (0.022)\tLoss 3.4350 (3.9690)\tPrec@1 38.672 (33.279)\tPrec@5 57.227 (52.135)\n",
      "Epoch: [0][1080/19336]\tTime 6.104 (6.193)\tData 0.024 (0.022)\tLoss 3.2515 (3.9643)\tPrec@1 42.188 (33.338)\tPrec@5 61.523 (52.191)\n",
      "Epoch: [0][1090/19336]\tTime 6.106 (6.192)\tData 0.019 (0.022)\tLoss 3.4532 (3.9597)\tPrec@1 37.109 (33.388)\tPrec@5 59.961 (52.247)\n",
      "Epoch: [0][1100/19336]\tTime 6.130 (6.191)\tData 0.024 (0.022)\tLoss 3.7796 (3.9557)\tPrec@1 36.914 (33.429)\tPrec@5 54.492 (52.290)\n",
      "Epoch: [0][1110/19336]\tTime 6.114 (6.190)\tData 0.022 (0.022)\tLoss 3.4038 (3.9510)\tPrec@1 38.281 (33.479)\tPrec@5 59.570 (52.349)\n",
      "Epoch: [0][1120/19336]\tTime 6.113 (6.190)\tData 0.022 (0.022)\tLoss 3.3205 (3.9464)\tPrec@1 42.773 (33.526)\tPrec@5 58.594 (52.401)\n",
      "Epoch: [0][1130/19336]\tTime 6.117 (6.189)\tData 0.024 (0.022)\tLoss 3.3400 (3.9417)\tPrec@1 40.625 (33.581)\tPrec@5 58.789 (52.461)\n",
      "Epoch: [0][1140/19336]\tTime 6.112 (6.189)\tData 0.023 (0.022)\tLoss 3.4218 (3.9369)\tPrec@1 37.695 (33.636)\tPrec@5 58.008 (52.517)\n",
      "Epoch: [0][1150/19336]\tTime 6.091 (6.188)\tData 0.024 (0.022)\tLoss 3.5614 (3.9324)\tPrec@1 37.500 (33.678)\tPrec@5 56.445 (52.567)\n",
      "Epoch: [0][1160/19336]\tTime 6.093 (6.187)\tData 0.020 (0.022)\tLoss 3.6492 (3.9281)\tPrec@1 35.547 (33.726)\tPrec@5 57.617 (52.619)\n",
      "Epoch: [0][1170/19336]\tTime 6.120 (6.187)\tData 0.045 (0.022)\tLoss 3.2225 (3.9241)\tPrec@1 38.867 (33.766)\tPrec@5 61.523 (52.666)\n",
      "Epoch: [0][1180/19336]\tTime 6.090 (6.186)\tData 0.016 (0.022)\tLoss 3.1975 (3.9199)\tPrec@1 39.062 (33.810)\tPrec@5 60.547 (52.716)\n",
      "Epoch: [0][1190/19336]\tTime 6.126 (6.186)\tData 0.024 (0.022)\tLoss 3.5092 (3.9158)\tPrec@1 38.477 (33.854)\tPrec@5 59.180 (52.766)\n",
      "Epoch: [0][1200/19336]\tTime 6.105 (6.185)\tData 0.021 (0.022)\tLoss 3.3275 (3.9111)\tPrec@1 36.914 (33.906)\tPrec@5 58.789 (52.819)\n",
      "Epoch: [0][1210/19336]\tTime 6.117 (6.185)\tData 0.020 (0.022)\tLoss 3.2983 (3.9069)\tPrec@1 41.211 (33.947)\tPrec@5 61.914 (52.868)\n",
      "Epoch: [0][1220/19336]\tTime 6.130 (6.184)\tData 0.026 (0.022)\tLoss 3.4444 (3.9030)\tPrec@1 39.648 (33.988)\tPrec@5 59.570 (52.911)\n",
      "Epoch: [0][1230/19336]\tTime 6.084 (6.184)\tData 0.020 (0.022)\tLoss 3.4038 (3.8990)\tPrec@1 40.625 (34.025)\tPrec@5 57.812 (52.955)\n",
      "Epoch: [0][1240/19336]\tTime 6.115 (6.183)\tData 0.025 (0.022)\tLoss 3.4713 (3.8956)\tPrec@1 39.062 (34.061)\tPrec@5 59.180 (52.993)\n",
      "Epoch: [0][1250/19336]\tTime 6.130 (6.183)\tData 0.028 (0.022)\tLoss 3.3890 (3.8912)\tPrec@1 40.039 (34.106)\tPrec@5 60.156 (53.047)\n",
      "Epoch: [0][1260/19336]\tTime 6.136 (6.182)\tData 0.026 (0.022)\tLoss 3.4959 (3.8872)\tPrec@1 38.281 (34.149)\tPrec@5 56.445 (53.097)\n",
      "Epoch: [0][1270/19336]\tTime 6.129 (6.182)\tData 0.024 (0.022)\tLoss 3.5417 (3.8835)\tPrec@1 35.352 (34.193)\tPrec@5 55.664 (53.141)\n",
      "Epoch: [0][1280/19336]\tTime 6.107 (6.181)\tData 0.027 (0.022)\tLoss 3.2106 (3.8794)\tPrec@1 40.820 (34.232)\tPrec@5 60.938 (53.185)\n",
      "Epoch: [0][1290/19336]\tTime 6.138 (6.181)\tData 0.020 (0.022)\tLoss 3.5270 (3.8755)\tPrec@1 37.695 (34.268)\tPrec@5 55.469 (53.234)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0][1300/19336]\tTime 6.128 (6.180)\tData 0.022 (0.022)\tLoss 3.3229 (3.8722)\tPrec@1 40.234 (34.300)\tPrec@5 59.375 (53.268)\n",
      "Epoch: [0][1310/19336]\tTime 6.093 (6.180)\tData 0.020 (0.022)\tLoss 3.5769 (3.8684)\tPrec@1 36.523 (34.337)\tPrec@5 56.055 (53.312)\n",
      "Epoch: [0][1320/19336]\tTime 6.097 (6.179)\tData 0.023 (0.022)\tLoss 3.2891 (3.8649)\tPrec@1 40.625 (34.374)\tPrec@5 58.398 (53.354)\n",
      "Epoch: [0][1330/19336]\tTime 6.113 (6.179)\tData 0.024 (0.022)\tLoss 3.6493 (3.8616)\tPrec@1 34.375 (34.402)\tPrec@5 53.516 (53.383)\n",
      "Epoch: [0][1340/19336]\tTime 6.115 (6.179)\tData 0.021 (0.022)\tLoss 3.2463 (3.8584)\tPrec@1 39.844 (34.430)\tPrec@5 59.180 (53.424)\n",
      "Epoch: [0][1350/19336]\tTime 6.125 (6.178)\tData 0.039 (0.022)\tLoss 3.5842 (3.8547)\tPrec@1 37.109 (34.468)\tPrec@5 56.055 (53.467)\n",
      "Epoch: [0][1360/19336]\tTime 6.151 (6.178)\tData 0.027 (0.022)\tLoss 3.2471 (3.8512)\tPrec@1 41.016 (34.502)\tPrec@5 59.375 (53.508)\n",
      "Epoch: [0][1370/19336]\tTime 6.157 (6.177)\tData 0.021 (0.022)\tLoss 3.2409 (3.8481)\tPrec@1 41.016 (34.534)\tPrec@5 59.180 (53.548)\n",
      "Epoch: [0][1380/19336]\tTime 6.128 (6.177)\tData 0.022 (0.022)\tLoss 3.1871 (3.8445)\tPrec@1 42.188 (34.568)\tPrec@5 60.938 (53.587)\n",
      "Epoch: [0][1390/19336]\tTime 6.129 (6.177)\tData 0.027 (0.022)\tLoss 3.2562 (3.8412)\tPrec@1 39.844 (34.601)\tPrec@5 61.328 (53.629)\n",
      "Epoch: [0][1400/19336]\tTime 6.112 (6.176)\tData 0.021 (0.022)\tLoss 3.2537 (3.8382)\tPrec@1 40.820 (34.629)\tPrec@5 60.938 (53.663)\n",
      "Epoch: [0][1410/19336]\tTime 6.108 (6.176)\tData 0.023 (0.022)\tLoss 3.0408 (3.8348)\tPrec@1 42.188 (34.667)\tPrec@5 65.234 (53.704)\n",
      "Epoch: [0][1420/19336]\tTime 6.122 (6.176)\tData 0.021 (0.022)\tLoss 3.2632 (3.8312)\tPrec@1 39.062 (34.699)\tPrec@5 62.305 (53.749)\n",
      "Epoch: [0][1430/19336]\tTime 6.100 (6.175)\tData 0.021 (0.022)\tLoss 3.1729 (3.8275)\tPrec@1 41.016 (34.733)\tPrec@5 61.133 (53.795)\n",
      "Epoch: [0][1440/19336]\tTime 6.091 (6.175)\tData 0.021 (0.022)\tLoss 3.1924 (3.8240)\tPrec@1 39.258 (34.770)\tPrec@5 59.766 (53.835)\n",
      "Epoch: [0][1450/19336]\tTime 6.122 (6.174)\tData 0.026 (0.022)\tLoss 3.1213 (3.8209)\tPrec@1 40.625 (34.803)\tPrec@5 64.062 (53.877)\n",
      "Epoch: [0][1460/19336]\tTime 6.103 (6.174)\tData 0.024 (0.022)\tLoss 3.4391 (3.8179)\tPrec@1 40.820 (34.838)\tPrec@5 58.008 (53.915)\n",
      "Epoch: [0][1470/19336]\tTime 6.123 (6.174)\tData 0.025 (0.022)\tLoss 3.6766 (3.8154)\tPrec@1 36.133 (34.865)\tPrec@5 57.617 (53.943)\n",
      "Epoch: [0][1480/19336]\tTime 6.066 (6.173)\tData 0.026 (0.022)\tLoss 3.5908 (3.8123)\tPrec@1 35.156 (34.897)\tPrec@5 56.250 (53.985)\n",
      "Epoch: [0][1490/19336]\tTime 6.133 (6.173)\tData 0.024 (0.022)\tLoss 3.2460 (3.8095)\tPrec@1 41.992 (34.928)\tPrec@5 60.938 (54.013)\n",
      "Epoch: [0][1500/19336]\tTime 6.102 (6.173)\tData 0.022 (0.022)\tLoss 3.4039 (3.8062)\tPrec@1 39.844 (34.963)\tPrec@5 59.570 (54.052)\n",
      "Epoch: [0][1510/19336]\tTime 6.142 (6.172)\tData 0.028 (0.022)\tLoss 3.3533 (3.8037)\tPrec@1 41.016 (34.990)\tPrec@5 59.375 (54.084)\n",
      "Epoch: [0][1520/19336]\tTime 6.060 (6.172)\tData 0.026 (0.022)\tLoss 3.3047 (3.8008)\tPrec@1 41.797 (35.024)\tPrec@5 57.812 (54.120)\n",
      "Epoch: [0][1530/19336]\tTime 6.134 (6.172)\tData 0.025 (0.022)\tLoss 3.3384 (3.7979)\tPrec@1 39.648 (35.056)\tPrec@5 62.109 (54.156)\n",
      "Epoch: [0][1540/19336]\tTime 6.142 (6.171)\tData 0.028 (0.022)\tLoss 3.6183 (3.7956)\tPrec@1 39.258 (35.080)\tPrec@5 56.836 (54.183)\n",
      "Epoch: [0][1550/19336]\tTime 6.140 (6.171)\tData 0.024 (0.023)\tLoss 3.2949 (3.7930)\tPrec@1 39.453 (35.107)\tPrec@5 60.156 (54.216)\n",
      "Epoch: [0][1560/19336]\tTime 6.115 (6.171)\tData 0.030 (0.023)\tLoss 3.1867 (3.7903)\tPrec@1 40.430 (35.135)\tPrec@5 62.109 (54.243)\n",
      "Epoch: [0][1570/19336]\tTime 6.176 (6.171)\tData 0.022 (0.023)\tLoss 3.3599 (3.7878)\tPrec@1 38.281 (35.161)\tPrec@5 58.984 (54.274)\n",
      "Epoch: [0][1580/19336]\tTime 6.105 (6.170)\tData 0.022 (0.023)\tLoss 3.2881 (3.7852)\tPrec@1 41.016 (35.183)\tPrec@5 61.328 (54.308)\n",
      "Epoch: [0][1590/19336]\tTime 6.133 (6.170)\tData 0.024 (0.023)\tLoss 3.4466 (3.7825)\tPrec@1 36.719 (35.211)\tPrec@5 58.008 (54.342)\n",
      "Epoch: [0][1600/19336]\tTime 6.126 (6.170)\tData 0.028 (0.023)\tLoss 3.3300 (3.7800)\tPrec@1 41.211 (35.235)\tPrec@5 61.133 (54.369)\n",
      "Epoch: [0][1610/19336]\tTime 6.119 (6.169)\tData 0.023 (0.023)\tLoss 3.3084 (3.7776)\tPrec@1 39.453 (35.253)\tPrec@5 58.984 (54.396)\n",
      "Epoch: [0][1620/19336]\tTime 6.091 (6.169)\tData 0.023 (0.023)\tLoss 3.2384 (3.7747)\tPrec@1 40.430 (35.289)\tPrec@5 61.719 (54.434)\n",
      "Epoch: [0][1630/19336]\tTime 6.101 (6.169)\tData 0.025 (0.023)\tLoss 3.4149 (3.7720)\tPrec@1 41.016 (35.323)\tPrec@5 57.227 (54.465)\n",
      "Epoch: [0][1640/19336]\tTime 6.149 (6.169)\tData 0.035 (0.023)\tLoss 3.1863 (3.7692)\tPrec@1 41.992 (35.353)\tPrec@5 60.938 (54.493)\n",
      "Epoch: [0][1650/19336]\tTime 6.138 (6.169)\tData 0.023 (0.023)\tLoss 3.1925 (3.7662)\tPrec@1 42.188 (35.392)\tPrec@5 61.328 (54.531)\n",
      "Epoch: [0][1660/19336]\tTime 6.172 (6.168)\tData 0.026 (0.023)\tLoss 3.4151 (3.7635)\tPrec@1 36.914 (35.418)\tPrec@5 57.422 (54.560)\n",
      "Epoch: [0][1670/19336]\tTime 6.122 (6.168)\tData 0.029 (0.023)\tLoss 3.2753 (3.7607)\tPrec@1 39.648 (35.448)\tPrec@5 56.445 (54.592)\n",
      "Epoch: [0][1680/19336]\tTime 6.116 (6.168)\tData 0.033 (0.023)\tLoss 3.2522 (3.7580)\tPrec@1 40.234 (35.471)\tPrec@5 60.156 (54.625)\n",
      "Epoch: [0][1690/19336]\tTime 6.118 (6.168)\tData 0.023 (0.023)\tLoss 3.3114 (3.7556)\tPrec@1 38.867 (35.495)\tPrec@5 59.180 (54.651)\n",
      "Epoch: [0][1700/19336]\tTime 6.116 (6.167)\tData 0.032 (0.023)\tLoss 3.3553 (3.7526)\tPrec@1 37.695 (35.527)\tPrec@5 59.180 (54.686)\n",
      "Epoch: [0][1710/19336]\tTime 6.141 (6.167)\tData 0.035 (0.023)\tLoss 3.4872 (3.7501)\tPrec@1 39.453 (35.554)\tPrec@5 58.594 (54.717)\n",
      "Epoch: [0][1720/19336]\tTime 6.122 (6.167)\tData 0.028 (0.023)\tLoss 3.0552 (3.7475)\tPrec@1 41.406 (35.584)\tPrec@5 63.672 (54.749)\n",
      "Epoch: [0][1730/19336]\tTime 6.115 (6.167)\tData 0.023 (0.023)\tLoss 3.2429 (3.7447)\tPrec@1 41.602 (35.615)\tPrec@5 60.938 (54.787)\n",
      "Epoch: [0][1740/19336]\tTime 6.116 (6.166)\tData 0.030 (0.023)\tLoss 3.3644 (3.7424)\tPrec@1 40.430 (35.635)\tPrec@5 58.984 (54.815)\n",
      "Epoch: [0][1750/19336]\tTime 6.095 (6.166)\tData 0.029 (0.023)\tLoss 3.2610 (3.7399)\tPrec@1 39.648 (35.665)\tPrec@5 63.086 (54.846)\n",
      "Epoch: [0][1760/19336]\tTime 6.123 (6.166)\tData 0.023 (0.023)\tLoss 3.1977 (3.7373)\tPrec@1 42.578 (35.687)\tPrec@5 59.375 (54.875)\n",
      "Epoch: [0][1770/19336]\tTime 6.119 (6.166)\tData 0.023 (0.023)\tLoss 3.6116 (3.7350)\tPrec@1 36.719 (35.712)\tPrec@5 56.055 (54.906)\n",
      "Epoch: [0][1780/19336]\tTime 6.158 (6.165)\tData 0.029 (0.023)\tLoss 3.1145 (3.7329)\tPrec@1 41.016 (35.735)\tPrec@5 63.086 (54.930)\n",
      "Epoch: [0][1790/19336]\tTime 6.124 (6.165)\tData 0.024 (0.023)\tLoss 3.2402 (3.7302)\tPrec@1 40.430 (35.762)\tPrec@5 62.695 (54.963)\n",
      "Epoch: [0][1800/19336]\tTime 6.153 (6.165)\tData 0.031 (0.023)\tLoss 3.3449 (3.7278)\tPrec@1 39.648 (35.781)\tPrec@5 58.594 (54.987)\n",
      "Epoch: [0][1810/19336]\tTime 6.120 (6.165)\tData 0.028 (0.023)\tLoss 3.4321 (3.7255)\tPrec@1 37.305 (35.805)\tPrec@5 58.594 (55.018)\n",
      "Epoch: [0][1820/19336]\tTime 6.118 (6.165)\tData 0.024 (0.023)\tLoss 3.1491 (3.7228)\tPrec@1 42.773 (35.835)\tPrec@5 60.547 (55.050)\n",
      "Epoch: [0][1830/19336]\tTime 6.155 (6.164)\tData 0.025 (0.023)\tLoss 3.1702 (3.7202)\tPrec@1 44.336 (35.868)\tPrec@5 61.523 (55.081)\n",
      "Epoch: [0][1840/19336]\tTime 6.122 (6.164)\tData 0.031 (0.023)\tLoss 3.2440 (3.7176)\tPrec@1 39.258 (35.895)\tPrec@5 62.305 (55.111)\n",
      "Epoch: [0][1850/19336]\tTime 6.148 (6.164)\tData 0.044 (0.023)\tLoss 3.5196 (3.7154)\tPrec@1 39.648 (35.920)\tPrec@5 57.227 (55.136)\n",
      "Epoch: [0][1860/19336]\tTime 6.101 (6.164)\tData 0.025 (0.023)\tLoss 3.1481 (3.7132)\tPrec@1 43.555 (35.943)\tPrec@5 63.086 (55.162)\n",
      "Epoch: [0][1870/19336]\tTime 6.128 (6.164)\tData 0.027 (0.023)\tLoss 3.3254 (3.7110)\tPrec@1 41.406 (35.969)\tPrec@5 60.547 (55.188)\n",
      "Epoch: [0][1880/19336]\tTime 6.129 (6.164)\tData 0.024 (0.023)\tLoss 3.1323 (3.7087)\tPrec@1 41.406 (35.993)\tPrec@5 61.914 (55.215)\n",
      "Epoch: [0][1890/19336]\tTime 6.138 (6.163)\tData 0.030 (0.023)\tLoss 3.1242 (3.7063)\tPrec@1 42.773 (36.022)\tPrec@5 61.719 (55.242)\n",
      "Epoch: [0][1900/19336]\tTime 6.119 (6.163)\tData 0.028 (0.023)\tLoss 3.1157 (3.7037)\tPrec@1 41.602 (36.050)\tPrec@5 61.914 (55.274)\n",
      "Epoch: [0][1910/19336]\tTime 6.108 (6.163)\tData 0.031 (0.023)\tLoss 3.4315 (3.7014)\tPrec@1 39.648 (36.080)\tPrec@5 57.227 (55.303)\n",
      "Epoch: [0][1920/19336]\tTime 6.132 (6.163)\tData 0.025 (0.023)\tLoss 3.3725 (3.6994)\tPrec@1 41.016 (36.104)\tPrec@5 59.766 (55.327)\n",
      "Epoch: [0][1930/19336]\tTime 6.102 (6.162)\tData 0.024 (0.023)\tLoss 3.1442 (3.6970)\tPrec@1 42.969 (36.132)\tPrec@5 61.328 (55.352)\n",
      "Epoch: [0][1940/19336]\tTime 6.136 (6.162)\tData 0.030 (0.023)\tLoss 2.9667 (3.6946)\tPrec@1 43.164 (36.160)\tPrec@5 64.062 (55.379)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0][1950/19336]\tTime 6.123 (6.162)\tData 0.031 (0.023)\tLoss 3.4217 (3.6925)\tPrec@1 38.477 (36.182)\tPrec@5 58.789 (55.405)\n",
      "Epoch: [0][1960/19336]\tTime 6.130 (6.162)\tData 0.031 (0.024)\tLoss 3.3537 (3.6904)\tPrec@1 41.797 (36.209)\tPrec@5 59.570 (55.433)\n",
      "Epoch: [0][1970/19336]\tTime 6.131 (6.162)\tData 0.038 (0.024)\tLoss 3.4758 (3.6886)\tPrec@1 36.523 (36.223)\tPrec@5 57.617 (55.451)\n",
      "Epoch: [0][1980/19336]\tTime 6.110 (6.162)\tData 0.026 (0.024)\tLoss 3.2077 (3.6862)\tPrec@1 42.188 (36.246)\tPrec@5 62.891 (55.481)\n",
      "Epoch: [0][1990/19336]\tTime 6.106 (6.161)\tData 0.035 (0.024)\tLoss 3.2050 (3.6839)\tPrec@1 42.969 (36.266)\tPrec@5 59.961 (55.506)\n",
      "Epoch: [0][2000/19336]\tTime 6.121 (6.161)\tData 0.028 (0.024)\tLoss 3.1017 (3.6816)\tPrec@1 42.188 (36.293)\tPrec@5 62.109 (55.535)\n",
      "Epoch: [0][2010/19336]\tTime 6.144 (6.161)\tData 0.025 (0.024)\tLoss 3.3886 (3.6796)\tPrec@1 40.625 (36.314)\tPrec@5 58.398 (55.556)\n",
      "Epoch: [0][2020/19336]\tTime 6.110 (6.161)\tData 0.024 (0.024)\tLoss 3.4032 (3.6776)\tPrec@1 40.039 (36.338)\tPrec@5 60.938 (55.582)\n",
      "Epoch: [0][2030/19336]\tTime 6.128 (6.161)\tData 0.027 (0.024)\tLoss 3.3675 (3.6755)\tPrec@1 39.258 (36.360)\tPrec@5 58.008 (55.608)\n",
      "Epoch: [0][2040/19336]\tTime 6.120 (6.161)\tData 0.032 (0.024)\tLoss 3.2382 (3.6739)\tPrec@1 39.258 (36.376)\tPrec@5 60.742 (55.629)\n",
      "Epoch: [0][2050/19336]\tTime 6.112 (6.161)\tData 0.032 (0.024)\tLoss 3.4494 (3.6720)\tPrec@1 41.797 (36.395)\tPrec@5 59.570 (55.651)\n",
      "Epoch: [0][2060/19336]\tTime 6.120 (6.161)\tData 0.030 (0.024)\tLoss 3.3443 (3.6703)\tPrec@1 41.797 (36.414)\tPrec@5 59.180 (55.670)\n",
      "Epoch: [0][2070/19336]\tTime 6.126 (6.161)\tData 0.026 (0.024)\tLoss 3.3285 (3.6684)\tPrec@1 38.672 (36.434)\tPrec@5 61.133 (55.690)\n",
      "Epoch: [0][2080/19336]\tTime 6.133 (6.160)\tData 0.025 (0.024)\tLoss 3.0714 (3.6661)\tPrec@1 44.531 (36.457)\tPrec@5 63.672 (55.717)\n",
      "Epoch: [0][2090/19336]\tTime 6.141 (6.160)\tData 0.025 (0.024)\tLoss 3.0975 (3.6638)\tPrec@1 44.922 (36.483)\tPrec@5 60.938 (55.743)\n",
      "Epoch: [0][2100/19336]\tTime 6.142 (6.160)\tData 0.028 (0.024)\tLoss 3.4895 (3.6622)\tPrec@1 37.305 (36.500)\tPrec@5 57.617 (55.763)\n",
      "Epoch: [0][2110/19336]\tTime 6.122 (6.160)\tData 0.033 (0.024)\tLoss 3.3188 (3.6601)\tPrec@1 40.430 (36.524)\tPrec@5 58.789 (55.789)\n",
      "Epoch: [0][2120/19336]\tTime 6.111 (6.160)\tData 0.034 (0.024)\tLoss 3.2358 (3.6577)\tPrec@1 39.844 (36.551)\tPrec@5 61.719 (55.818)\n",
      "Epoch: [0][2130/19336]\tTime 6.101 (6.160)\tData 0.035 (0.024)\tLoss 3.2719 (3.6557)\tPrec@1 40.820 (36.571)\tPrec@5 62.109 (55.839)\n",
      "Epoch: [0][2140/19336]\tTime 6.139 (6.159)\tData 0.035 (0.025)\tLoss 3.2265 (3.6539)\tPrec@1 39.062 (36.591)\tPrec@5 59.766 (55.860)\n",
      "Epoch: [0][2150/19336]\tTime 6.134 (6.159)\tData 0.031 (0.025)\tLoss 3.3160 (3.6521)\tPrec@1 44.336 (36.613)\tPrec@5 61.719 (55.884)\n",
      "Epoch: [0][2160/19336]\tTime 6.129 (6.159)\tData 0.034 (0.025)\tLoss 3.3997 (3.6503)\tPrec@1 40.039 (36.629)\tPrec@5 58.398 (55.906)\n",
      "Epoch: [0][2170/19336]\tTime 6.136 (6.159)\tData 0.029 (0.025)\tLoss 3.1029 (3.6483)\tPrec@1 43.164 (36.651)\tPrec@5 61.133 (55.927)\n",
      "Epoch: [0][2180/19336]\tTime 6.108 (6.159)\tData 0.027 (0.025)\tLoss 3.3422 (3.6465)\tPrec@1 36.719 (36.671)\tPrec@5 60.742 (55.947)\n",
      "Epoch: [0][2190/19336]\tTime 6.150 (6.159)\tData 0.027 (0.025)\tLoss 3.1094 (3.6446)\tPrec@1 42.773 (36.693)\tPrec@5 63.867 (55.973)\n",
      "Epoch: [0][2200/19336]\tTime 6.102 (6.158)\tData 0.030 (0.025)\tLoss 3.1658 (3.6427)\tPrec@1 41.992 (36.721)\tPrec@5 63.086 (55.997)\n",
      "Epoch: [0][2210/19336]\tTime 6.073 (6.158)\tData 0.027 (0.025)\tLoss 3.2772 (3.6407)\tPrec@1 41.992 (36.746)\tPrec@5 60.352 (56.018)\n",
      "Epoch: [0][2220/19336]\tTime 6.147 (6.158)\tData 0.032 (0.025)\tLoss 3.2764 (3.6387)\tPrec@1 38.672 (36.767)\tPrec@5 61.328 (56.043)\n",
      "Epoch: [0][2230/19336]\tTime 6.150 (6.158)\tData 0.026 (0.025)\tLoss 3.1632 (3.6368)\tPrec@1 39.648 (36.788)\tPrec@5 60.156 (56.063)\n",
      "Epoch: [0][2240/19336]\tTime 6.143 (6.158)\tData 0.029 (0.025)\tLoss 3.1369 (3.6351)\tPrec@1 42.773 (36.807)\tPrec@5 61.914 (56.086)\n",
      "Epoch: [0][2250/19336]\tTime 6.168 (6.158)\tData 0.027 (0.025)\tLoss 3.0886 (3.6331)\tPrec@1 44.727 (36.831)\tPrec@5 62.109 (56.109)\n",
      "Epoch: [0][2260/19336]\tTime 6.138 (6.158)\tData 0.027 (0.025)\tLoss 3.3422 (3.6315)\tPrec@1 39.258 (36.845)\tPrec@5 59.180 (56.130)\n",
      "Epoch: [0][2270/19336]\tTime 6.131 (6.157)\tData 0.027 (0.025)\tLoss 3.3335 (3.6298)\tPrec@1 37.109 (36.860)\tPrec@5 58.203 (56.149)\n",
      "Epoch: [0][2280/19336]\tTime 6.128 (6.157)\tData 0.028 (0.025)\tLoss 3.1776 (3.6277)\tPrec@1 41.016 (36.883)\tPrec@5 60.742 (56.173)\n",
      "Epoch: [0][2290/19336]\tTime 6.143 (6.157)\tData 0.031 (0.025)\tLoss 3.3738 (3.6259)\tPrec@1 38.477 (36.905)\tPrec@5 59.180 (56.195)\n",
      "Epoch: [0][2300/19336]\tTime 6.127 (6.157)\tData 0.026 (0.025)\tLoss 3.1444 (3.6239)\tPrec@1 42.773 (36.930)\tPrec@5 62.891 (56.219)\n",
      "Epoch: [0][2310/19336]\tTime 6.136 (6.157)\tData 0.026 (0.025)\tLoss 3.1145 (3.6219)\tPrec@1 41.797 (36.953)\tPrec@5 63.477 (56.243)\n",
      "Epoch: [0][2320/19336]\tTime 6.136 (6.157)\tData 0.027 (0.025)\tLoss 3.2230 (3.6199)\tPrec@1 40.625 (36.974)\tPrec@5 62.891 (56.267)\n",
      "Epoch: [0][2330/19336]\tTime 6.114 (6.157)\tData 0.026 (0.025)\tLoss 3.1334 (3.6184)\tPrec@1 42.773 (36.989)\tPrec@5 62.695 (56.283)\n",
      "Epoch: [0][2340/19336]\tTime 6.118 (6.157)\tData 0.027 (0.025)\tLoss 3.1610 (3.6167)\tPrec@1 42.188 (37.010)\tPrec@5 61.328 (56.305)\n",
      "Epoch: [0][2350/19336]\tTime 6.111 (6.157)\tData 0.030 (0.025)\tLoss 3.1940 (3.6150)\tPrec@1 42.578 (37.029)\tPrec@5 61.523 (56.324)\n",
      "Epoch: [0][2360/19336]\tTime 6.112 (6.157)\tData 0.029 (0.025)\tLoss 3.2087 (3.6134)\tPrec@1 42.969 (37.049)\tPrec@5 62.500 (56.346)\n",
      "Epoch: [0][2370/19336]\tTime 6.115 (6.156)\tData 0.026 (0.025)\tLoss 3.0086 (3.6117)\tPrec@1 44.141 (37.069)\tPrec@5 62.891 (56.367)\n",
      "Epoch: [0][2380/19336]\tTime 6.142 (6.156)\tData 0.037 (0.025)\tLoss 3.1579 (3.6102)\tPrec@1 42.773 (37.089)\tPrec@5 60.352 (56.384)\n",
      "Epoch: [0][2390/19336]\tTime 6.141 (6.156)\tData 0.026 (0.025)\tLoss 2.9733 (3.6085)\tPrec@1 40.039 (37.107)\tPrec@5 64.062 (56.405)\n",
      "Epoch: [0][2400/19336]\tTime 6.116 (6.156)\tData 0.036 (0.025)\tLoss 3.1046 (3.6068)\tPrec@1 40.625 (37.125)\tPrec@5 62.500 (56.428)\n",
      "Epoch: [0][2410/19336]\tTime 6.119 (6.156)\tData 0.030 (0.025)\tLoss 3.1764 (3.6051)\tPrec@1 37.500 (37.143)\tPrec@5 62.109 (56.447)\n",
      "Epoch: [0][2420/19336]\tTime 6.132 (6.156)\tData 0.031 (0.025)\tLoss 3.1918 (3.6036)\tPrec@1 44.531 (37.161)\tPrec@5 60.938 (56.467)\n",
      "Epoch: [0][2430/19336]\tTime 6.123 (6.156)\tData 0.032 (0.025)\tLoss 3.1472 (3.6019)\tPrec@1 41.211 (37.178)\tPrec@5 63.477 (56.485)\n",
      "Epoch: [0][2440/19336]\tTime 6.119 (6.156)\tData 0.039 (0.025)\tLoss 3.1949 (3.6002)\tPrec@1 40.430 (37.198)\tPrec@5 62.500 (56.508)\n",
      "Epoch: [0][2450/19336]\tTime 6.126 (6.155)\tData 0.028 (0.025)\tLoss 2.9592 (3.5985)\tPrec@1 46.484 (37.218)\tPrec@5 67.383 (56.532)\n",
      "Epoch: [0][2460/19336]\tTime 6.110 (6.155)\tData 0.034 (0.025)\tLoss 3.3578 (3.5972)\tPrec@1 37.305 (37.232)\tPrec@5 60.156 (56.550)\n",
      "Epoch: [0][2470/19336]\tTime 6.089 (6.155)\tData 0.028 (0.025)\tLoss 3.1848 (3.5957)\tPrec@1 41.992 (37.248)\tPrec@5 61.914 (56.569)\n",
      "Epoch: [0][2480/19336]\tTime 6.122 (6.155)\tData 0.030 (0.025)\tLoss 3.2372 (3.5943)\tPrec@1 39.844 (37.260)\tPrec@5 62.305 (56.588)\n",
      "Epoch: [0][2490/19336]\tTime 6.120 (6.155)\tData 0.028 (0.025)\tLoss 2.9694 (3.5924)\tPrec@1 45.703 (37.282)\tPrec@5 65.234 (56.611)\n",
      "Epoch: [0][2500/19336]\tTime 6.101 (6.155)\tData 0.031 (0.025)\tLoss 3.2379 (3.5909)\tPrec@1 39.258 (37.299)\tPrec@5 61.133 (56.631)\n",
      "Epoch: [0][2510/19336]\tTime 6.102 (6.155)\tData 0.031 (0.025)\tLoss 3.2913 (3.5896)\tPrec@1 39.844 (37.316)\tPrec@5 59.766 (56.648)\n",
      "Epoch: [0][2520/19336]\tTime 6.089 (6.155)\tData 0.033 (0.025)\tLoss 3.1200 (3.5880)\tPrec@1 40.039 (37.336)\tPrec@5 61.719 (56.669)\n",
      "Epoch: [0][2530/19336]\tTime 6.133 (6.155)\tData 0.032 (0.025)\tLoss 2.9103 (3.5864)\tPrec@1 46.094 (37.354)\tPrec@5 65.625 (56.690)\n",
      "Epoch: [0][2540/19336]\tTime 6.132 (6.154)\tData 0.029 (0.025)\tLoss 3.3710 (3.5846)\tPrec@1 38.477 (37.372)\tPrec@5 58.984 (56.712)\n",
      "Epoch: [0][2550/19336]\tTime 6.134 (6.154)\tData 0.043 (0.025)\tLoss 3.1688 (3.5829)\tPrec@1 40.430 (37.389)\tPrec@5 61.719 (56.733)\n",
      "Epoch: [0][2560/19336]\tTime 6.130 (6.154)\tData 0.036 (0.025)\tLoss 3.3374 (3.5817)\tPrec@1 39.844 (37.405)\tPrec@5 58.594 (56.747)\n",
      "Epoch: [0][2570/19336]\tTime 6.136 (6.154)\tData 0.028 (0.026)\tLoss 3.2256 (3.5802)\tPrec@1 41.992 (37.422)\tPrec@5 61.328 (56.764)\n",
      "Epoch: [0][2580/19336]\tTime 6.111 (6.154)\tData 0.028 (0.026)\tLoss 3.2777 (3.5788)\tPrec@1 39.844 (37.438)\tPrec@5 59.375 (56.780)\n",
      "Epoch: [0][2590/19336]\tTime 6.176 (6.154)\tData 0.035 (0.026)\tLoss 3.2453 (3.5773)\tPrec@1 42.578 (37.455)\tPrec@5 61.523 (56.797)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0][2600/19336]\tTime 6.137 (6.154)\tData 0.031 (0.026)\tLoss 3.0972 (3.5757)\tPrec@1 42.188 (37.473)\tPrec@5 65.430 (56.818)\n",
      "Epoch: [0][2610/19336]\tTime 6.109 (6.154)\tData 0.038 (0.026)\tLoss 3.2741 (3.5742)\tPrec@1 41.211 (37.490)\tPrec@5 60.156 (56.836)\n",
      "Epoch: [0][2620/19336]\tTime 6.123 (6.154)\tData 0.029 (0.026)\tLoss 3.2421 (3.5730)\tPrec@1 40.625 (37.502)\tPrec@5 63.086 (56.851)\n",
      "Epoch: [0][2630/19336]\tTime 6.138 (6.154)\tData 0.032 (0.026)\tLoss 2.9408 (3.5716)\tPrec@1 44.922 (37.520)\tPrec@5 64.844 (56.869)\n",
      "Epoch: [0][2640/19336]\tTime 6.136 (6.153)\tData 0.041 (0.026)\tLoss 3.0869 (3.5702)\tPrec@1 44.336 (37.537)\tPrec@5 62.500 (56.887)\n",
      "Epoch: [0][2650/19336]\tTime 6.135 (6.153)\tData 0.038 (0.026)\tLoss 3.3805 (3.5688)\tPrec@1 40.234 (37.552)\tPrec@5 57.812 (56.904)\n",
      "Epoch: [0][2660/19336]\tTime 6.097 (6.153)\tData 0.032 (0.026)\tLoss 3.3009 (3.5675)\tPrec@1 41.211 (37.568)\tPrec@5 60.352 (56.919)\n",
      "Epoch: [0][2670/19336]\tTime 6.113 (6.153)\tData 0.036 (0.026)\tLoss 3.2774 (3.5663)\tPrec@1 41.016 (37.582)\tPrec@5 61.328 (56.935)\n",
      "Epoch: [0][2680/19336]\tTime 6.106 (6.153)\tData 0.030 (0.026)\tLoss 3.2953 (3.5654)\tPrec@1 43.750 (37.590)\tPrec@5 59.766 (56.944)\n",
      "Epoch: [0][2690/19336]\tTime 6.124 (6.153)\tData 0.039 (0.026)\tLoss 3.3069 (3.5642)\tPrec@1 39.648 (37.599)\tPrec@5 59.375 (56.955)\n",
      "Epoch: [0][2700/19336]\tTime 6.148 (6.153)\tData 0.030 (0.026)\tLoss 3.2096 (3.5630)\tPrec@1 43.750 (37.614)\tPrec@5 61.914 (56.970)\n",
      "Epoch: [0][2710/19336]\tTime 6.120 (6.153)\tData 0.037 (0.026)\tLoss 3.3353 (3.5616)\tPrec@1 41.406 (37.632)\tPrec@5 60.156 (56.988)\n",
      "Epoch: [0][2720/19336]\tTime 6.112 (6.153)\tData 0.030 (0.026)\tLoss 3.2689 (3.5603)\tPrec@1 40.625 (37.646)\tPrec@5 59.766 (57.006)\n",
      "Epoch: [0][2730/19336]\tTime 6.090 (6.153)\tData 0.029 (0.026)\tLoss 3.2594 (3.5589)\tPrec@1 39.062 (37.661)\tPrec@5 60.156 (57.021)\n",
      "Epoch: [0][2740/19336]\tTime 6.138 (6.153)\tData 0.033 (0.026)\tLoss 3.0246 (3.5575)\tPrec@1 40.625 (37.679)\tPrec@5 64.648 (57.040)\n",
      "Epoch: [0][2750/19336]\tTime 6.130 (6.153)\tData 0.037 (0.026)\tLoss 3.3587 (3.5561)\tPrec@1 38.867 (37.692)\tPrec@5 58.398 (57.056)\n",
      "Epoch: [0][2760/19336]\tTime 6.135 (6.153)\tData 0.037 (0.026)\tLoss 3.4849 (3.5548)\tPrec@1 39.648 (37.705)\tPrec@5 58.594 (57.073)\n",
      "Epoch: [0][2770/19336]\tTime 6.132 (6.152)\tData 0.046 (0.026)\tLoss 3.0406 (3.5533)\tPrec@1 43.555 (37.721)\tPrec@5 63.672 (57.093)\n",
      "Epoch: [0][2780/19336]\tTime 6.116 (6.152)\tData 0.029 (0.026)\tLoss 3.0559 (3.5521)\tPrec@1 42.578 (37.735)\tPrec@5 63.867 (57.108)\n",
      "Epoch: [0][2790/19336]\tTime 6.106 (6.152)\tData 0.038 (0.026)\tLoss 3.3979 (3.5509)\tPrec@1 41.992 (37.750)\tPrec@5 58.398 (57.123)\n",
      "Epoch: [0][2800/19336]\tTime 6.115 (6.152)\tData 0.031 (0.026)\tLoss 3.0962 (3.5497)\tPrec@1 43.555 (37.763)\tPrec@5 61.914 (57.136)\n",
      "Epoch: [0][2810/19336]\tTime 6.140 (6.152)\tData 0.029 (0.026)\tLoss 3.0474 (3.5485)\tPrec@1 44.336 (37.776)\tPrec@5 62.500 (57.150)\n",
      "Epoch: [0][2820/19336]\tTime 6.126 (6.152)\tData 0.031 (0.026)\tLoss 3.0333 (3.5472)\tPrec@1 46.484 (37.791)\tPrec@5 63.477 (57.169)\n",
      "Epoch: [0][2830/19336]\tTime 6.125 (6.152)\tData 0.039 (0.026)\tLoss 3.2425 (3.5460)\tPrec@1 40.820 (37.807)\tPrec@5 60.742 (57.184)\n",
      "Epoch: [0][2840/19336]\tTime 6.141 (6.152)\tData 0.035 (0.026)\tLoss 3.4647 (3.5450)\tPrec@1 38.086 (37.818)\tPrec@5 59.180 (57.197)\n",
      "Epoch: [0][2850/19336]\tTime 6.122 (6.152)\tData 0.030 (0.026)\tLoss 2.9592 (3.5436)\tPrec@1 43.750 (37.833)\tPrec@5 64.453 (57.214)\n",
      "Epoch: [0][2860/19336]\tTime 6.129 (6.152)\tData 0.031 (0.026)\tLoss 3.3520 (3.5421)\tPrec@1 40.430 (37.849)\tPrec@5 60.352 (57.233)\n",
      "Epoch: [0][2870/19336]\tTime 6.140 (6.152)\tData 0.055 (0.026)\tLoss 3.2085 (3.5408)\tPrec@1 41.797 (37.866)\tPrec@5 63.672 (57.250)\n",
      "Epoch: [0][2880/19336]\tTime 6.123 (6.151)\tData 0.035 (0.026)\tLoss 3.1218 (3.5396)\tPrec@1 42.773 (37.876)\tPrec@5 62.500 (57.264)\n",
      "Epoch: [0][2890/19336]\tTime 6.145 (6.151)\tData 0.030 (0.026)\tLoss 3.1285 (3.5384)\tPrec@1 41.406 (37.891)\tPrec@5 61.523 (57.277)\n",
      "Epoch: [0][2900/19336]\tTime 6.129 (6.151)\tData 0.030 (0.026)\tLoss 3.3770 (3.5373)\tPrec@1 39.844 (37.903)\tPrec@5 57.812 (57.289)\n",
      "Epoch: [0][2910/19336]\tTime 6.098 (6.151)\tData 0.035 (0.026)\tLoss 3.1586 (3.5362)\tPrec@1 43.555 (37.917)\tPrec@5 61.719 (57.301)\n",
      "Epoch: [0][2920/19336]\tTime 6.133 (6.151)\tData 0.034 (0.027)\tLoss 3.2413 (3.5352)\tPrec@1 40.820 (37.929)\tPrec@5 61.328 (57.316)\n",
      "Epoch: [0][2930/19336]\tTime 6.108 (6.151)\tData 0.030 (0.027)\tLoss 3.1548 (3.5338)\tPrec@1 42.969 (37.943)\tPrec@5 62.891 (57.333)\n",
      "Epoch: [0][2940/19336]\tTime 6.146 (6.151)\tData 0.034 (0.027)\tLoss 3.2842 (3.5329)\tPrec@1 42.383 (37.956)\tPrec@5 59.766 (57.343)\n",
      "Epoch: [0][2950/19336]\tTime 6.135 (6.151)\tData 0.030 (0.027)\tLoss 3.1539 (3.5317)\tPrec@1 42.188 (37.973)\tPrec@5 64.062 (57.359)\n",
      "Epoch: [0][2960/19336]\tTime 6.149 (6.151)\tData 0.036 (0.027)\tLoss 3.2403 (3.5305)\tPrec@1 42.578 (37.988)\tPrec@5 59.961 (57.375)\n",
      "Epoch: [0][2970/19336]\tTime 6.154 (6.151)\tData 0.038 (0.027)\tLoss 2.8499 (3.5293)\tPrec@1 45.312 (37.999)\tPrec@5 66.992 (57.388)\n",
      "Epoch: [0][2980/19336]\tTime 6.136 (6.151)\tData 0.033 (0.027)\tLoss 3.1480 (3.5281)\tPrec@1 41.992 (38.014)\tPrec@5 61.914 (57.400)\n",
      "Epoch: [0][2990/19336]\tTime 6.139 (6.151)\tData 0.038 (0.027)\tLoss 3.2580 (3.5268)\tPrec@1 41.016 (38.028)\tPrec@5 60.156 (57.415)\n",
      "Epoch: [0][3000/19336]\tTime 6.137 (6.151)\tData 0.031 (0.027)\tLoss 3.3125 (3.5254)\tPrec@1 40.625 (38.046)\tPrec@5 60.547 (57.433)\n",
      "Epoch: [0][3010/19336]\tTime 6.100 (6.151)\tData 0.035 (0.027)\tLoss 3.0658 (3.5242)\tPrec@1 45.117 (38.060)\tPrec@5 63.477 (57.448)\n",
      "Epoch: [0][3020/19336]\tTime 6.127 (6.151)\tData 0.031 (0.027)\tLoss 3.1851 (3.5230)\tPrec@1 40.039 (38.071)\tPrec@5 61.328 (57.463)\n",
      "Epoch: [0][3030/19336]\tTime 6.114 (6.151)\tData 0.031 (0.027)\tLoss 3.2310 (3.5222)\tPrec@1 42.383 (38.081)\tPrec@5 61.523 (57.473)\n",
      "Epoch: [0][3040/19336]\tTime 6.073 (6.151)\tData 0.044 (0.027)\tLoss 3.2600 (3.5208)\tPrec@1 42.578 (38.097)\tPrec@5 62.500 (57.491)\n",
      "Epoch: [0][3050/19336]\tTime 6.101 (6.151)\tData 0.034 (0.027)\tLoss 3.2875 (3.5198)\tPrec@1 43.164 (38.111)\tPrec@5 59.375 (57.504)\n",
      "Epoch: [0][3060/19336]\tTime 6.155 (6.151)\tData 0.038 (0.027)\tLoss 3.0956 (3.5189)\tPrec@1 44.531 (38.122)\tPrec@5 63.281 (57.516)\n",
      "Epoch: [0][3070/19336]\tTime 6.228 (6.151)\tData 0.031 (0.027)\tLoss 3.2386 (3.5178)\tPrec@1 42.383 (38.134)\tPrec@5 59.766 (57.529)\n",
      "Epoch: [0][3080/19336]\tTime 6.105 (6.151)\tData 0.047 (0.027)\tLoss 3.2355 (3.5167)\tPrec@1 38.672 (38.147)\tPrec@5 60.742 (57.543)\n",
      "Epoch: [0][3090/19336]\tTime 6.228 (6.151)\tData 0.035 (0.027)\tLoss 3.1107 (3.5156)\tPrec@1 41.992 (38.157)\tPrec@5 62.695 (57.557)\n",
      "Epoch: [0][3100/19336]\tTime 6.127 (6.150)\tData 0.040 (0.027)\tLoss 3.2656 (3.5148)\tPrec@1 40.820 (38.167)\tPrec@5 60.352 (57.566)\n",
      "Epoch: [0][3110/19336]\tTime 6.133 (6.150)\tData 0.032 (0.027)\tLoss 3.1469 (3.5137)\tPrec@1 43.750 (38.180)\tPrec@5 62.891 (57.580)\n",
      "Epoch: [0][3120/19336]\tTime 6.115 (6.150)\tData 0.037 (0.027)\tLoss 3.0453 (3.5123)\tPrec@1 42.969 (38.195)\tPrec@5 64.648 (57.598)\n",
      "Epoch: [0][3130/19336]\tTime 6.119 (6.150)\tData 0.038 (0.027)\tLoss 3.0711 (3.5113)\tPrec@1 40.820 (38.205)\tPrec@5 63.867 (57.613)\n",
      "Epoch: [0][3140/19336]\tTime 6.169 (6.150)\tData 0.039 (0.027)\tLoss 3.3479 (3.5103)\tPrec@1 37.891 (38.216)\tPrec@5 59.375 (57.624)\n",
      "Epoch: [0][3150/19336]\tTime 6.110 (6.150)\tData 0.038 (0.027)\tLoss 3.2836 (3.5093)\tPrec@1 39.844 (38.228)\tPrec@5 60.938 (57.635)\n",
      "Epoch: [0][3160/19336]\tTime 6.107 (6.150)\tData 0.031 (0.027)\tLoss 3.2302 (3.5081)\tPrec@1 41.797 (38.240)\tPrec@5 61.133 (57.650)\n",
      "Epoch: [0][3170/19336]\tTime 6.135 (6.150)\tData 0.032 (0.028)\tLoss 3.3003 (3.5071)\tPrec@1 39.453 (38.251)\tPrec@5 60.547 (57.662)\n",
      "Epoch: [0][3180/19336]\tTime 6.141 (6.150)\tData 0.035 (0.028)\tLoss 3.0770 (3.5060)\tPrec@1 43.945 (38.265)\tPrec@5 62.891 (57.678)\n",
      "Epoch: [0][3190/19336]\tTime 6.078 (6.150)\tData 0.033 (0.028)\tLoss 3.2302 (3.5051)\tPrec@1 42.188 (38.276)\tPrec@5 61.719 (57.690)\n",
      "Epoch: [0][3200/19336]\tTime 6.120 (6.150)\tData 0.039 (0.028)\tLoss 3.3290 (3.5041)\tPrec@1 41.211 (38.289)\tPrec@5 59.180 (57.703)\n",
      "Epoch: [0][3210/19336]\tTime 6.111 (6.150)\tData 0.041 (0.028)\tLoss 3.2739 (3.5030)\tPrec@1 39.258 (38.299)\tPrec@5 60.938 (57.715)\n",
      "Epoch: [0][3220/19336]\tTime 6.124 (6.150)\tData 0.034 (0.028)\tLoss 3.2443 (3.5021)\tPrec@1 43.164 (38.311)\tPrec@5 60.742 (57.727)\n",
      "Epoch: [0][3230/19336]\tTime 6.135 (6.150)\tData 0.034 (0.028)\tLoss 3.0177 (3.5008)\tPrec@1 45.312 (38.323)\tPrec@5 63.672 (57.744)\n",
      "Epoch: [0][3240/19336]\tTime 6.118 (6.150)\tData 0.032 (0.028)\tLoss 3.0998 (3.4997)\tPrec@1 43.164 (38.336)\tPrec@5 63.477 (57.760)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0][3250/19336]\tTime 6.120 (6.149)\tData 0.036 (0.028)\tLoss 3.2319 (3.4989)\tPrec@1 42.188 (38.347)\tPrec@5 62.891 (57.771)\n",
      "Epoch: [0][3260/19336]\tTime 6.123 (6.149)\tData 0.032 (0.028)\tLoss 3.0732 (3.4978)\tPrec@1 43.555 (38.359)\tPrec@5 61.328 (57.783)\n",
      "Epoch: [0][3270/19336]\tTime 6.106 (6.149)\tData 0.032 (0.028)\tLoss 2.7620 (3.4966)\tPrec@1 48.438 (38.373)\tPrec@5 65.430 (57.796)\n",
      "Epoch: [0][3280/19336]\tTime 6.109 (6.149)\tData 0.032 (0.028)\tLoss 3.0051 (3.4958)\tPrec@1 44.727 (38.383)\tPrec@5 64.453 (57.806)\n",
      "Epoch: [0][3290/19336]\tTime 6.127 (6.149)\tData 0.036 (0.028)\tLoss 3.0954 (3.4946)\tPrec@1 42.969 (38.398)\tPrec@5 65.039 (57.822)\n",
      "Epoch: [0][3300/19336]\tTime 6.146 (6.149)\tData 0.036 (0.028)\tLoss 3.1584 (3.4938)\tPrec@1 44.141 (38.410)\tPrec@5 62.500 (57.832)\n",
      "Epoch: [0][3310/19336]\tTime 6.144 (6.149)\tData 0.037 (0.028)\tLoss 2.9229 (3.4924)\tPrec@1 45.117 (38.426)\tPrec@5 65.039 (57.849)\n",
      "Epoch: [0][3320/19336]\tTime 6.147 (6.149)\tData 0.034 (0.028)\tLoss 3.2532 (3.4915)\tPrec@1 42.188 (38.436)\tPrec@5 59.570 (57.858)\n",
      "Epoch: [0][3330/19336]\tTime 6.162 (6.149)\tData 0.034 (0.028)\tLoss 3.0468 (3.4903)\tPrec@1 43.945 (38.447)\tPrec@5 63.281 (57.872)\n",
      "Epoch: [0][3340/19336]\tTime 6.167 (6.149)\tData 0.032 (0.028)\tLoss 3.2877 (3.4893)\tPrec@1 40.234 (38.458)\tPrec@5 61.914 (57.884)\n",
      "Epoch: [0][3350/19336]\tTime 6.113 (6.149)\tData 0.034 (0.028)\tLoss 3.1731 (3.4883)\tPrec@1 42.773 (38.469)\tPrec@5 62.109 (57.898)\n",
      "Epoch: [0][3360/19336]\tTime 6.137 (6.149)\tData 0.033 (0.028)\tLoss 3.3628 (3.4873)\tPrec@1 41.992 (38.484)\tPrec@5 58.594 (57.911)\n",
      "Epoch: [0][3370/19336]\tTime 6.096 (6.149)\tData 0.034 (0.028)\tLoss 3.0651 (3.4863)\tPrec@1 41.406 (38.497)\tPrec@5 63.281 (57.922)\n",
      "Epoch: [0][3380/19336]\tTime 6.120 (6.149)\tData 0.032 (0.028)\tLoss 3.3899 (3.4852)\tPrec@1 38.086 (38.508)\tPrec@5 57.227 (57.936)\n",
      "Epoch: [0][3390/19336]\tTime 6.151 (6.149)\tData 0.039 (0.028)\tLoss 3.1743 (3.4843)\tPrec@1 44.141 (38.518)\tPrec@5 63.672 (57.950)\n",
      "Epoch: [0][3400/19336]\tTime 6.153 (6.149)\tData 0.042 (0.028)\tLoss 3.0454 (3.4833)\tPrec@1 43.945 (38.530)\tPrec@5 62.500 (57.963)\n",
      "Epoch: [0][3410/19336]\tTime 6.142 (6.149)\tData 0.033 (0.028)\tLoss 3.1263 (3.4823)\tPrec@1 40.625 (38.539)\tPrec@5 62.109 (57.975)\n",
      "Epoch: [0][3420/19336]\tTime 6.109 (6.149)\tData 0.033 (0.028)\tLoss 3.0327 (3.4811)\tPrec@1 46.094 (38.555)\tPrec@5 65.039 (57.991)\n",
      "Epoch: [0][3430/19336]\tTime 6.141 (6.148)\tData 0.033 (0.028)\tLoss 3.2589 (3.4802)\tPrec@1 42.383 (38.563)\tPrec@5 62.695 (58.003)\n",
      "Epoch: [0][3440/19336]\tTime 6.128 (6.148)\tData 0.034 (0.028)\tLoss 3.3504 (3.4792)\tPrec@1 36.719 (38.572)\tPrec@5 60.742 (58.016)\n",
      "Epoch: [0][3450/19336]\tTime 6.110 (6.148)\tData 0.039 (0.028)\tLoss 3.1266 (3.4781)\tPrec@1 42.383 (38.582)\tPrec@5 62.109 (58.030)\n",
      "Epoch: [0][3460/19336]\tTime 6.137 (6.148)\tData 0.039 (0.028)\tLoss 2.9875 (3.4771)\tPrec@1 41.602 (38.593)\tPrec@5 63.281 (58.042)\n",
      "Epoch: [0][3470/19336]\tTime 6.095 (6.148)\tData 0.036 (0.028)\tLoss 3.2348 (3.4763)\tPrec@1 42.188 (38.601)\tPrec@5 60.352 (58.052)\n",
      "Epoch: [0][3480/19336]\tTime 6.145 (6.148)\tData 0.034 (0.028)\tLoss 3.0096 (3.4753)\tPrec@1 42.773 (38.612)\tPrec@5 61.719 (58.064)\n",
      "Epoch: [0][3490/19336]\tTime 6.112 (6.148)\tData 0.039 (0.028)\tLoss 3.0280 (3.4743)\tPrec@1 45.508 (38.625)\tPrec@5 66.211 (58.076)\n",
      "Epoch: [0][3500/19336]\tTime 6.123 (6.148)\tData 0.034 (0.028)\tLoss 3.1592 (3.4732)\tPrec@1 39.258 (38.636)\tPrec@5 62.109 (58.090)\n",
      "Epoch: [0][3510/19336]\tTime 6.116 (6.148)\tData 0.033 (0.028)\tLoss 3.2426 (3.4723)\tPrec@1 43.750 (38.647)\tPrec@5 61.523 (58.102)\n",
      "Epoch: [0][3520/19336]\tTime 6.127 (6.148)\tData 0.034 (0.028)\tLoss 2.8519 (3.4713)\tPrec@1 46.289 (38.658)\tPrec@5 67.773 (58.114)\n",
      "Epoch: [0][3530/19336]\tTime 6.125 (6.148)\tData 0.040 (0.028)\tLoss 3.0538 (3.4704)\tPrec@1 41.797 (38.667)\tPrec@5 63.086 (58.126)\n",
      "Epoch: [0][3540/19336]\tTime 6.138 (6.148)\tData 0.038 (0.028)\tLoss 2.9673 (3.4695)\tPrec@1 45.312 (38.675)\tPrec@5 63.867 (58.137)\n",
      "Epoch: [0][3550/19336]\tTime 6.158 (6.148)\tData 0.033 (0.028)\tLoss 3.0548 (3.4684)\tPrec@1 46.094 (38.689)\tPrec@5 64.648 (58.151)\n",
      "Epoch: [0][3560/19336]\tTime 6.122 (6.148)\tData 0.036 (0.028)\tLoss 3.1273 (3.4672)\tPrec@1 40.820 (38.703)\tPrec@5 63.086 (58.165)\n",
      "Epoch: [0][3570/19336]\tTime 6.121 (6.148)\tData 0.034 (0.029)\tLoss 3.2238 (3.4665)\tPrec@1 40.039 (38.712)\tPrec@5 59.570 (58.174)\n",
      "Epoch: [0][3580/19336]\tTime 6.100 (6.148)\tData 0.039 (0.029)\tLoss 3.0921 (3.4654)\tPrec@1 45.117 (38.724)\tPrec@5 60.938 (58.188)\n",
      "Epoch: [0][3590/19336]\tTime 6.159 (6.148)\tData 0.042 (0.029)\tLoss 3.0255 (3.4643)\tPrec@1 42.773 (38.736)\tPrec@5 62.891 (58.202)\n",
      "Epoch: [0][3600/19336]\tTime 6.098 (6.148)\tData 0.035 (0.029)\tLoss 3.0889 (3.4633)\tPrec@1 41.016 (38.747)\tPrec@5 62.109 (58.214)\n",
      "Epoch: [0][3610/19336]\tTime 6.116 (6.148)\tData 0.036 (0.029)\tLoss 3.1170 (3.4623)\tPrec@1 43.164 (38.759)\tPrec@5 64.453 (58.226)\n",
      "Epoch: [0][3620/19336]\tTime 6.135 (6.147)\tData 0.034 (0.029)\tLoss 3.2997 (3.4614)\tPrec@1 41.406 (38.768)\tPrec@5 59.961 (58.237)\n",
      "Epoch: [0][3630/19336]\tTime 6.196 (6.147)\tData 0.035 (0.029)\tLoss 3.1639 (3.4605)\tPrec@1 39.062 (38.778)\tPrec@5 62.109 (58.248)\n",
      "Epoch: [0][3640/19336]\tTime 6.167 (6.147)\tData 0.036 (0.029)\tLoss 3.0399 (3.4598)\tPrec@1 45.508 (38.789)\tPrec@5 62.500 (58.258)\n",
      "Epoch: [0][3650/19336]\tTime 6.116 (6.147)\tData 0.038 (0.029)\tLoss 3.2250 (3.4588)\tPrec@1 41.602 (38.797)\tPrec@5 59.180 (58.268)\n",
      "Epoch: [0][3660/19336]\tTime 6.100 (6.147)\tData 0.041 (0.029)\tLoss 3.0635 (3.4580)\tPrec@1 42.188 (38.806)\tPrec@5 61.914 (58.279)\n",
      "Epoch: [0][3670/19336]\tTime 6.122 (6.147)\tData 0.034 (0.029)\tLoss 3.1030 (3.4571)\tPrec@1 44.727 (38.818)\tPrec@5 62.109 (58.288)\n",
      "Epoch: [0][3680/19336]\tTime 6.102 (6.147)\tData 0.043 (0.029)\tLoss 3.0378 (3.4563)\tPrec@1 43.164 (38.829)\tPrec@5 63.867 (58.299)\n",
      "Epoch: [0][3690/19336]\tTime 6.126 (6.147)\tData 0.040 (0.029)\tLoss 3.0961 (3.4554)\tPrec@1 44.336 (38.837)\tPrec@5 61.133 (58.309)\n",
      "Epoch: [0][3700/19336]\tTime 6.144 (6.147)\tData 0.038 (0.029)\tLoss 2.9770 (3.4543)\tPrec@1 45.117 (38.850)\tPrec@5 63.477 (58.322)\n",
      "Epoch: [0][3710/19336]\tTime 6.119 (6.147)\tData 0.036 (0.029)\tLoss 3.1686 (3.4535)\tPrec@1 40.234 (38.858)\tPrec@5 61.328 (58.331)\n",
      "Epoch: [0][3720/19336]\tTime 6.114 (6.147)\tData 0.036 (0.029)\tLoss 3.3046 (3.4529)\tPrec@1 43.945 (38.863)\tPrec@5 60.156 (58.339)\n",
      "Epoch: [0][3730/19336]\tTime 6.125 (6.147)\tData 0.039 (0.029)\tLoss 3.2063 (3.4520)\tPrec@1 41.211 (38.874)\tPrec@5 61.719 (58.351)\n",
      "Epoch: [0][3740/19336]\tTime 6.127 (6.147)\tData 0.038 (0.029)\tLoss 3.4495 (3.4511)\tPrec@1 39.258 (38.884)\tPrec@5 59.570 (58.362)\n",
      "Epoch: [0][3750/19336]\tTime 6.146 (6.147)\tData 0.042 (0.029)\tLoss 3.0548 (3.4503)\tPrec@1 42.969 (38.893)\tPrec@5 60.352 (58.373)\n",
      "Epoch: [0][3760/19336]\tTime 6.118 (6.147)\tData 0.040 (0.029)\tLoss 3.0596 (3.4494)\tPrec@1 44.141 (38.904)\tPrec@5 61.914 (58.384)\n",
      "Epoch: [0][3770/19336]\tTime 6.120 (6.147)\tData 0.039 (0.029)\tLoss 3.2277 (3.4485)\tPrec@1 43.555 (38.915)\tPrec@5 62.305 (58.396)\n",
      "Epoch: [0][3780/19336]\tTime 6.112 (6.147)\tData 0.037 (0.029)\tLoss 3.2046 (3.4475)\tPrec@1 42.383 (38.928)\tPrec@5 62.500 (58.409)\n",
      "Epoch: [0][3790/19336]\tTime 6.093 (6.147)\tData 0.035 (0.029)\tLoss 3.2538 (3.4466)\tPrec@1 41.602 (38.937)\tPrec@5 59.180 (58.417)\n",
      "Epoch: [0][3800/19336]\tTime 6.112 (6.147)\tData 0.045 (0.029)\tLoss 3.3650 (3.4457)\tPrec@1 40.234 (38.948)\tPrec@5 58.594 (58.427)\n",
      "Epoch: [0][3810/19336]\tTime 6.110 (6.147)\tData 0.035 (0.029)\tLoss 2.9054 (3.4447)\tPrec@1 45.312 (38.959)\tPrec@5 65.625 (58.438)\n",
      "Epoch: [0][3820/19336]\tTime 6.117 (6.146)\tData 0.042 (0.029)\tLoss 3.1166 (3.4438)\tPrec@1 44.141 (38.970)\tPrec@5 61.133 (58.448)\n",
      "Epoch: [0][3830/19336]\tTime 6.138 (6.146)\tData 0.044 (0.029)\tLoss 3.2565 (3.4430)\tPrec@1 40.039 (38.979)\tPrec@5 60.742 (58.460)\n",
      "Epoch: [0][3840/19336]\tTime 6.138 (6.146)\tData 0.036 (0.029)\tLoss 3.0255 (3.4422)\tPrec@1 43.164 (38.987)\tPrec@5 65.625 (58.471)\n",
      "Epoch: [0][3850/19336]\tTime 6.122 (6.146)\tData 0.036 (0.029)\tLoss 3.1585 (3.4414)\tPrec@1 41.992 (38.997)\tPrec@5 61.328 (58.481)\n",
      "Epoch: [0][3860/19336]\tTime 6.146 (6.146)\tData 0.038 (0.029)\tLoss 3.2492 (3.4407)\tPrec@1 39.062 (39.006)\tPrec@5 60.938 (58.491)\n",
      "Epoch: [0][3870/19336]\tTime 6.142 (6.146)\tData 0.025 (0.029)\tLoss 3.1959 (3.4398)\tPrec@1 44.727 (39.016)\tPrec@5 60.547 (58.498)\n",
      "Epoch: [0][3880/19336]\tTime 6.114 (6.146)\tData 0.024 (0.029)\tLoss 3.0661 (3.4389)\tPrec@1 42.578 (39.024)\tPrec@5 63.477 (58.510)\n",
      "Epoch: [0][3890/19336]\tTime 6.144 (6.146)\tData 0.030 (0.029)\tLoss 3.1766 (3.4381)\tPrec@1 40.820 (39.032)\tPrec@5 61.523 (58.521)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0][3900/19336]\tTime 6.112 (6.146)\tData 0.024 (0.029)\tLoss 3.1881 (3.4374)\tPrec@1 43.555 (39.041)\tPrec@5 61.914 (58.531)\n",
      "Epoch: [0][3910/19336]\tTime 6.099 (6.146)\tData 0.023 (0.029)\tLoss 2.8847 (3.4365)\tPrec@1 45.898 (39.049)\tPrec@5 64.844 (58.540)\n",
      "Epoch: [0][3920/19336]\tTime 6.193 (6.146)\tData 0.037 (0.029)\tLoss 3.0738 (3.4356)\tPrec@1 43.359 (39.059)\tPrec@5 63.281 (58.550)\n",
      "Epoch: [0][3930/19336]\tTime 6.114 (6.146)\tData 0.024 (0.029)\tLoss 3.0486 (3.4348)\tPrec@1 41.797 (39.067)\tPrec@5 61.914 (58.559)\n",
      "Epoch: [0][3940/19336]\tTime 6.105 (6.146)\tData 0.024 (0.029)\tLoss 3.0262 (3.4341)\tPrec@1 43.164 (39.076)\tPrec@5 63.672 (58.568)\n",
      "Epoch: [0][3950/19336]\tTime 6.105 (6.146)\tData 0.030 (0.029)\tLoss 3.2186 (3.4332)\tPrec@1 38.281 (39.086)\tPrec@5 60.547 (58.577)\n",
      "Epoch: [0][3960/19336]\tTime 6.159 (6.146)\tData 0.025 (0.029)\tLoss 3.1876 (3.4324)\tPrec@1 43.359 (39.097)\tPrec@5 62.305 (58.586)\n",
      "Epoch: [0][3970/19336]\tTime 6.125 (6.146)\tData 0.026 (0.029)\tLoss 2.9968 (3.4314)\tPrec@1 43.750 (39.109)\tPrec@5 66.211 (58.600)\n",
      "Epoch: [0][3980/19336]\tTime 6.097 (6.146)\tData 0.024 (0.029)\tLoss 3.1391 (3.4306)\tPrec@1 41.602 (39.117)\tPrec@5 62.109 (58.610)\n",
      "Epoch: [0][3990/19336]\tTime 6.149 (6.146)\tData 0.026 (0.029)\tLoss 3.4872 (3.4299)\tPrec@1 37.500 (39.126)\tPrec@5 58.789 (58.621)\n",
      "Epoch: [0][4000/19336]\tTime 6.142 (6.146)\tData 0.025 (0.029)\tLoss 3.0373 (3.4291)\tPrec@1 42.773 (39.133)\tPrec@5 61.719 (58.628)\n",
      "Epoch: [0][4010/19336]\tTime 6.122 (6.146)\tData 0.032 (0.029)\tLoss 2.9526 (3.4283)\tPrec@1 44.336 (39.143)\tPrec@5 66.406 (58.639)\n",
      "Epoch: [0][4020/19336]\tTime 6.157 (6.146)\tData 0.027 (0.029)\tLoss 3.2948 (3.4275)\tPrec@1 37.891 (39.151)\tPrec@5 59.961 (58.649)\n",
      "Epoch: [0][4030/19336]\tTime 6.127 (6.146)\tData 0.027 (0.029)\tLoss 3.1079 (3.4265)\tPrec@1 40.820 (39.162)\tPrec@5 61.914 (58.662)\n",
      "Epoch: [0][4040/19336]\tTime 6.125 (6.146)\tData 0.026 (0.029)\tLoss 3.0868 (3.4256)\tPrec@1 42.969 (39.172)\tPrec@5 62.695 (58.672)\n",
      "Epoch: [0][4050/19336]\tTime 6.111 (6.146)\tData 0.026 (0.029)\tLoss 3.2059 (3.4249)\tPrec@1 40.234 (39.180)\tPrec@5 61.133 (58.681)\n",
      "Epoch: [0][4060/19336]\tTime 6.111 (6.145)\tData 0.025 (0.029)\tLoss 3.0937 (3.4240)\tPrec@1 42.383 (39.191)\tPrec@5 62.891 (58.692)\n",
      "Epoch: [0][4070/19336]\tTime 6.129 (6.145)\tData 0.034 (0.029)\tLoss 3.0364 (3.4234)\tPrec@1 42.383 (39.199)\tPrec@5 65.625 (58.700)\n",
      "Epoch: [0][4080/19336]\tTime 6.104 (6.145)\tData 0.033 (0.029)\tLoss 2.9639 (3.4226)\tPrec@1 48.047 (39.209)\tPrec@5 65.625 (58.710)\n",
      "Epoch: [0][4090/19336]\tTime 6.127 (6.145)\tData 0.027 (0.029)\tLoss 3.1362 (3.4220)\tPrec@1 41.797 (39.215)\tPrec@5 61.133 (58.717)\n",
      "Epoch: [0][4100/19336]\tTime 6.114 (6.145)\tData 0.027 (0.029)\tLoss 3.2796 (3.4213)\tPrec@1 40.039 (39.221)\tPrec@5 61.523 (58.724)\n",
      "Epoch: [0][4110/19336]\tTime 6.115 (6.145)\tData 0.026 (0.029)\tLoss 3.0754 (3.4204)\tPrec@1 43.945 (39.231)\tPrec@5 62.109 (58.737)\n",
      "Epoch: [0][4120/19336]\tTime 6.143 (6.145)\tData 0.025 (0.029)\tLoss 3.2809 (3.4197)\tPrec@1 38.672 (39.237)\tPrec@5 59.961 (58.745)\n",
      "Epoch: [0][4130/19336]\tTime 6.134 (6.145)\tData 0.032 (0.029)\tLoss 2.7864 (3.4187)\tPrec@1 47.070 (39.248)\tPrec@5 67.773 (58.755)\n",
      "Epoch: [0][4140/19336]\tTime 6.120 (6.145)\tData 0.027 (0.029)\tLoss 2.9672 (3.4180)\tPrec@1 43.945 (39.257)\tPrec@5 64.062 (58.763)\n",
      "Epoch: [0][4150/19336]\tTime 6.113 (6.145)\tData 0.025 (0.029)\tLoss 3.0147 (3.4173)\tPrec@1 42.773 (39.264)\tPrec@5 63.281 (58.773)\n",
      "Epoch: [0][4160/19336]\tTime 6.126 (6.145)\tData 0.025 (0.029)\tLoss 3.0096 (3.4163)\tPrec@1 42.383 (39.276)\tPrec@5 60.742 (58.785)\n",
      "Epoch: [0][4170/19336]\tTime 6.099 (6.145)\tData 0.029 (0.029)\tLoss 3.2447 (3.4156)\tPrec@1 38.867 (39.283)\tPrec@5 62.109 (58.792)\n",
      "Epoch: [0][4180/19336]\tTime 6.128 (6.145)\tData 0.026 (0.029)\tLoss 3.1491 (3.4150)\tPrec@1 41.992 (39.292)\tPrec@5 62.695 (58.801)\n",
      "Epoch: [0][4190/19336]\tTime 6.141 (6.145)\tData 0.032 (0.029)\tLoss 3.2042 (3.4145)\tPrec@1 41.797 (39.298)\tPrec@5 60.547 (58.808)\n",
      "Epoch: [0][4200/19336]\tTime 6.119 (6.145)\tData 0.026 (0.029)\tLoss 3.0394 (3.4138)\tPrec@1 44.727 (39.304)\tPrec@5 65.234 (58.816)\n",
      "Epoch: [0][4210/19336]\tTime 6.117 (6.145)\tData 0.026 (0.029)\tLoss 3.0318 (3.4128)\tPrec@1 45.508 (39.316)\tPrec@5 63.672 (58.829)\n",
      "Epoch: [0][4220/19336]\tTime 6.115 (6.145)\tData 0.030 (0.029)\tLoss 3.2791 (3.4122)\tPrec@1 41.992 (39.323)\tPrec@5 60.352 (58.836)\n",
      "Epoch: [0][4230/19336]\tTime 6.131 (6.145)\tData 0.027 (0.029)\tLoss 2.9418 (3.4113)\tPrec@1 44.336 (39.336)\tPrec@5 64.453 (58.847)\n",
      "Epoch: [0][4240/19336]\tTime 6.115 (6.145)\tData 0.035 (0.029)\tLoss 3.1515 (3.4105)\tPrec@1 41.211 (39.343)\tPrec@5 60.742 (58.855)\n",
      "Epoch: [0][4250/19336]\tTime 6.149 (6.144)\tData 0.029 (0.029)\tLoss 3.2192 (3.4099)\tPrec@1 40.039 (39.350)\tPrec@5 59.961 (58.862)\n",
      "Epoch: [0][4260/19336]\tTime 6.116 (6.144)\tData 0.034 (0.029)\tLoss 2.9338 (3.4092)\tPrec@1 44.922 (39.358)\tPrec@5 66.211 (58.871)\n",
      "Epoch: [0][4270/19336]\tTime 6.129 (6.144)\tData 0.025 (0.029)\tLoss 3.1442 (3.4084)\tPrec@1 40.820 (39.367)\tPrec@5 59.180 (58.880)\n",
      "Epoch: [0][4280/19336]\tTime 6.123 (6.144)\tData 0.026 (0.029)\tLoss 3.3427 (3.4077)\tPrec@1 40.625 (39.377)\tPrec@5 57.617 (58.889)\n",
      "Epoch: [0][4290/19336]\tTime 6.125 (6.144)\tData 0.027 (0.029)\tLoss 3.1923 (3.4070)\tPrec@1 43.359 (39.385)\tPrec@5 61.523 (58.897)\n",
      "Epoch: [0][4300/19336]\tTime 6.113 (6.144)\tData 0.032 (0.029)\tLoss 3.0382 (3.4063)\tPrec@1 46.875 (39.394)\tPrec@5 64.062 (58.906)\n",
      "Epoch: [0][4310/19336]\tTime 6.115 (6.144)\tData 0.031 (0.029)\tLoss 2.9556 (3.4056)\tPrec@1 45.703 (39.404)\tPrec@5 63.672 (58.913)\n",
      "Epoch: [0][4320/19336]\tTime 6.174 (6.144)\tData 0.037 (0.029)\tLoss 3.0720 (3.4047)\tPrec@1 42.969 (39.412)\tPrec@5 63.086 (58.924)\n",
      "Epoch: [0][4330/19336]\tTime 6.112 (6.144)\tData 0.027 (0.029)\tLoss 3.1084 (3.4040)\tPrec@1 40.430 (39.419)\tPrec@5 62.500 (58.931)\n",
      "Epoch: [0][4340/19336]\tTime 6.136 (6.144)\tData 0.049 (0.029)\tLoss 3.0264 (3.4035)\tPrec@1 43.555 (39.425)\tPrec@5 64.062 (58.939)\n",
      "Epoch: [0][4350/19336]\tTime 6.127 (6.144)\tData 0.025 (0.029)\tLoss 2.9735 (3.4028)\tPrec@1 46.289 (39.431)\tPrec@5 64.648 (58.948)\n",
      "Epoch: [0][4360/19336]\tTime 6.129 (6.144)\tData 0.034 (0.029)\tLoss 3.0425 (3.4023)\tPrec@1 47.656 (39.440)\tPrec@5 64.648 (58.955)\n",
      "Epoch: [0][4370/19336]\tTime 6.190 (6.144)\tData 0.056 (0.029)\tLoss 2.9424 (3.4015)\tPrec@1 45.898 (39.451)\tPrec@5 63.672 (58.965)\n",
      "Epoch: [0][4380/19336]\tTime 6.128 (6.144)\tData 0.026 (0.029)\tLoss 3.3004 (3.4009)\tPrec@1 39.258 (39.457)\tPrec@5 59.375 (58.971)\n",
      "Epoch: [0][4390/19336]\tTime 6.136 (6.144)\tData 0.026 (0.029)\tLoss 2.9733 (3.4001)\tPrec@1 48.047 (39.468)\tPrec@5 62.109 (58.981)\n",
      "Epoch: [0][4400/19336]\tTime 6.118 (6.144)\tData 0.031 (0.029)\tLoss 3.2185 (3.3994)\tPrec@1 40.039 (39.477)\tPrec@5 61.719 (58.990)\n",
      "Epoch: [0][4410/19336]\tTime 6.140 (6.144)\tData 0.028 (0.029)\tLoss 3.1609 (3.3986)\tPrec@1 39.844 (39.485)\tPrec@5 61.719 (59.001)\n",
      "Epoch: [0][4420/19336]\tTime 6.120 (6.144)\tData 0.029 (0.029)\tLoss 2.8876 (3.3980)\tPrec@1 47.070 (39.493)\tPrec@5 64.453 (59.008)\n",
      "Epoch: [0][4430/19336]\tTime 6.138 (6.144)\tData 0.026 (0.029)\tLoss 3.0724 (3.3973)\tPrec@1 40.234 (39.499)\tPrec@5 61.328 (59.016)\n",
      "Epoch: [0][4440/19336]\tTime 6.132 (6.144)\tData 0.029 (0.029)\tLoss 3.0512 (3.3967)\tPrec@1 44.141 (39.504)\tPrec@5 62.500 (59.022)\n",
      "Epoch: [0][4450/19336]\tTime 6.096 (6.143)\tData 0.026 (0.029)\tLoss 3.2909 (3.3962)\tPrec@1 40.234 (39.511)\tPrec@5 60.547 (59.029)\n",
      "Epoch: [0][4460/19336]\tTime 6.122 (6.143)\tData 0.026 (0.029)\tLoss 3.0748 (3.3955)\tPrec@1 44.922 (39.520)\tPrec@5 61.328 (59.038)\n",
      "Epoch: [0][4470/19336]\tTime 6.112 (6.143)\tData 0.033 (0.029)\tLoss 2.8828 (3.3949)\tPrec@1 46.875 (39.526)\tPrec@5 65.039 (59.046)\n",
      "Epoch: [0][4480/19336]\tTime 6.125 (6.143)\tData 0.030 (0.029)\tLoss 3.1169 (3.3941)\tPrec@1 42.969 (39.533)\tPrec@5 62.109 (59.056)\n",
      "Epoch: [0][4490/19336]\tTime 6.150 (6.143)\tData 0.026 (0.029)\tLoss 3.3327 (3.3935)\tPrec@1 41.016 (39.540)\tPrec@5 59.766 (59.063)\n",
      "Epoch: [0][4500/19336]\tTime 6.161 (6.143)\tData 0.036 (0.029)\tLoss 2.8515 (3.3929)\tPrec@1 44.336 (39.547)\tPrec@5 67.969 (59.071)\n",
      "Epoch: [0][4510/19336]\tTime 6.089 (6.143)\tData 0.030 (0.029)\tLoss 3.0086 (3.3920)\tPrec@1 43.750 (39.556)\tPrec@5 65.039 (59.081)\n",
      "Epoch: [0][4520/19336]\tTime 6.135 (6.143)\tData 0.026 (0.029)\tLoss 2.9139 (3.3913)\tPrec@1 46.484 (39.563)\tPrec@5 65.820 (59.088)\n",
      "Epoch: [0][4530/19336]\tTime 6.091 (6.143)\tData 0.028 (0.029)\tLoss 3.1698 (3.3908)\tPrec@1 42.578 (39.568)\tPrec@5 61.328 (59.095)\n",
      "Epoch: [0][4540/19336]\tTime 6.067 (6.143)\tData 0.026 (0.029)\tLoss 2.8738 (3.3900)\tPrec@1 47.266 (39.580)\tPrec@5 63.281 (59.104)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0][4550/19336]\tTime 6.130 (6.143)\tData 0.030 (0.029)\tLoss 3.1063 (3.3894)\tPrec@1 40.820 (39.586)\tPrec@5 64.258 (59.112)\n",
      "Epoch: [0][4560/19336]\tTime 6.119 (6.143)\tData 0.026 (0.029)\tLoss 3.0219 (3.3887)\tPrec@1 43.555 (39.594)\tPrec@5 62.500 (59.118)\n",
      "Epoch: [0][4570/19336]\tTime 6.087 (6.143)\tData 0.028 (0.030)\tLoss 3.3757 (3.3882)\tPrec@1 41.406 (39.601)\tPrec@5 59.570 (59.125)\n",
      "Epoch: [0][4580/19336]\tTime 6.103 (6.143)\tData 0.027 (0.030)\tLoss 3.0737 (3.3875)\tPrec@1 44.336 (39.608)\tPrec@5 62.891 (59.134)\n",
      "Epoch: [0][4590/19336]\tTime 6.153 (6.143)\tData 0.032 (0.030)\tLoss 3.3422 (3.3869)\tPrec@1 43.750 (39.614)\tPrec@5 59.570 (59.141)\n",
      "Epoch: [0][4600/19336]\tTime 6.089 (6.143)\tData 0.026 (0.030)\tLoss 3.1094 (3.3863)\tPrec@1 43.359 (39.622)\tPrec@5 61.328 (59.148)\n",
      "Epoch: [0][4610/19336]\tTime 6.161 (6.143)\tData 0.032 (0.030)\tLoss 3.0547 (3.3856)\tPrec@1 43.750 (39.629)\tPrec@5 64.258 (59.155)\n",
      "Epoch: [0][4620/19336]\tTime 6.115 (6.143)\tData 0.027 (0.030)\tLoss 3.1887 (3.3850)\tPrec@1 43.750 (39.638)\tPrec@5 61.328 (59.162)\n",
      "Epoch: [0][4630/19336]\tTime 6.106 (6.143)\tData 0.037 (0.030)\tLoss 3.1036 (3.3844)\tPrec@1 43.359 (39.646)\tPrec@5 61.914 (59.171)\n",
      "Epoch: [0][4640/19336]\tTime 6.122 (6.143)\tData 0.035 (0.030)\tLoss 3.1086 (3.3839)\tPrec@1 43.945 (39.651)\tPrec@5 62.500 (59.176)\n",
      "Epoch: [0][4650/19336]\tTime 6.132 (6.143)\tData 0.035 (0.030)\tLoss 3.1667 (3.3833)\tPrec@1 41.406 (39.659)\tPrec@5 61.133 (59.183)\n",
      "Epoch: [0][4660/19336]\tTime 6.134 (6.142)\tData 0.027 (0.030)\tLoss 3.0860 (3.3826)\tPrec@1 40.820 (39.667)\tPrec@5 65.625 (59.192)\n",
      "Epoch: [0][4670/19336]\tTime 6.139 (6.142)\tData 0.029 (0.030)\tLoss 3.0049 (3.3818)\tPrec@1 44.336 (39.676)\tPrec@5 64.453 (59.201)\n",
      "Epoch: [0][4680/19336]\tTime 6.079 (6.142)\tData 0.027 (0.030)\tLoss 3.2433 (3.3813)\tPrec@1 42.383 (39.682)\tPrec@5 61.914 (59.208)\n",
      "Epoch: [0][4690/19336]\tTime 6.104 (6.142)\tData 0.027 (0.030)\tLoss 3.3210 (3.3808)\tPrec@1 40.234 (39.689)\tPrec@5 56.836 (59.214)\n",
      "Epoch: [0][4700/19336]\tTime 6.093 (6.142)\tData 0.037 (0.030)\tLoss 3.1164 (3.3801)\tPrec@1 41.602 (39.695)\tPrec@5 59.961 (59.222)\n",
      "Epoch: [0][4710/19336]\tTime 6.103 (6.142)\tData 0.030 (0.030)\tLoss 3.1273 (3.3794)\tPrec@1 39.062 (39.701)\tPrec@5 62.305 (59.230)\n",
      "Epoch: [0][4720/19336]\tTime 6.102 (6.142)\tData 0.027 (0.030)\tLoss 3.5060 (3.3790)\tPrec@1 37.695 (39.706)\tPrec@5 58.203 (59.236)\n",
      "Epoch: [0][4730/19336]\tTime 6.104 (6.142)\tData 0.032 (0.030)\tLoss 3.0879 (3.3782)\tPrec@1 42.773 (39.714)\tPrec@5 62.305 (59.244)\n",
      "Epoch: [0][4740/19336]\tTime 6.122 (6.142)\tData 0.031 (0.030)\tLoss 3.1956 (3.3776)\tPrec@1 42.383 (39.723)\tPrec@5 62.891 (59.254)\n",
      "Epoch: [0][4750/19336]\tTime 6.137 (6.142)\tData 0.027 (0.030)\tLoss 2.9228 (3.3769)\tPrec@1 47.461 (39.733)\tPrec@5 66.211 (59.263)\n",
      "Epoch: [0][4760/19336]\tTime 6.143 (6.142)\tData 0.027 (0.030)\tLoss 3.1474 (3.3763)\tPrec@1 42.188 (39.738)\tPrec@5 65.039 (59.271)\n",
      "Epoch: [0][4770/19336]\tTime 6.121 (6.142)\tData 0.028 (0.030)\tLoss 2.9810 (3.3758)\tPrec@1 46.094 (39.744)\tPrec@5 64.648 (59.278)\n",
      "Epoch: [0][4780/19336]\tTime 6.138 (6.142)\tData 0.030 (0.030)\tLoss 3.1172 (3.3751)\tPrec@1 43.164 (39.751)\tPrec@5 64.062 (59.285)\n",
      "Epoch: [0][4790/19336]\tTime 6.118 (6.142)\tData 0.028 (0.030)\tLoss 2.9673 (3.3746)\tPrec@1 46.680 (39.756)\tPrec@5 62.891 (59.290)\n",
      "Epoch: [0][4800/19336]\tTime 6.115 (6.142)\tData 0.030 (0.030)\tLoss 2.9443 (3.3740)\tPrec@1 43.164 (39.762)\tPrec@5 61.328 (59.298)\n",
      "Epoch: [0][4810/19336]\tTime 6.120 (6.142)\tData 0.028 (0.030)\tLoss 3.2878 (3.3735)\tPrec@1 38.281 (39.768)\tPrec@5 61.523 (59.304)\n",
      "Epoch: [0][4820/19336]\tTime 6.138 (6.142)\tData 0.052 (0.030)\tLoss 3.3770 (3.3729)\tPrec@1 39.062 (39.773)\tPrec@5 58.398 (59.311)\n",
      "Epoch: [0][4830/19336]\tTime 6.144 (6.142)\tData 0.027 (0.030)\tLoss 3.0622 (3.3723)\tPrec@1 41.602 (39.780)\tPrec@5 62.500 (59.319)\n",
      "Epoch: [0][4840/19336]\tTime 6.120 (6.142)\tData 0.035 (0.030)\tLoss 3.2338 (3.3717)\tPrec@1 40.625 (39.787)\tPrec@5 61.914 (59.326)\n",
      "Epoch: [0][4850/19336]\tTime 6.117 (6.142)\tData 0.029 (0.030)\tLoss 3.1955 (3.3710)\tPrec@1 44.336 (39.794)\tPrec@5 63.086 (59.335)\n",
      "Epoch: [0][4860/19336]\tTime 6.137 (6.142)\tData 0.028 (0.030)\tLoss 2.8560 (3.3703)\tPrec@1 46.680 (39.802)\tPrec@5 64.453 (59.343)\n",
      "Epoch: [0][4870/19336]\tTime 6.129 (6.142)\tData 0.028 (0.030)\tLoss 2.9769 (3.3697)\tPrec@1 42.383 (39.809)\tPrec@5 64.648 (59.351)\n",
      "Epoch: [0][4880/19336]\tTime 6.107 (6.141)\tData 0.028 (0.030)\tLoss 3.1034 (3.3691)\tPrec@1 40.625 (39.813)\tPrec@5 61.523 (59.356)\n",
      "Epoch: [0][4890/19336]\tTime 6.120 (6.141)\tData 0.039 (0.030)\tLoss 3.2461 (3.3685)\tPrec@1 40.625 (39.819)\tPrec@5 60.547 (59.365)\n",
      "Epoch: [0][4900/19336]\tTime 6.135 (6.141)\tData 0.038 (0.030)\tLoss 2.9247 (3.3680)\tPrec@1 44.531 (39.826)\tPrec@5 64.648 (59.372)\n",
      "Epoch: [0][4910/19336]\tTime 6.116 (6.141)\tData 0.034 (0.030)\tLoss 2.9979 (3.3674)\tPrec@1 44.141 (39.831)\tPrec@5 63.477 (59.379)\n",
      "Epoch: [0][4920/19336]\tTime 6.134 (6.141)\tData 0.035 (0.030)\tLoss 3.0102 (3.3668)\tPrec@1 44.531 (39.839)\tPrec@5 64.258 (59.387)\n",
      "Epoch: [0][4930/19336]\tTime 6.165 (6.141)\tData 0.044 (0.030)\tLoss 3.2792 (3.3664)\tPrec@1 41.797 (39.844)\tPrec@5 59.766 (59.392)\n",
      "Epoch: [0][4940/19336]\tTime 6.084 (6.141)\tData 0.027 (0.030)\tLoss 3.1527 (3.3657)\tPrec@1 41.211 (39.852)\tPrec@5 61.914 (59.399)\n",
      "Epoch: [0][4950/19336]\tTime 6.106 (6.141)\tData 0.036 (0.030)\tLoss 3.0815 (3.3651)\tPrec@1 43.555 (39.860)\tPrec@5 61.523 (59.408)\n",
      "Epoch: [0][4960/19336]\tTime 6.123 (6.141)\tData 0.030 (0.030)\tLoss 3.1530 (3.3644)\tPrec@1 41.211 (39.867)\tPrec@5 62.500 (59.417)\n",
      "Epoch: [0][4970/19336]\tTime 6.113 (6.141)\tData 0.036 (0.030)\tLoss 2.9665 (3.3640)\tPrec@1 43.750 (39.872)\tPrec@5 62.695 (59.421)\n",
      "Epoch: [0][4980/19336]\tTime 6.118 (6.141)\tData 0.036 (0.030)\tLoss 3.1321 (3.3633)\tPrec@1 39.844 (39.880)\tPrec@5 62.305 (59.429)\n",
      "Epoch: [0][4990/19336]\tTime 6.138 (6.141)\tData 0.030 (0.030)\tLoss 2.9957 (3.3627)\tPrec@1 44.141 (39.886)\tPrec@5 62.109 (59.435)\n",
      "Epoch: [0][5000/19336]\tTime 6.110 (6.141)\tData 0.037 (0.030)\tLoss 3.0442 (3.3622)\tPrec@1 42.773 (39.892)\tPrec@5 65.430 (59.443)\n",
      "Epoch: [0][5010/19336]\tTime 6.130 (6.141)\tData 0.033 (0.030)\tLoss 3.0820 (3.3615)\tPrec@1 40.625 (39.900)\tPrec@5 63.672 (59.451)\n",
      "Epoch: [0][5020/19336]\tTime 6.103 (6.141)\tData 0.030 (0.030)\tLoss 3.1678 (3.3608)\tPrec@1 43.555 (39.907)\tPrec@5 62.891 (59.460)\n",
      "Epoch: [0][5030/19336]\tTime 6.146 (6.141)\tData 0.032 (0.030)\tLoss 3.1655 (3.3602)\tPrec@1 40.625 (39.915)\tPrec@5 62.695 (59.468)\n",
      "Epoch: [0][5040/19336]\tTime 6.131 (6.141)\tData 0.034 (0.030)\tLoss 3.0243 (3.3597)\tPrec@1 45.508 (39.922)\tPrec@5 64.062 (59.474)\n",
      "Epoch: [0][5050/19336]\tTime 6.122 (6.141)\tData 0.031 (0.030)\tLoss 3.0045 (3.3592)\tPrec@1 43.359 (39.927)\tPrec@5 64.062 (59.481)\n",
      "Epoch: [0][5060/19336]\tTime 6.146 (6.141)\tData 0.032 (0.030)\tLoss 2.9372 (3.3585)\tPrec@1 44.727 (39.936)\tPrec@5 65.820 (59.489)\n",
      "Epoch: [0][5070/19336]\tTime 6.129 (6.141)\tData 0.027 (0.030)\tLoss 3.1556 (3.3579)\tPrec@1 43.750 (39.944)\tPrec@5 63.867 (59.496)\n",
      "Epoch: [0][5080/19336]\tTime 6.094 (6.141)\tData 0.028 (0.030)\tLoss 3.0100 (3.3574)\tPrec@1 45.117 (39.951)\tPrec@5 64.453 (59.503)\n",
      "Epoch: [0][5090/19336]\tTime 6.114 (6.141)\tData 0.031 (0.030)\tLoss 2.9847 (3.3567)\tPrec@1 46.094 (39.958)\tPrec@5 65.625 (59.511)\n",
      "Epoch: [0][5100/19336]\tTime 6.101 (6.141)\tData 0.034 (0.030)\tLoss 3.1368 (3.3562)\tPrec@1 39.648 (39.964)\tPrec@5 61.328 (59.518)\n",
      "Epoch: [0][5110/19336]\tTime 6.116 (6.141)\tData 0.034 (0.030)\tLoss 3.0177 (3.3555)\tPrec@1 42.188 (39.969)\tPrec@5 64.062 (59.525)\n",
      "Epoch: [0][5120/19336]\tTime 6.037 (6.141)\tData 0.030 (0.030)\tLoss 3.0732 (3.3549)\tPrec@1 44.141 (39.977)\tPrec@5 64.258 (59.533)\n",
      "Epoch: [0][5130/19336]\tTime 6.078 (6.141)\tData 0.028 (0.030)\tLoss 3.1910 (3.3542)\tPrec@1 39.844 (39.984)\tPrec@5 58.984 (59.542)\n",
      "Epoch: [0][5140/19336]\tTime 6.122 (6.141)\tData 0.035 (0.030)\tLoss 3.1914 (3.3536)\tPrec@1 40.430 (39.990)\tPrec@5 61.914 (59.550)\n",
      "Epoch: [0][5150/19336]\tTime 6.088 (6.141)\tData 0.028 (0.030)\tLoss 3.0466 (3.3531)\tPrec@1 41.602 (39.996)\tPrec@5 63.477 (59.556)\n",
      "Epoch: [0][5160/19336]\tTime 6.137 (6.141)\tData 0.036 (0.030)\tLoss 3.2168 (3.3526)\tPrec@1 41.797 (40.002)\tPrec@5 61.523 (59.563)\n",
      "Epoch: [0][5170/19336]\tTime 6.143 (6.141)\tData 0.029 (0.030)\tLoss 3.0054 (3.3520)\tPrec@1 45.312 (40.009)\tPrec@5 64.062 (59.569)\n",
      "Epoch: [0][5180/19336]\tTime 6.116 (6.141)\tData 0.029 (0.030)\tLoss 3.0005 (3.3514)\tPrec@1 45.703 (40.017)\tPrec@5 64.844 (59.576)\n",
      "Epoch: [0][5190/19336]\tTime 6.161 (6.141)\tData 0.038 (0.030)\tLoss 3.1148 (3.3510)\tPrec@1 43.750 (40.022)\tPrec@5 61.523 (59.581)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0][5200/19336]\tTime 6.145 (6.141)\tData 0.034 (0.030)\tLoss 3.2626 (3.3504)\tPrec@1 39.258 (40.029)\tPrec@5 58.008 (59.587)\n",
      "Epoch: [0][5210/19336]\tTime 6.126 (6.141)\tData 0.034 (0.030)\tLoss 3.0567 (3.3499)\tPrec@1 42.578 (40.035)\tPrec@5 63.867 (59.592)\n",
      "Epoch: [0][5220/19336]\tTime 6.118 (6.141)\tData 0.028 (0.030)\tLoss 3.2878 (3.3496)\tPrec@1 40.430 (40.039)\tPrec@5 58.789 (59.596)\n",
      "Epoch: [0][5230/19336]\tTime 6.131 (6.141)\tData 0.034 (0.030)\tLoss 2.9776 (3.3489)\tPrec@1 47.461 (40.046)\tPrec@5 66.016 (59.604)\n",
      "Epoch: [0][5240/19336]\tTime 6.141 (6.141)\tData 0.028 (0.030)\tLoss 2.9059 (3.3484)\tPrec@1 46.680 (40.052)\tPrec@5 66.602 (59.611)\n",
      "Epoch: [0][5250/19336]\tTime 6.113 (6.140)\tData 0.037 (0.030)\tLoss 3.0211 (3.3478)\tPrec@1 44.336 (40.060)\tPrec@5 63.086 (59.618)\n",
      "Epoch: [0][5260/19336]\tTime 6.126 (6.140)\tData 0.033 (0.030)\tLoss 3.0273 (3.3472)\tPrec@1 42.773 (40.067)\tPrec@5 65.430 (59.626)\n",
      "Epoch: [0][5270/19336]\tTime 6.127 (6.140)\tData 0.031 (0.030)\tLoss 3.2069 (3.3466)\tPrec@1 40.625 (40.074)\tPrec@5 60.352 (59.633)\n",
      "Epoch: [0][5280/19336]\tTime 6.099 (6.140)\tData 0.038 (0.030)\tLoss 3.1556 (3.3462)\tPrec@1 44.531 (40.078)\tPrec@5 60.938 (59.638)\n",
      "Epoch: [0][5290/19336]\tTime 6.134 (6.140)\tData 0.028 (0.030)\tLoss 3.1482 (3.3456)\tPrec@1 39.453 (40.085)\tPrec@5 60.156 (59.645)\n",
      "Epoch: [0][5300/19336]\tTime 6.081 (6.140)\tData 0.028 (0.030)\tLoss 3.1175 (3.3451)\tPrec@1 42.578 (40.090)\tPrec@5 62.305 (59.651)\n",
      "Epoch: [0][5310/19336]\tTime 6.104 (6.140)\tData 0.028 (0.030)\tLoss 2.9312 (3.3446)\tPrec@1 44.531 (40.095)\tPrec@5 63.086 (59.657)\n",
      "Epoch: [0][5320/19336]\tTime 6.116 (6.140)\tData 0.034 (0.030)\tLoss 3.3731 (3.3440)\tPrec@1 40.820 (40.104)\tPrec@5 57.227 (59.665)\n",
      "Epoch: [0][5330/19336]\tTime 6.131 (6.140)\tData 0.028 (0.030)\tLoss 3.0778 (3.3434)\tPrec@1 40.430 (40.109)\tPrec@5 61.914 (59.673)\n",
      "Epoch: [0][5340/19336]\tTime 6.138 (6.140)\tData 0.033 (0.030)\tLoss 3.2020 (3.3431)\tPrec@1 43.750 (40.114)\tPrec@5 60.742 (59.677)\n",
      "Epoch: [0][5350/19336]\tTime 6.096 (6.140)\tData 0.029 (0.030)\tLoss 2.8399 (3.3425)\tPrec@1 44.727 (40.121)\tPrec@5 66.797 (59.686)\n",
      "Epoch: [0][5360/19336]\tTime 6.109 (6.140)\tData 0.032 (0.030)\tLoss 3.0504 (3.3421)\tPrec@1 43.555 (40.127)\tPrec@5 63.086 (59.691)\n",
      "Epoch: [0][5370/19336]\tTime 6.122 (6.140)\tData 0.033 (0.030)\tLoss 3.0170 (3.3415)\tPrec@1 42.773 (40.132)\tPrec@5 64.844 (59.699)\n",
      "Epoch: [0][5380/19336]\tTime 6.104 (6.140)\tData 0.032 (0.030)\tLoss 2.9838 (3.3410)\tPrec@1 44.531 (40.138)\tPrec@5 62.109 (59.706)\n",
      "Epoch: [0][5390/19336]\tTime 6.120 (6.140)\tData 0.036 (0.030)\tLoss 3.0933 (3.3404)\tPrec@1 43.359 (40.146)\tPrec@5 62.695 (59.714)\n",
      "Epoch: [0][5400/19336]\tTime 6.110 (6.140)\tData 0.031 (0.030)\tLoss 2.9701 (3.3399)\tPrec@1 44.727 (40.151)\tPrec@5 63.867 (59.721)\n",
      "Epoch: [0][5410/19336]\tTime 6.123 (6.140)\tData 0.038 (0.030)\tLoss 3.0760 (3.3395)\tPrec@1 42.969 (40.157)\tPrec@5 64.258 (59.728)\n",
      "Epoch: [0][5420/19336]\tTime 6.145 (6.140)\tData 0.035 (0.030)\tLoss 3.0773 (3.3388)\tPrec@1 44.336 (40.163)\tPrec@5 63.867 (59.737)\n",
      "Epoch: [0][5430/19336]\tTime 6.092 (6.140)\tData 0.029 (0.030)\tLoss 3.0050 (3.3382)\tPrec@1 45.312 (40.171)\tPrec@5 64.844 (59.744)\n",
      "Epoch: [0][5440/19336]\tTime 6.123 (6.140)\tData 0.034 (0.030)\tLoss 2.9833 (3.3376)\tPrec@1 43.555 (40.178)\tPrec@5 65.430 (59.753)\n",
      "Epoch: [0][5450/19336]\tTime 6.107 (6.140)\tData 0.031 (0.030)\tLoss 3.4034 (3.3374)\tPrec@1 39.062 (40.180)\tPrec@5 61.523 (59.757)\n",
      "Epoch: [0][5460/19336]\tTime 6.099 (6.140)\tData 0.035 (0.030)\tLoss 3.0364 (3.3368)\tPrec@1 43.359 (40.185)\tPrec@5 63.672 (59.764)\n",
      "Epoch: [0][5470/19336]\tTime 6.136 (6.140)\tData 0.035 (0.030)\tLoss 3.3073 (3.3365)\tPrec@1 41.602 (40.189)\tPrec@5 59.375 (59.768)\n",
      "Epoch: [0][5480/19336]\tTime 6.142 (6.140)\tData 0.031 (0.030)\tLoss 3.2066 (3.3361)\tPrec@1 42.578 (40.193)\tPrec@5 59.766 (59.772)\n",
      "Epoch: [0][5490/19336]\tTime 6.109 (6.139)\tData 0.034 (0.030)\tLoss 2.8843 (3.3355)\tPrec@1 47.266 (40.200)\tPrec@5 66.211 (59.780)\n",
      "Epoch: [0][5500/19336]\tTime 6.154 (6.139)\tData 0.029 (0.030)\tLoss 3.1313 (3.3350)\tPrec@1 42.969 (40.207)\tPrec@5 62.109 (59.787)\n",
      "Epoch: [0][5510/19336]\tTime 6.113 (6.139)\tData 0.037 (0.030)\tLoss 3.2094 (3.3344)\tPrec@1 43.750 (40.213)\tPrec@5 59.961 (59.794)\n",
      "Epoch: [0][5520/19336]\tTime 6.094 (6.139)\tData 0.029 (0.030)\tLoss 3.3327 (3.3339)\tPrec@1 38.477 (40.217)\tPrec@5 59.375 (59.799)\n",
      "Epoch: [0][5530/19336]\tTime 6.108 (6.139)\tData 0.030 (0.030)\tLoss 3.0378 (3.3333)\tPrec@1 42.773 (40.225)\tPrec@5 62.695 (59.807)\n",
      "Epoch: [0][5540/19336]\tTime 6.113 (6.139)\tData 0.030 (0.030)\tLoss 3.1653 (3.3328)\tPrec@1 40.039 (40.232)\tPrec@5 61.914 (59.813)\n",
      "Epoch: [0][5550/19336]\tTime 6.107 (6.139)\tData 0.031 (0.030)\tLoss 2.8763 (3.3322)\tPrec@1 45.312 (40.238)\tPrec@5 65.430 (59.819)\n",
      "Epoch: [0][5560/19336]\tTime 6.129 (6.139)\tData 0.032 (0.030)\tLoss 2.7986 (3.3317)\tPrec@1 45.703 (40.243)\tPrec@5 67.188 (59.825)\n",
      "Epoch: [0][5570/19336]\tTime 6.162 (6.139)\tData 0.036 (0.030)\tLoss 3.0466 (3.3312)\tPrec@1 42.969 (40.249)\tPrec@5 63.086 (59.831)\n",
      "Epoch: [0][5580/19336]\tTime 6.116 (6.139)\tData 0.029 (0.030)\tLoss 3.1668 (3.3307)\tPrec@1 39.648 (40.254)\tPrec@5 61.328 (59.837)\n",
      "Epoch: [0][5590/19336]\tTime 6.110 (6.139)\tData 0.029 (0.030)\tLoss 3.0459 (3.3300)\tPrec@1 44.531 (40.262)\tPrec@5 63.672 (59.847)\n",
      "Epoch: [0][5600/19336]\tTime 6.118 (6.139)\tData 0.030 (0.030)\tLoss 3.2296 (3.3296)\tPrec@1 42.578 (40.267)\tPrec@5 59.766 (59.851)\n",
      "Epoch: [0][5610/19336]\tTime 6.132 (6.139)\tData 0.032 (0.030)\tLoss 2.9848 (3.3291)\tPrec@1 43.945 (40.273)\tPrec@5 62.305 (59.858)\n",
      "Epoch: [0][5620/19336]\tTime 6.098 (6.139)\tData 0.039 (0.030)\tLoss 3.1463 (3.3287)\tPrec@1 44.531 (40.278)\tPrec@5 62.305 (59.862)\n",
      "Epoch: [0][5630/19336]\tTime 6.114 (6.139)\tData 0.030 (0.030)\tLoss 3.1691 (3.3283)\tPrec@1 41.016 (40.283)\tPrec@5 63.281 (59.869)\n",
      "Epoch: [0][5640/19336]\tTime 6.132 (6.139)\tData 0.037 (0.030)\tLoss 3.1940 (3.3279)\tPrec@1 42.188 (40.287)\tPrec@5 63.086 (59.873)\n",
      "Epoch: [0][5650/19336]\tTime 6.111 (6.139)\tData 0.035 (0.030)\tLoss 2.9615 (3.3273)\tPrec@1 43.164 (40.295)\tPrec@5 64.453 (59.881)\n",
      "Epoch: [0][5660/19336]\tTime 6.115 (6.139)\tData 0.038 (0.030)\tLoss 3.1695 (3.3268)\tPrec@1 42.969 (40.301)\tPrec@5 61.914 (59.887)\n",
      "Epoch: [0][5670/19336]\tTime 6.098 (6.139)\tData 0.040 (0.030)\tLoss 2.9485 (3.3263)\tPrec@1 44.922 (40.306)\tPrec@5 63.867 (59.894)\n",
      "Epoch: [0][5680/19336]\tTime 6.122 (6.139)\tData 0.041 (0.030)\tLoss 3.0903 (3.3257)\tPrec@1 42.578 (40.312)\tPrec@5 61.328 (59.901)\n",
      "Epoch: [0][5690/19336]\tTime 6.134 (6.139)\tData 0.030 (0.030)\tLoss 3.3719 (3.3252)\tPrec@1 39.453 (40.318)\tPrec@5 60.352 (59.907)\n",
      "Epoch: [0][5700/19336]\tTime 6.137 (6.139)\tData 0.030 (0.030)\tLoss 3.0851 (3.3248)\tPrec@1 42.188 (40.322)\tPrec@5 62.109 (59.914)\n",
      "Epoch: [0][5710/19336]\tTime 6.131 (6.139)\tData 0.031 (0.030)\tLoss 2.9835 (3.3243)\tPrec@1 43.945 (40.328)\tPrec@5 65.039 (59.921)\n",
      "Epoch: [0][5720/19336]\tTime 6.032 (6.139)\tData 0.030 (0.030)\tLoss 3.0761 (3.3239)\tPrec@1 41.016 (40.331)\tPrec@5 61.523 (59.925)\n",
      "Epoch: [0][5730/19336]\tTime 6.162 (6.139)\tData 0.030 (0.030)\tLoss 2.7842 (3.3235)\tPrec@1 47.656 (40.336)\tPrec@5 65.820 (59.931)\n",
      "Epoch: [0][5740/19336]\tTime 6.124 (6.139)\tData 0.031 (0.030)\tLoss 2.8254 (3.3229)\tPrec@1 47.656 (40.343)\tPrec@5 66.211 (59.937)\n",
      "Epoch: [0][5750/19336]\tTime 6.138 (6.139)\tData 0.032 (0.030)\tLoss 3.0106 (3.3224)\tPrec@1 44.141 (40.349)\tPrec@5 63.477 (59.944)\n",
      "Epoch: [0][5760/19336]\tTime 6.126 (6.139)\tData 0.038 (0.030)\tLoss 3.0374 (3.3219)\tPrec@1 42.188 (40.354)\tPrec@5 63.672 (59.952)\n",
      "Epoch: [0][5770/19336]\tTime 6.111 (6.138)\tData 0.034 (0.030)\tLoss 3.0395 (3.3212)\tPrec@1 42.969 (40.362)\tPrec@5 63.672 (59.961)\n",
      "Epoch: [0][5780/19336]\tTime 6.129 (6.138)\tData 0.030 (0.030)\tLoss 3.2238 (3.3207)\tPrec@1 41.211 (40.367)\tPrec@5 58.984 (59.967)\n",
      "Epoch: [0][5790/19336]\tTime 6.102 (6.138)\tData 0.032 (0.030)\tLoss 2.9763 (3.3202)\tPrec@1 41.406 (40.373)\tPrec@5 62.891 (59.974)\n",
      "Epoch: [0][5800/19336]\tTime 6.068 (6.138)\tData 0.039 (0.030)\tLoss 2.9035 (3.3197)\tPrec@1 47.656 (40.378)\tPrec@5 67.773 (59.980)\n",
      "Epoch: [0][5810/19336]\tTime 6.111 (6.138)\tData 0.034 (0.030)\tLoss 3.1654 (3.3194)\tPrec@1 41.211 (40.382)\tPrec@5 62.109 (59.985)\n",
      "Epoch: [0][5820/19336]\tTime 6.103 (6.138)\tData 0.039 (0.030)\tLoss 2.9503 (3.3190)\tPrec@1 43.359 (40.386)\tPrec@5 61.719 (59.989)\n",
      "Epoch: [0][5830/19336]\tTime 6.118 (6.138)\tData 0.038 (0.030)\tLoss 3.0014 (3.3184)\tPrec@1 44.531 (40.392)\tPrec@5 63.086 (59.996)\n",
      "Epoch: [0][5840/19336]\tTime 6.111 (6.138)\tData 0.033 (0.030)\tLoss 2.8968 (3.3180)\tPrec@1 46.875 (40.397)\tPrec@5 65.625 (60.000)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0][5850/19336]\tTime 6.125 (6.138)\tData 0.042 (0.030)\tLoss 3.1063 (3.3176)\tPrec@1 43.750 (40.402)\tPrec@5 60.742 (60.006)\n",
      "Epoch: [0][5860/19336]\tTime 6.124 (6.138)\tData 0.036 (0.030)\tLoss 3.1616 (3.3172)\tPrec@1 41.602 (40.408)\tPrec@5 60.938 (60.011)\n",
      "Epoch: [0][5870/19336]\tTime 6.139 (6.138)\tData 0.030 (0.030)\tLoss 3.1530 (3.3168)\tPrec@1 43.945 (40.414)\tPrec@5 64.648 (60.016)\n",
      "Epoch: [0][5880/19336]\tTime 6.116 (6.138)\tData 0.036 (0.030)\tLoss 2.9527 (3.3162)\tPrec@1 43.164 (40.420)\tPrec@5 63.867 (60.024)\n",
      "Epoch: [0][5890/19336]\tTime 6.145 (6.138)\tData 0.041 (0.030)\tLoss 3.0797 (3.3158)\tPrec@1 44.336 (40.425)\tPrec@5 63.672 (60.028)\n",
      "Epoch: [0][5900/19336]\tTime 6.136 (6.138)\tData 0.033 (0.030)\tLoss 2.9771 (3.3154)\tPrec@1 43.164 (40.430)\tPrec@5 63.086 (60.034)\n",
      "Epoch: [0][5910/19336]\tTime 6.130 (6.138)\tData 0.036 (0.030)\tLoss 3.1390 (3.3149)\tPrec@1 42.578 (40.435)\tPrec@5 61.133 (60.039)\n",
      "Epoch: [0][5920/19336]\tTime 6.122 (6.138)\tData 0.038 (0.030)\tLoss 2.8091 (3.3143)\tPrec@1 45.508 (40.442)\tPrec@5 67.188 (60.047)\n",
      "Epoch: [0][5930/19336]\tTime 6.158 (6.138)\tData 0.031 (0.030)\tLoss 3.1686 (3.3138)\tPrec@1 39.648 (40.448)\tPrec@5 60.938 (60.053)\n",
      "Epoch: [0][5940/19336]\tTime 6.179 (6.138)\tData 0.031 (0.030)\tLoss 3.1029 (3.3134)\tPrec@1 42.383 (40.452)\tPrec@5 63.281 (60.058)\n",
      "Epoch: [0][5950/19336]\tTime 6.148 (6.138)\tData 0.057 (0.030)\tLoss 3.1247 (3.3130)\tPrec@1 42.578 (40.456)\tPrec@5 62.891 (60.063)\n",
      "Epoch: [0][5960/19336]\tTime 6.120 (6.138)\tData 0.034 (0.030)\tLoss 2.7889 (3.3124)\tPrec@1 44.727 (40.462)\tPrec@5 65.430 (60.070)\n",
      "Epoch: [0][5970/19336]\tTime 6.116 (6.138)\tData 0.040 (0.030)\tLoss 2.9817 (3.3120)\tPrec@1 42.578 (40.467)\tPrec@5 63.867 (60.075)\n",
      "Epoch: [0][5980/19336]\tTime 6.112 (6.138)\tData 0.034 (0.030)\tLoss 2.9037 (3.3115)\tPrec@1 44.531 (40.474)\tPrec@5 64.062 (60.083)\n",
      "Epoch: [0][5990/19336]\tTime 6.125 (6.138)\tData 0.036 (0.030)\tLoss 3.0042 (3.3111)\tPrec@1 44.336 (40.478)\tPrec@5 64.453 (60.088)\n",
      "Epoch: [0][6000/19336]\tTime 6.117 (6.138)\tData 0.032 (0.030)\tLoss 3.1432 (3.3107)\tPrec@1 43.164 (40.482)\tPrec@5 63.281 (60.094)\n",
      "Epoch: [0][6010/19336]\tTime 6.124 (6.138)\tData 0.037 (0.031)\tLoss 3.0144 (3.3102)\tPrec@1 43.945 (40.486)\tPrec@5 61.914 (60.098)\n",
      "Epoch: [0][6020/19336]\tTime 6.132 (6.138)\tData 0.037 (0.031)\tLoss 3.2638 (3.3098)\tPrec@1 41.797 (40.491)\tPrec@5 60.352 (60.102)\n",
      "Epoch: [0][6030/19336]\tTime 6.087 (6.138)\tData 0.034 (0.031)\tLoss 2.9650 (3.3093)\tPrec@1 43.945 (40.495)\tPrec@5 65.820 (60.109)\n",
      "Epoch: [0][6040/19336]\tTime 6.134 (6.138)\tData 0.034 (0.031)\tLoss 3.1366 (3.3090)\tPrec@1 41.992 (40.499)\tPrec@5 60.938 (60.114)\n",
      "Epoch: [0][6050/19336]\tTime 6.106 (6.138)\tData 0.031 (0.031)\tLoss 2.9441 (3.3085)\tPrec@1 45.703 (40.504)\tPrec@5 64.648 (60.119)\n",
      "Epoch: [0][6060/19336]\tTime 6.138 (6.138)\tData 0.031 (0.031)\tLoss 3.1520 (3.3081)\tPrec@1 42.188 (40.510)\tPrec@5 61.719 (60.126)\n",
      "Epoch: [0][6070/19336]\tTime 6.089 (6.138)\tData 0.040 (0.031)\tLoss 3.0398 (3.3074)\tPrec@1 43.555 (40.518)\tPrec@5 61.914 (60.133)\n",
      "Epoch: [0][6080/19336]\tTime 6.151 (6.138)\tData 0.031 (0.031)\tLoss 3.0270 (3.3069)\tPrec@1 45.117 (40.524)\tPrec@5 64.258 (60.140)\n",
      "Epoch: [0][6090/19336]\tTime 6.146 (6.138)\tData 0.030 (0.031)\tLoss 2.8784 (3.3065)\tPrec@1 45.898 (40.531)\tPrec@5 66.211 (60.146)\n",
      "Epoch: [0][6100/19336]\tTime 6.136 (6.138)\tData 0.034 (0.031)\tLoss 3.1350 (3.3061)\tPrec@1 40.820 (40.535)\tPrec@5 64.648 (60.153)\n",
      "Epoch: [0][6110/19336]\tTime 6.084 (6.138)\tData 0.033 (0.031)\tLoss 3.1041 (3.3057)\tPrec@1 42.773 (40.540)\tPrec@5 63.477 (60.158)\n",
      "Epoch: [0][6120/19336]\tTime 6.160 (6.138)\tData 0.032 (0.031)\tLoss 3.1947 (3.3053)\tPrec@1 41.211 (40.544)\tPrec@5 61.914 (60.165)\n",
      "Epoch: [0][6130/19336]\tTime 6.113 (6.138)\tData 0.030 (0.031)\tLoss 3.3189 (3.3049)\tPrec@1 39.453 (40.549)\tPrec@5 59.180 (60.168)\n",
      "Epoch: [0][6140/19336]\tTime 6.120 (6.138)\tData 0.035 (0.031)\tLoss 3.2915 (3.3045)\tPrec@1 40.234 (40.555)\tPrec@5 58.594 (60.174)\n",
      "Epoch: [0][6150/19336]\tTime 6.118 (6.138)\tData 0.035 (0.031)\tLoss 2.9626 (3.3041)\tPrec@1 45.312 (40.559)\tPrec@5 63.867 (60.180)\n",
      "Epoch: [0][6160/19336]\tTime 6.105 (6.138)\tData 0.031 (0.031)\tLoss 2.9321 (3.3037)\tPrec@1 44.336 (40.565)\tPrec@5 64.453 (60.185)\n",
      "Epoch: [0][6170/19336]\tTime 6.161 (6.138)\tData 0.031 (0.031)\tLoss 2.9955 (3.3032)\tPrec@1 45.117 (40.570)\tPrec@5 63.672 (60.190)\n",
      "Epoch: [0][6180/19336]\tTime 6.131 (6.138)\tData 0.030 (0.031)\tLoss 3.1608 (3.3028)\tPrec@1 40.820 (40.574)\tPrec@5 61.133 (60.195)\n",
      "Epoch: [0][6190/19336]\tTime 6.134 (6.138)\tData 0.039 (0.031)\tLoss 3.0910 (3.3023)\tPrec@1 44.727 (40.580)\tPrec@5 62.500 (60.200)\n",
      "Epoch: [0][6200/19336]\tTime 6.112 (6.138)\tData 0.034 (0.031)\tLoss 3.3321 (3.3020)\tPrec@1 39.258 (40.585)\tPrec@5 59.766 (60.204)\n",
      "Epoch: [0][6210/19336]\tTime 6.110 (6.138)\tData 0.036 (0.031)\tLoss 3.1686 (3.3016)\tPrec@1 44.531 (40.589)\tPrec@5 62.695 (60.210)\n",
      "Epoch: [0][6220/19336]\tTime 6.245 (6.138)\tData 0.031 (0.031)\tLoss 2.9138 (3.3012)\tPrec@1 45.508 (40.594)\tPrec@5 65.234 (60.215)\n",
      "Epoch: [0][6230/19336]\tTime 6.117 (6.138)\tData 0.038 (0.031)\tLoss 2.8051 (3.3008)\tPrec@1 45.508 (40.599)\tPrec@5 64.648 (60.220)\n",
      "Epoch: [0][6240/19336]\tTime 6.112 (6.138)\tData 0.036 (0.031)\tLoss 3.1200 (3.3004)\tPrec@1 43.750 (40.603)\tPrec@5 62.109 (60.226)\n",
      "Epoch: [0][6250/19336]\tTime 6.087 (6.138)\tData 0.031 (0.031)\tLoss 3.2030 (3.3000)\tPrec@1 40.820 (40.607)\tPrec@5 60.742 (60.230)\n",
      "Epoch: [0][6260/19336]\tTime 6.130 (6.138)\tData 0.033 (0.031)\tLoss 2.9021 (3.2995)\tPrec@1 43.945 (40.612)\tPrec@5 66.602 (60.235)\n",
      "Epoch: [0][6270/19336]\tTime 6.106 (6.138)\tData 0.036 (0.031)\tLoss 3.2072 (3.2992)\tPrec@1 41.211 (40.616)\tPrec@5 61.719 (60.239)\n",
      "Epoch: [0][6280/19336]\tTime 6.102 (6.138)\tData 0.042 (0.031)\tLoss 2.9078 (3.2988)\tPrec@1 46.875 (40.622)\tPrec@5 66.602 (60.244)\n",
      "Epoch: [0][6290/19336]\tTime 6.127 (6.138)\tData 0.032 (0.031)\tLoss 2.9713 (3.2983)\tPrec@1 44.922 (40.628)\tPrec@5 66.016 (60.251)\n",
      "Epoch: [0][6300/19336]\tTime 6.118 (6.137)\tData 0.039 (0.031)\tLoss 3.0043 (3.2979)\tPrec@1 43.750 (40.632)\tPrec@5 66.992 (60.256)\n",
      "Epoch: [0][6310/19336]\tTime 6.115 (6.137)\tData 0.035 (0.031)\tLoss 3.0451 (3.2977)\tPrec@1 42.188 (40.636)\tPrec@5 63.086 (60.261)\n",
      "Epoch: [0][6320/19336]\tTime 6.144 (6.137)\tData 0.041 (0.031)\tLoss 3.1028 (3.2973)\tPrec@1 43.555 (40.641)\tPrec@5 61.914 (60.266)\n",
      "Epoch: [0][6330/19336]\tTime 6.103 (6.137)\tData 0.043 (0.031)\tLoss 3.1545 (3.2968)\tPrec@1 40.820 (40.645)\tPrec@5 60.742 (60.273)\n",
      "Epoch: [0][6340/19336]\tTime 6.154 (6.137)\tData 0.043 (0.031)\tLoss 2.9953 (3.2964)\tPrec@1 43.555 (40.650)\tPrec@5 63.086 (60.278)\n",
      "Epoch: [0][6350/19336]\tTime 6.050 (6.137)\tData 0.038 (0.031)\tLoss 3.0953 (3.2960)\tPrec@1 44.531 (40.655)\tPrec@5 63.477 (60.284)\n",
      "Epoch: [0][6360/19336]\tTime 6.115 (6.137)\tData 0.037 (0.031)\tLoss 3.0429 (3.2955)\tPrec@1 42.578 (40.660)\tPrec@5 64.062 (60.290)\n",
      "Epoch: [0][6370/19336]\tTime 6.096 (6.137)\tData 0.034 (0.031)\tLoss 2.8609 (3.2951)\tPrec@1 44.922 (40.664)\tPrec@5 67.773 (60.295)\n",
      "Epoch: [0][6380/19336]\tTime 6.092 (6.137)\tData 0.041 (0.031)\tLoss 3.1835 (3.2947)\tPrec@1 42.188 (40.668)\tPrec@5 61.719 (60.300)\n",
      "Epoch: [0][6390/19336]\tTime 6.122 (6.137)\tData 0.037 (0.031)\tLoss 2.8887 (3.2943)\tPrec@1 45.312 (40.672)\tPrec@5 66.211 (60.306)\n",
      "Epoch: [0][6400/19336]\tTime 6.139 (6.137)\tData 0.040 (0.031)\tLoss 2.9988 (3.2939)\tPrec@1 42.578 (40.675)\tPrec@5 63.867 (60.309)\n",
      "Epoch: [0][6410/19336]\tTime 6.122 (6.137)\tData 0.034 (0.031)\tLoss 2.9939 (3.2935)\tPrec@1 46.094 (40.679)\tPrec@5 64.062 (60.315)\n",
      "Epoch: [0][6420/19336]\tTime 6.156 (6.137)\tData 0.049 (0.031)\tLoss 2.9038 (3.2931)\tPrec@1 45.508 (40.684)\tPrec@5 63.672 (60.318)\n",
      "Epoch: [0][6430/19336]\tTime 6.091 (6.137)\tData 0.033 (0.031)\tLoss 3.1809 (3.2927)\tPrec@1 42.773 (40.688)\tPrec@5 59.766 (60.324)\n",
      "Epoch: [0][6440/19336]\tTime 6.111 (6.137)\tData 0.064 (0.031)\tLoss 3.1120 (3.2923)\tPrec@1 44.922 (40.693)\tPrec@5 61.914 (60.330)\n",
      "Epoch: [0][6450/19336]\tTime 6.109 (6.137)\tData 0.037 (0.031)\tLoss 3.0800 (3.2918)\tPrec@1 43.359 (40.699)\tPrec@5 63.477 (60.335)\n",
      "Epoch: [0][6460/19336]\tTime 6.080 (6.137)\tData 0.035 (0.031)\tLoss 2.9469 (3.2915)\tPrec@1 47.070 (40.702)\tPrec@5 67.383 (60.341)\n",
      "Epoch: [0][6470/19336]\tTime 6.108 (6.137)\tData 0.040 (0.031)\tLoss 2.9288 (3.2910)\tPrec@1 47.266 (40.708)\tPrec@5 63.867 (60.345)\n",
      "Epoch: [0][6480/19336]\tTime 6.105 (6.137)\tData 0.042 (0.031)\tLoss 3.0192 (3.2907)\tPrec@1 42.188 (40.713)\tPrec@5 62.891 (60.350)\n",
      "Epoch: [0][6490/19336]\tTime 6.132 (6.137)\tData 0.035 (0.031)\tLoss 3.1031 (3.2903)\tPrec@1 42.188 (40.717)\tPrec@5 62.305 (60.354)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0][6500/19336]\tTime 6.153 (6.137)\tData 0.039 (0.031)\tLoss 3.0378 (3.2898)\tPrec@1 43.164 (40.723)\tPrec@5 63.867 (60.360)\n",
      "Epoch: [0][6510/19336]\tTime 6.126 (6.137)\tData 0.035 (0.031)\tLoss 3.0784 (3.2894)\tPrec@1 42.773 (40.730)\tPrec@5 62.891 (60.363)\n",
      "Epoch: [0][6520/19336]\tTime 6.137 (6.137)\tData 0.033 (0.031)\tLoss 2.9650 (3.2891)\tPrec@1 44.336 (40.734)\tPrec@5 66.406 (60.369)\n",
      "Epoch: [0][6530/19336]\tTime 6.128 (6.137)\tData 0.035 (0.031)\tLoss 3.1815 (3.2886)\tPrec@1 43.750 (40.741)\tPrec@5 63.086 (60.376)\n",
      "Epoch: [0][6540/19336]\tTime 6.102 (6.137)\tData 0.033 (0.031)\tLoss 3.1740 (3.2882)\tPrec@1 41.602 (40.745)\tPrec@5 62.500 (60.382)\n",
      "Epoch: [0][6550/19336]\tTime 6.209 (6.137)\tData 0.042 (0.031)\tLoss 2.9978 (3.2878)\tPrec@1 42.773 (40.750)\tPrec@5 62.109 (60.386)\n",
      "Epoch: [0][6560/19336]\tTime 6.123 (6.137)\tData 0.046 (0.031)\tLoss 2.9329 (3.2875)\tPrec@1 43.164 (40.753)\tPrec@5 65.234 (60.390)\n",
      "Epoch: [0][6570/19336]\tTime 6.095 (6.137)\tData 0.032 (0.031)\tLoss 3.0709 (3.2872)\tPrec@1 44.531 (40.758)\tPrec@5 62.500 (60.395)\n",
      "Epoch: [0][6580/19336]\tTime 6.116 (6.137)\tData 0.036 (0.031)\tLoss 2.9272 (3.2866)\tPrec@1 46.094 (40.764)\tPrec@5 63.867 (60.401)\n",
      "Epoch: [0][6590/19336]\tTime 6.135 (6.137)\tData 0.039 (0.031)\tLoss 3.0763 (3.2864)\tPrec@1 41.211 (40.766)\tPrec@5 64.648 (60.405)\n",
      "Epoch: [0][6600/19336]\tTime 6.121 (6.137)\tData 0.040 (0.031)\tLoss 2.8952 (3.2860)\tPrec@1 46.875 (40.770)\tPrec@5 64.648 (60.411)\n",
      "Epoch: [0][6610/19336]\tTime 6.087 (6.137)\tData 0.035 (0.031)\tLoss 3.0098 (3.2855)\tPrec@1 43.555 (40.776)\tPrec@5 63.086 (60.417)\n",
      "Epoch: [0][6620/19336]\tTime 6.119 (6.137)\tData 0.038 (0.031)\tLoss 2.9671 (3.2851)\tPrec@1 44.336 (40.780)\tPrec@5 65.820 (60.423)\n",
      "Epoch: [0][6630/19336]\tTime 6.119 (6.137)\tData 0.040 (0.031)\tLoss 2.7211 (3.2848)\tPrec@1 47.852 (40.784)\tPrec@5 69.141 (60.428)\n",
      "Epoch: [0][6640/19336]\tTime 6.119 (6.137)\tData 0.035 (0.031)\tLoss 2.9940 (3.2845)\tPrec@1 44.531 (40.788)\tPrec@5 65.039 (60.431)\n",
      "Epoch: [0][6650/19336]\tTime 6.110 (6.137)\tData 0.039 (0.031)\tLoss 2.9911 (3.2840)\tPrec@1 44.727 (40.793)\tPrec@5 63.672 (60.437)\n",
      "Epoch: [0][6660/19336]\tTime 6.135 (6.137)\tData 0.038 (0.031)\tLoss 3.1273 (3.2837)\tPrec@1 41.797 (40.797)\tPrec@5 63.281 (60.441)\n",
      "Epoch: [0][6670/19336]\tTime 6.124 (6.137)\tData 0.039 (0.031)\tLoss 3.1077 (3.2833)\tPrec@1 42.188 (40.801)\tPrec@5 63.281 (60.444)\n",
      "Epoch: [0][6680/19336]\tTime 6.125 (6.137)\tData 0.032 (0.031)\tLoss 3.0694 (3.2829)\tPrec@1 42.969 (40.805)\tPrec@5 61.914 (60.448)\n",
      "Epoch: [0][6690/19336]\tTime 6.175 (6.137)\tData 0.041 (0.031)\tLoss 3.1386 (3.2826)\tPrec@1 43.164 (40.809)\tPrec@5 59.570 (60.452)\n",
      "Epoch: [0][6700/19336]\tTime 6.129 (6.137)\tData 0.034 (0.031)\tLoss 3.0304 (3.2823)\tPrec@1 44.141 (40.813)\tPrec@5 64.258 (60.457)\n",
      "Epoch: [0][6710/19336]\tTime 6.093 (6.137)\tData 0.040 (0.031)\tLoss 3.0740 (3.2819)\tPrec@1 42.188 (40.818)\tPrec@5 62.109 (60.462)\n",
      "Epoch: [0][6720/19336]\tTime 6.068 (6.137)\tData 0.032 (0.031)\tLoss 3.0175 (3.2815)\tPrec@1 43.555 (40.821)\tPrec@5 63.672 (60.467)\n",
      "Epoch: [0][6730/19336]\tTime 6.128 (6.136)\tData 0.033 (0.031)\tLoss 3.0317 (3.2812)\tPrec@1 41.406 (40.826)\tPrec@5 62.109 (60.474)\n",
      "Epoch: [0][6740/19336]\tTime 6.104 (6.136)\tData 0.036 (0.031)\tLoss 2.8091 (3.2807)\tPrec@1 47.656 (40.831)\tPrec@5 68.359 (60.480)\n",
      "Epoch: [0][6750/19336]\tTime 6.125 (6.136)\tData 0.040 (0.031)\tLoss 3.0556 (3.2804)\tPrec@1 43.164 (40.834)\tPrec@5 63.281 (60.485)\n",
      "Epoch: [0][6760/19336]\tTime 6.076 (6.136)\tData 0.037 (0.031)\tLoss 2.8234 (3.2799)\tPrec@1 45.703 (40.840)\tPrec@5 67.188 (60.492)\n",
      "Epoch: [0][6770/19336]\tTime 6.118 (6.136)\tData 0.042 (0.031)\tLoss 2.9898 (3.2795)\tPrec@1 42.969 (40.844)\tPrec@5 64.453 (60.496)\n",
      "Epoch: [0][6780/19336]\tTime 6.112 (6.136)\tData 0.032 (0.031)\tLoss 3.2680 (3.2792)\tPrec@1 43.164 (40.849)\tPrec@5 61.523 (60.501)\n",
      "Epoch: [0][6790/19336]\tTime 6.118 (6.136)\tData 0.032 (0.031)\tLoss 2.9935 (3.2789)\tPrec@1 41.211 (40.851)\tPrec@5 63.477 (60.503)\n",
      "Epoch: [0][6800/19336]\tTime 6.113 (6.136)\tData 0.033 (0.031)\tLoss 2.7800 (3.2785)\tPrec@1 44.141 (40.856)\tPrec@5 67.578 (60.509)\n",
      "Epoch: [0][6810/19336]\tTime 6.108 (6.136)\tData 0.033 (0.031)\tLoss 3.1871 (3.2781)\tPrec@1 42.578 (40.861)\tPrec@5 61.133 (60.514)\n",
      "Epoch: [0][6820/19336]\tTime 6.145 (6.136)\tData 0.047 (0.031)\tLoss 2.8522 (3.2778)\tPrec@1 44.141 (40.864)\tPrec@5 67.188 (60.518)\n",
      "Epoch: [0][6830/19336]\tTime 6.098 (6.136)\tData 0.034 (0.031)\tLoss 3.2044 (3.2775)\tPrec@1 42.188 (40.867)\tPrec@5 58.398 (60.521)\n",
      "Epoch: [0][6840/19336]\tTime 6.102 (6.136)\tData 0.037 (0.031)\tLoss 3.0392 (3.2772)\tPrec@1 43.555 (40.873)\tPrec@5 66.406 (60.526)\n",
      "Epoch: [0][6850/19336]\tTime 6.117 (6.136)\tData 0.033 (0.031)\tLoss 3.1819 (3.2768)\tPrec@1 43.164 (40.877)\tPrec@5 61.328 (60.531)\n",
      "Epoch: [0][6860/19336]\tTime 6.068 (6.136)\tData 0.033 (0.031)\tLoss 2.9994 (3.2764)\tPrec@1 45.703 (40.881)\tPrec@5 66.211 (60.537)\n",
      "Epoch: [0][6870/19336]\tTime 6.139 (6.136)\tData 0.033 (0.031)\tLoss 3.2481 (3.2761)\tPrec@1 42.578 (40.884)\tPrec@5 60.938 (60.540)\n",
      "Epoch: [0][6880/19336]\tTime 6.150 (6.136)\tData 0.046 (0.031)\tLoss 3.0771 (3.2757)\tPrec@1 42.383 (40.889)\tPrec@5 63.086 (60.545)\n",
      "Epoch: [0][6890/19336]\tTime 6.138 (6.136)\tData 0.059 (0.031)\tLoss 2.9456 (3.2753)\tPrec@1 46.289 (40.894)\tPrec@5 64.648 (60.551)\n",
      "Epoch: [0][6900/19336]\tTime 6.136 (6.136)\tData 0.033 (0.031)\tLoss 3.1899 (3.2750)\tPrec@1 44.727 (40.899)\tPrec@5 62.891 (60.555)\n",
      "Epoch: [0][6910/19336]\tTime 6.147 (6.136)\tData 0.044 (0.031)\tLoss 2.9555 (3.2746)\tPrec@1 46.289 (40.904)\tPrec@5 65.430 (60.560)\n",
      "Epoch: [0][6920/19336]\tTime 6.110 (6.136)\tData 0.034 (0.031)\tLoss 2.9466 (3.2742)\tPrec@1 46.289 (40.908)\tPrec@5 65.430 (60.565)\n",
      "Epoch: [0][6930/19336]\tTime 6.125 (6.136)\tData 0.033 (0.031)\tLoss 2.7895 (3.2739)\tPrec@1 47.656 (40.913)\tPrec@5 67.188 (60.570)\n",
      "Epoch: [0][6940/19336]\tTime 6.093 (6.136)\tData 0.034 (0.031)\tLoss 2.8712 (3.2735)\tPrec@1 49.023 (40.918)\tPrec@5 65.625 (60.574)\n",
      "Epoch: [0][6950/19336]\tTime 6.107 (6.136)\tData 0.041 (0.031)\tLoss 2.8233 (3.2731)\tPrec@1 44.922 (40.923)\tPrec@5 66.406 (60.580)\n",
      "Epoch: [0][6960/19336]\tTime 6.100 (6.136)\tData 0.033 (0.031)\tLoss 3.1266 (3.2727)\tPrec@1 41.016 (40.928)\tPrec@5 62.891 (60.586)\n",
      "Epoch: [0][6970/19336]\tTime 6.139 (6.136)\tData 0.035 (0.031)\tLoss 3.0578 (3.2723)\tPrec@1 43.945 (40.932)\tPrec@5 62.695 (60.591)\n",
      "Epoch: [0][6980/19336]\tTime 6.109 (6.136)\tData 0.043 (0.031)\tLoss 3.0793 (3.2719)\tPrec@1 42.969 (40.936)\tPrec@5 62.500 (60.596)\n",
      "Epoch: [0][6990/19336]\tTime 6.087 (6.136)\tData 0.033 (0.031)\tLoss 3.1465 (3.2715)\tPrec@1 41.406 (40.942)\tPrec@5 60.547 (60.602)\n",
      "Epoch: [0][7000/19336]\tTime 6.121 (6.136)\tData 0.037 (0.031)\tLoss 3.0727 (3.2711)\tPrec@1 44.727 (40.946)\tPrec@5 63.281 (60.607)\n",
      "Epoch: [0][7010/19336]\tTime 6.157 (6.136)\tData 0.035 (0.032)\tLoss 2.9658 (3.2707)\tPrec@1 42.578 (40.950)\tPrec@5 64.062 (60.613)\n",
      "Epoch: [0][7020/19336]\tTime 6.124 (6.136)\tData 0.034 (0.032)\tLoss 2.9846 (3.2703)\tPrec@1 43.164 (40.955)\tPrec@5 63.672 (60.618)\n",
      "Epoch: [0][7030/19336]\tTime 6.123 (6.136)\tData 0.034 (0.032)\tLoss 3.0614 (3.2699)\tPrec@1 46.484 (40.960)\tPrec@5 64.062 (60.623)\n",
      "Epoch: [0][7040/19336]\tTime 6.124 (6.136)\tData 0.040 (0.032)\tLoss 3.1363 (3.2695)\tPrec@1 44.727 (40.964)\tPrec@5 63.867 (60.628)\n",
      "Epoch: [0][7050/19336]\tTime 6.121 (6.136)\tData 0.039 (0.032)\tLoss 2.7668 (3.2692)\tPrec@1 49.219 (40.969)\tPrec@5 65.820 (60.631)\n",
      "Epoch: [0][7060/19336]\tTime 6.127 (6.136)\tData 0.035 (0.032)\tLoss 3.0607 (3.2688)\tPrec@1 42.773 (40.973)\tPrec@5 63.867 (60.636)\n",
      "Epoch: [0][7070/19336]\tTime 6.132 (6.136)\tData 0.039 (0.032)\tLoss 3.0135 (3.2685)\tPrec@1 41.797 (40.976)\tPrec@5 64.648 (60.640)\n",
      "Epoch: [0][7080/19336]\tTime 6.123 (6.136)\tData 0.041 (0.032)\tLoss 2.9300 (3.2682)\tPrec@1 46.680 (40.980)\tPrec@5 65.625 (60.644)\n",
      "Epoch: [0][7090/19336]\tTime 6.144 (6.136)\tData 0.044 (0.032)\tLoss 3.1540 (3.2679)\tPrec@1 41.211 (40.983)\tPrec@5 60.156 (60.648)\n",
      "Epoch: [0][7100/19336]\tTime 6.100 (6.136)\tData 0.040 (0.032)\tLoss 3.2338 (3.2675)\tPrec@1 40.820 (40.987)\tPrec@5 60.352 (60.653)\n",
      "Epoch: [0][7110/19336]\tTime 6.121 (6.136)\tData 0.034 (0.032)\tLoss 3.3587 (3.2672)\tPrec@1 40.039 (40.991)\tPrec@5 58.203 (60.656)\n",
      "Epoch: [0][7120/19336]\tTime 6.114 (6.136)\tData 0.037 (0.032)\tLoss 2.9534 (3.2668)\tPrec@1 45.703 (40.996)\tPrec@5 65.039 (60.661)\n",
      "Epoch: [0][7130/19336]\tTime 6.131 (6.136)\tData 0.042 (0.032)\tLoss 2.8298 (3.2665)\tPrec@1 46.289 (41.000)\tPrec@5 65.039 (60.666)\n",
      "Epoch: [0][7140/19336]\tTime 6.061 (6.136)\tData 0.034 (0.032)\tLoss 3.1655 (3.2662)\tPrec@1 41.602 (41.003)\tPrec@5 60.547 (60.669)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0][7150/19336]\tTime 6.135 (6.136)\tData 0.045 (0.032)\tLoss 2.9579 (3.2659)\tPrec@1 45.117 (41.006)\tPrec@5 66.016 (60.673)\n",
      "Epoch: [0][7160/19336]\tTime 6.113 (6.136)\tData 0.038 (0.032)\tLoss 3.1435 (3.2656)\tPrec@1 42.773 (41.009)\tPrec@5 62.109 (60.676)\n",
      "Epoch: [0][7170/19336]\tTime 6.143 (6.136)\tData 0.045 (0.032)\tLoss 2.8959 (3.2653)\tPrec@1 48.047 (41.013)\tPrec@5 63.672 (60.680)\n",
      "Epoch: [0][7180/19336]\tTime 6.141 (6.136)\tData 0.041 (0.032)\tLoss 3.0843 (3.2648)\tPrec@1 40.625 (41.018)\tPrec@5 63.672 (60.685)\n",
      "Epoch: [0][7190/19336]\tTime 6.123 (6.136)\tData 0.039 (0.032)\tLoss 3.1477 (3.2645)\tPrec@1 43.359 (41.022)\tPrec@5 62.500 (60.690)\n",
      "Epoch: [0][7200/19336]\tTime 6.116 (6.136)\tData 0.042 (0.032)\tLoss 2.9381 (3.2641)\tPrec@1 46.289 (41.027)\tPrec@5 63.477 (60.695)\n",
      "Epoch: [0][7210/19336]\tTime 6.105 (6.136)\tData 0.035 (0.032)\tLoss 3.2019 (3.2638)\tPrec@1 41.016 (41.030)\tPrec@5 61.914 (60.699)\n",
      "Epoch: [0][7220/19336]\tTime 6.115 (6.136)\tData 0.044 (0.032)\tLoss 3.2061 (3.2635)\tPrec@1 43.164 (41.034)\tPrec@5 62.109 (60.704)\n",
      "Epoch: [0][7230/19336]\tTime 6.206 (6.136)\tData 0.043 (0.032)\tLoss 2.9554 (3.2631)\tPrec@1 42.969 (41.037)\tPrec@5 64.062 (60.708)\n",
      "Epoch: [0][7240/19336]\tTime 6.078 (6.136)\tData 0.035 (0.032)\tLoss 3.2650 (3.2627)\tPrec@1 38.477 (41.041)\tPrec@5 61.133 (60.713)\n",
      "Epoch: [0][7250/19336]\tTime 6.112 (6.136)\tData 0.040 (0.032)\tLoss 3.1412 (3.2624)\tPrec@1 42.383 (41.046)\tPrec@5 63.281 (60.717)\n",
      "Epoch: [0][7260/19336]\tTime 6.062 (6.136)\tData 0.035 (0.032)\tLoss 3.1688 (3.2621)\tPrec@1 41.211 (41.049)\tPrec@5 63.672 (60.721)\n",
      "Epoch: [0][7270/19336]\tTime 6.148 (6.136)\tData 0.035 (0.032)\tLoss 2.8397 (3.2616)\tPrec@1 48.438 (41.056)\tPrec@5 65.820 (60.727)\n",
      "Epoch: [0][7280/19336]\tTime 6.125 (6.136)\tData 0.042 (0.032)\tLoss 3.1763 (3.2613)\tPrec@1 40.430 (41.060)\tPrec@5 58.398 (60.731)\n",
      "Epoch: [0][7290/19336]\tTime 6.121 (6.136)\tData 0.034 (0.032)\tLoss 3.0648 (3.2609)\tPrec@1 44.531 (41.064)\tPrec@5 63.477 (60.735)\n",
      "Epoch: [0][7300/19336]\tTime 6.112 (6.136)\tData 0.043 (0.032)\tLoss 2.9589 (3.2605)\tPrec@1 42.383 (41.069)\tPrec@5 65.625 (60.740)\n",
      "Epoch: [0][7310/19336]\tTime 6.120 (6.136)\tData 0.033 (0.032)\tLoss 3.1259 (3.2602)\tPrec@1 42.383 (41.073)\tPrec@5 61.914 (60.744)\n",
      "Epoch: [0][7320/19336]\tTime 6.096 (6.136)\tData 0.039 (0.032)\tLoss 2.9791 (3.2598)\tPrec@1 43.945 (41.077)\tPrec@5 66.797 (60.748)\n",
      "Epoch: [0][7330/19336]\tTime 6.103 (6.136)\tData 0.050 (0.032)\tLoss 3.0052 (3.2595)\tPrec@1 43.945 (41.082)\tPrec@5 63.086 (60.752)\n",
      "Epoch: [0][7340/19336]\tTime 6.105 (6.136)\tData 0.040 (0.032)\tLoss 2.9388 (3.2592)\tPrec@1 45.117 (41.084)\tPrec@5 64.062 (60.755)\n",
      "Epoch: [0][7350/19336]\tTime 6.125 (6.135)\tData 0.043 (0.032)\tLoss 2.9435 (3.2588)\tPrec@1 46.289 (41.089)\tPrec@5 66.602 (60.761)\n",
      "Epoch: [0][7360/19336]\tTime 6.087 (6.135)\tData 0.034 (0.032)\tLoss 3.1182 (3.2584)\tPrec@1 41.211 (41.092)\tPrec@5 63.086 (60.766)\n",
      "Epoch: [0][7370/19336]\tTime 6.149 (6.135)\tData 0.037 (0.032)\tLoss 3.1735 (3.2582)\tPrec@1 42.383 (41.095)\tPrec@5 60.938 (60.769)\n",
      "Epoch: [0][7380/19336]\tTime 6.127 (6.135)\tData 0.045 (0.032)\tLoss 2.9982 (3.2579)\tPrec@1 44.531 (41.099)\tPrec@5 63.867 (60.773)\n",
      "Epoch: [0][7390/19336]\tTime 6.099 (6.135)\tData 0.046 (0.032)\tLoss 2.8601 (3.2575)\tPrec@1 46.289 (41.103)\tPrec@5 65.430 (60.778)\n",
      "Epoch: [0][7400/19336]\tTime 6.145 (6.135)\tData 0.043 (0.032)\tLoss 2.9074 (3.2573)\tPrec@1 45.703 (41.106)\tPrec@5 64.258 (60.780)\n",
      "Epoch: [0][7410/19336]\tTime 6.015 (6.135)\tData 0.044 (0.032)\tLoss 3.1526 (3.2570)\tPrec@1 42.578 (41.110)\tPrec@5 63.086 (60.784)\n",
      "Epoch: [0][7420/19336]\tTime 6.115 (6.135)\tData 0.044 (0.032)\tLoss 2.8650 (3.2566)\tPrec@1 45.898 (41.114)\tPrec@5 66.797 (60.789)\n",
      "Epoch: [0][7430/19336]\tTime 6.097 (6.135)\tData 0.046 (0.032)\tLoss 3.0360 (3.2563)\tPrec@1 45.117 (41.117)\tPrec@5 61.719 (60.792)\n",
      "Epoch: [0][7440/19336]\tTime 6.114 (6.135)\tData 0.035 (0.032)\tLoss 2.9858 (3.2561)\tPrec@1 44.336 (41.120)\tPrec@5 65.820 (60.795)\n",
      "Epoch: [0][7450/19336]\tTime 6.137 (6.135)\tData 0.035 (0.032)\tLoss 3.2006 (3.2558)\tPrec@1 41.992 (41.123)\tPrec@5 59.766 (60.799)\n",
      "Epoch: [0][7460/19336]\tTime 6.125 (6.135)\tData 0.047 (0.032)\tLoss 3.0385 (3.2555)\tPrec@1 45.312 (41.127)\tPrec@5 61.523 (60.802)\n",
      "Epoch: [0][7470/19336]\tTime 6.140 (6.135)\tData 0.040 (0.032)\tLoss 2.9649 (3.2552)\tPrec@1 44.727 (41.131)\tPrec@5 63.086 (60.805)\n",
      "Epoch: [0][7480/19336]\tTime 6.109 (6.135)\tData 0.039 (0.032)\tLoss 2.8392 (3.2548)\tPrec@1 45.508 (41.135)\tPrec@5 66.211 (60.810)\n",
      "Epoch: [0][7490/19336]\tTime 6.140 (6.135)\tData 0.038 (0.032)\tLoss 3.0078 (3.2546)\tPrec@1 44.727 (41.138)\tPrec@5 64.453 (60.814)\n",
      "Epoch: [0][7500/19336]\tTime 6.088 (6.135)\tData 0.036 (0.032)\tLoss 3.0333 (3.2543)\tPrec@1 46.875 (41.141)\tPrec@5 64.844 (60.817)\n",
      "Epoch: [0][7510/19336]\tTime 6.121 (6.135)\tData 0.040 (0.032)\tLoss 3.0805 (3.2540)\tPrec@1 44.336 (41.145)\tPrec@5 62.109 (60.821)\n",
      "Epoch: [0][7520/19336]\tTime 6.119 (6.135)\tData 0.040 (0.032)\tLoss 3.2247 (3.2537)\tPrec@1 41.797 (41.150)\tPrec@5 60.547 (60.825)\n",
      "Epoch: [0][7530/19336]\tTime 6.137 (6.135)\tData 0.037 (0.032)\tLoss 2.9597 (3.2534)\tPrec@1 44.531 (41.155)\tPrec@5 65.430 (60.829)\n",
      "Epoch: [0][7540/19336]\tTime 6.144 (6.135)\tData 0.036 (0.032)\tLoss 3.0259 (3.2531)\tPrec@1 42.383 (41.158)\tPrec@5 62.109 (60.832)\n",
      "Epoch: [0][7550/19336]\tTime 6.109 (6.135)\tData 0.035 (0.032)\tLoss 3.0746 (3.2528)\tPrec@1 42.383 (41.162)\tPrec@5 62.109 (60.837)\n",
      "Epoch: [0][7560/19336]\tTime 6.137 (6.135)\tData 0.042 (0.032)\tLoss 2.8200 (3.2524)\tPrec@1 46.875 (41.166)\tPrec@5 64.648 (60.840)\n",
      "Epoch: [0][7570/19336]\tTime 6.116 (6.135)\tData 0.044 (0.032)\tLoss 2.9145 (3.2522)\tPrec@1 44.727 (41.170)\tPrec@5 64.453 (60.843)\n",
      "Epoch: [0][7580/19336]\tTime 6.131 (6.135)\tData 0.037 (0.032)\tLoss 3.0965 (3.2518)\tPrec@1 45.117 (41.175)\tPrec@5 64.453 (60.848)\n",
      "Epoch: [0][7590/19336]\tTime 6.112 (6.135)\tData 0.042 (0.032)\tLoss 3.0727 (3.2515)\tPrec@1 42.969 (41.179)\tPrec@5 65.234 (60.852)\n",
      "Epoch: [0][7600/19336]\tTime 6.058 (6.135)\tData 0.043 (0.032)\tLoss 2.9176 (3.2511)\tPrec@1 46.289 (41.183)\tPrec@5 66.016 (60.857)\n",
      "Epoch: [0][7610/19336]\tTime 6.115 (6.135)\tData 0.039 (0.032)\tLoss 2.9463 (3.2508)\tPrec@1 44.727 (41.187)\tPrec@5 64.258 (60.860)\n",
      "Epoch: [0][7620/19336]\tTime 6.167 (6.135)\tData 0.037 (0.032)\tLoss 2.9581 (3.2504)\tPrec@1 46.680 (41.192)\tPrec@5 65.430 (60.866)\n",
      "Epoch: [0][7630/19336]\tTime 6.138 (6.135)\tData 0.038 (0.032)\tLoss 3.0626 (3.2500)\tPrec@1 43.945 (41.198)\tPrec@5 61.328 (60.869)\n",
      "Epoch: [0][7640/19336]\tTime 6.103 (6.135)\tData 0.038 (0.032)\tLoss 2.9532 (3.2497)\tPrec@1 47.070 (41.201)\tPrec@5 65.234 (60.874)\n",
      "Epoch: [0][7650/19336]\tTime 6.132 (6.135)\tData 0.041 (0.032)\tLoss 2.8202 (3.2494)\tPrec@1 45.312 (41.206)\tPrec@5 68.750 (60.878)\n",
      "Epoch: [0][7660/19336]\tTime 6.125 (6.135)\tData 0.043 (0.032)\tLoss 3.1177 (3.2491)\tPrec@1 41.211 (41.210)\tPrec@5 63.281 (60.881)\n",
      "Epoch: [0][7670/19336]\tTime 6.129 (6.135)\tData 0.039 (0.032)\tLoss 3.0011 (3.2488)\tPrec@1 44.531 (41.213)\tPrec@5 62.109 (60.884)\n",
      "Epoch: [0][7680/19336]\tTime 6.134 (6.135)\tData 0.042 (0.032)\tLoss 2.9144 (3.2484)\tPrec@1 44.336 (41.218)\tPrec@5 65.039 (60.889)\n",
      "Epoch: [0][7690/19336]\tTime 6.098 (6.135)\tData 0.035 (0.032)\tLoss 2.8103 (3.2481)\tPrec@1 43.555 (41.222)\tPrec@5 67.188 (60.893)\n",
      "Epoch: [0][7700/19336]\tTime 6.089 (6.135)\tData 0.041 (0.032)\tLoss 2.9745 (3.2477)\tPrec@1 47.070 (41.226)\tPrec@5 65.234 (60.898)\n",
      "Epoch: [0][7710/19336]\tTime 6.126 (6.135)\tData 0.036 (0.032)\tLoss 2.9455 (3.2474)\tPrec@1 45.508 (41.230)\tPrec@5 66.211 (60.903)\n",
      "Epoch: [0][7720/19336]\tTime 6.112 (6.135)\tData 0.039 (0.032)\tLoss 3.0409 (3.2471)\tPrec@1 44.141 (41.234)\tPrec@5 63.867 (60.906)\n",
      "Epoch: [0][7730/19336]\tTime 6.126 (6.135)\tData 0.038 (0.032)\tLoss 2.9421 (3.2469)\tPrec@1 43.164 (41.236)\tPrec@5 65.820 (60.908)\n",
      "Epoch: [0][7740/19336]\tTime 6.161 (6.135)\tData 0.037 (0.032)\tLoss 3.1510 (3.2466)\tPrec@1 43.945 (41.242)\tPrec@5 62.305 (60.912)\n",
      "Epoch: [0][7750/19336]\tTime 6.112 (6.135)\tData 0.038 (0.032)\tLoss 3.1219 (3.2463)\tPrec@1 42.773 (41.245)\tPrec@5 63.281 (60.915)\n",
      "Epoch: [0][7760/19336]\tTime 6.190 (6.135)\tData 0.040 (0.032)\tLoss 3.1936 (3.2461)\tPrec@1 43.359 (41.247)\tPrec@5 62.891 (60.918)\n",
      "Epoch: [0][7770/19336]\tTime 6.110 (6.135)\tData 0.044 (0.032)\tLoss 2.8269 (3.2458)\tPrec@1 47.852 (41.252)\tPrec@5 67.188 (60.923)\n",
      "Epoch: [0][7780/19336]\tTime 6.142 (6.135)\tData 0.036 (0.032)\tLoss 2.7624 (3.2455)\tPrec@1 46.094 (41.255)\tPrec@5 68.555 (60.927)\n",
      "Epoch: [0][7790/19336]\tTime 6.109 (6.135)\tData 0.036 (0.032)\tLoss 3.1724 (3.2452)\tPrec@1 42.383 (41.258)\tPrec@5 60.742 (60.929)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0][7800/19336]\tTime 6.127 (6.135)\tData 0.042 (0.032)\tLoss 2.8530 (3.2449)\tPrec@1 47.461 (41.261)\tPrec@5 65.430 (60.932)\n",
      "Epoch: [0][7810/19336]\tTime 6.127 (6.135)\tData 0.040 (0.032)\tLoss 3.2043 (3.2446)\tPrec@1 38.672 (41.264)\tPrec@5 60.547 (60.936)\n",
      "Epoch: [0][7820/19336]\tTime 6.066 (6.135)\tData 0.041 (0.032)\tLoss 2.9168 (3.2444)\tPrec@1 46.680 (41.267)\tPrec@5 64.844 (60.940)\n",
      "Epoch: [0][7830/19336]\tTime 6.102 (6.135)\tData 0.045 (0.032)\tLoss 3.0417 (3.2441)\tPrec@1 44.531 (41.271)\tPrec@5 64.062 (60.944)\n",
      "Epoch: [0][7840/19336]\tTime 6.107 (6.135)\tData 0.035 (0.032)\tLoss 2.9665 (3.2438)\tPrec@1 45.117 (41.274)\tPrec@5 64.258 (60.946)\n",
      "Epoch: [0][7850/19336]\tTime 6.148 (6.135)\tData 0.037 (0.032)\tLoss 2.8501 (3.2435)\tPrec@1 46.680 (41.279)\tPrec@5 66.211 (60.952)\n",
      "Epoch: [0][7860/19336]\tTime 6.125 (6.134)\tData 0.043 (0.032)\tLoss 3.2717 (3.2431)\tPrec@1 42.578 (41.282)\tPrec@5 62.891 (60.955)\n",
      "Epoch: [0][7870/19336]\tTime 6.109 (6.134)\tData 0.038 (0.032)\tLoss 3.0720 (3.2428)\tPrec@1 43.359 (41.285)\tPrec@5 64.258 (60.959)\n",
      "Epoch: [0][7880/19336]\tTime 6.078 (6.134)\tData 0.045 (0.032)\tLoss 3.1610 (3.2424)\tPrec@1 43.164 (41.290)\tPrec@5 64.062 (60.963)\n",
      "Epoch: [0][7890/19336]\tTime 6.165 (6.134)\tData 0.052 (0.032)\tLoss 2.9834 (3.2421)\tPrec@1 43.945 (41.295)\tPrec@5 63.672 (60.967)\n",
      "Epoch: [0][7900/19336]\tTime 6.140 (6.134)\tData 0.043 (0.032)\tLoss 2.9528 (3.2417)\tPrec@1 46.094 (41.300)\tPrec@5 63.281 (60.972)\n",
      "Epoch: [0][7910/19336]\tTime 6.122 (6.134)\tData 0.043 (0.032)\tLoss 3.0156 (3.2414)\tPrec@1 42.383 (41.304)\tPrec@5 63.477 (60.976)\n",
      "Epoch: [0][7920/19336]\tTime 6.124 (6.134)\tData 0.040 (0.032)\tLoss 2.7872 (3.2411)\tPrec@1 49.609 (41.308)\tPrec@5 68.359 (60.980)\n",
      "Epoch: [0][7930/19336]\tTime 6.108 (6.134)\tData 0.040 (0.032)\tLoss 2.9503 (3.2408)\tPrec@1 40.820 (41.311)\tPrec@5 63.477 (60.982)\n",
      "Epoch: [0][7940/19336]\tTime 6.158 (6.134)\tData 0.035 (0.032)\tLoss 2.9323 (3.2405)\tPrec@1 45.898 (41.315)\tPrec@5 65.234 (60.986)\n",
      "Epoch: [0][7950/19336]\tTime 6.138 (6.134)\tData 0.036 (0.032)\tLoss 2.9459 (3.2402)\tPrec@1 45.508 (41.318)\tPrec@5 66.797 (60.989)\n",
      "Epoch: [0][7960/19336]\tTime 6.101 (6.134)\tData 0.045 (0.032)\tLoss 2.8304 (3.2399)\tPrec@1 44.336 (41.321)\tPrec@5 65.625 (60.993)\n",
      "Epoch: [0][7970/19336]\tTime 6.131 (6.134)\tData 0.036 (0.033)\tLoss 2.9836 (3.2396)\tPrec@1 47.656 (41.325)\tPrec@5 66.016 (60.997)\n",
      "Epoch: [0][7980/19336]\tTime 6.136 (6.134)\tData 0.041 (0.033)\tLoss 2.9925 (3.2393)\tPrec@1 43.164 (41.328)\tPrec@5 65.820 (61.001)\n",
      "Epoch: [0][7990/19336]\tTime 6.107 (6.134)\tData 0.044 (0.033)\tLoss 2.9071 (3.2391)\tPrec@1 47.070 (41.331)\tPrec@5 66.016 (61.004)\n",
      "Epoch: [0][8000/19336]\tTime 6.106 (6.134)\tData 0.039 (0.033)\tLoss 2.9837 (3.2389)\tPrec@1 45.508 (41.334)\tPrec@5 65.430 (61.007)\n",
      "Epoch: [0][8010/19336]\tTime 6.127 (6.134)\tData 0.050 (0.033)\tLoss 3.2539 (3.2386)\tPrec@1 42.383 (41.337)\tPrec@5 61.719 (61.010)\n",
      "Epoch: [0][8020/19336]\tTime 6.117 (6.134)\tData 0.049 (0.033)\tLoss 3.1161 (3.2384)\tPrec@1 38.672 (41.340)\tPrec@5 60.156 (61.013)\n",
      "Epoch: [0][8030/19336]\tTime 6.115 (6.134)\tData 0.040 (0.033)\tLoss 2.8147 (3.2381)\tPrec@1 44.531 (41.343)\tPrec@5 67.383 (61.017)\n",
      "Epoch: [0][8040/19336]\tTime 6.131 (6.134)\tData 0.041 (0.033)\tLoss 2.7717 (3.2378)\tPrec@1 47.070 (41.346)\tPrec@5 65.820 (61.021)\n",
      "Epoch: [0][8050/19336]\tTime 6.114 (6.134)\tData 0.038 (0.033)\tLoss 2.9353 (3.2375)\tPrec@1 46.484 (41.349)\tPrec@5 64.258 (61.023)\n",
      "Epoch: [0][8060/19336]\tTime 6.091 (6.134)\tData 0.039 (0.033)\tLoss 3.0862 (3.2372)\tPrec@1 41.406 (41.352)\tPrec@5 65.234 (61.028)\n",
      "Epoch: [0][8070/19336]\tTime 6.093 (6.134)\tData 0.037 (0.033)\tLoss 3.1380 (3.2370)\tPrec@1 42.383 (41.355)\tPrec@5 62.695 (61.032)\n",
      "Epoch: [0][8080/19336]\tTime 6.132 (6.134)\tData 0.039 (0.033)\tLoss 2.8702 (3.2367)\tPrec@1 44.727 (41.358)\tPrec@5 66.797 (61.036)\n",
      "Epoch: [0][8090/19336]\tTime 6.085 (6.134)\tData 0.036 (0.033)\tLoss 3.1569 (3.2365)\tPrec@1 41.602 (41.361)\tPrec@5 63.672 (61.039)\n",
      "Epoch: [0][8100/19336]\tTime 6.105 (6.134)\tData 0.042 (0.033)\tLoss 3.0604 (3.2362)\tPrec@1 42.773 (41.364)\tPrec@5 63.867 (61.044)\n",
      "Epoch: [0][8110/19336]\tTime 6.113 (6.134)\tData 0.049 (0.033)\tLoss 2.8237 (3.2359)\tPrec@1 47.070 (41.367)\tPrec@5 65.820 (61.048)\n",
      "Epoch: [0][8120/19336]\tTime 6.109 (6.134)\tData 0.047 (0.033)\tLoss 2.9785 (3.2356)\tPrec@1 45.312 (41.371)\tPrec@5 64.648 (61.052)\n",
      "Epoch: [0][8130/19336]\tTime 6.110 (6.134)\tData 0.042 (0.033)\tLoss 2.9315 (3.2353)\tPrec@1 43.945 (41.374)\tPrec@5 65.234 (61.056)\n",
      "Epoch: [0][8140/19336]\tTime 6.098 (6.134)\tData 0.042 (0.033)\tLoss 3.0724 (3.2350)\tPrec@1 46.680 (41.378)\tPrec@5 63.672 (61.059)\n",
      "Epoch: [0][8150/19336]\tTime 6.093 (6.134)\tData 0.041 (0.033)\tLoss 2.8130 (3.2347)\tPrec@1 46.289 (41.381)\tPrec@5 64.062 (61.063)\n",
      "Epoch: [0][8160/19336]\tTime 6.095 (6.134)\tData 0.038 (0.033)\tLoss 2.9383 (3.2345)\tPrec@1 40.820 (41.384)\tPrec@5 64.844 (61.065)\n",
      "Epoch: [0][8170/19336]\tTime 6.143 (6.134)\tData 0.037 (0.033)\tLoss 2.9694 (3.2342)\tPrec@1 41.797 (41.387)\tPrec@5 66.016 (61.069)\n",
      "Epoch: [0][8180/19336]\tTime 6.149 (6.134)\tData 0.037 (0.033)\tLoss 2.8816 (3.2339)\tPrec@1 42.773 (41.390)\tPrec@5 66.016 (61.073)\n",
      "Epoch: [0][8190/19336]\tTime 6.121 (6.134)\tData 0.041 (0.033)\tLoss 3.0595 (3.2336)\tPrec@1 41.797 (41.393)\tPrec@5 63.477 (61.076)\n",
      "Epoch: [0][8200/19336]\tTime 6.172 (6.134)\tData 0.039 (0.033)\tLoss 3.0811 (3.2332)\tPrec@1 41.992 (41.397)\tPrec@5 64.062 (61.081)\n",
      "Epoch: [0][8210/19336]\tTime 6.108 (6.134)\tData 0.037 (0.033)\tLoss 3.0961 (3.2330)\tPrec@1 40.430 (41.400)\tPrec@5 61.719 (61.084)\n",
      "Epoch: [0][8220/19336]\tTime 6.092 (6.134)\tData 0.037 (0.033)\tLoss 3.2183 (3.2328)\tPrec@1 43.359 (41.403)\tPrec@5 61.719 (61.087)\n",
      "Epoch: [0][8230/19336]\tTime 6.085 (6.134)\tData 0.041 (0.033)\tLoss 2.9216 (3.2326)\tPrec@1 45.508 (41.404)\tPrec@5 64.648 (61.089)\n",
      "Epoch: [0][8240/19336]\tTime 6.117 (6.134)\tData 0.037 (0.033)\tLoss 3.1492 (3.2324)\tPrec@1 42.773 (41.407)\tPrec@5 63.086 (61.092)\n",
      "Epoch: [0][8250/19336]\tTime 6.097 (6.134)\tData 0.037 (0.033)\tLoss 3.1994 (3.2322)\tPrec@1 39.453 (41.410)\tPrec@5 59.961 (61.094)\n",
      "Epoch: [0][8260/19336]\tTime 6.111 (6.134)\tData 0.043 (0.033)\tLoss 3.2167 (3.2319)\tPrec@1 42.188 (41.413)\tPrec@5 60.547 (61.097)\n",
      "Epoch: [0][8270/19336]\tTime 6.132 (6.134)\tData 0.049 (0.033)\tLoss 2.7686 (3.2315)\tPrec@1 46.484 (41.418)\tPrec@5 66.211 (61.102)\n",
      "Epoch: [0][8280/19336]\tTime 6.120 (6.134)\tData 0.045 (0.033)\tLoss 2.8967 (3.2311)\tPrec@1 47.852 (41.424)\tPrec@5 65.430 (61.106)\n",
      "Epoch: [0][8290/19336]\tTime 6.103 (6.134)\tData 0.038 (0.033)\tLoss 3.0839 (3.2309)\tPrec@1 42.773 (41.427)\tPrec@5 63.086 (61.110)\n",
      "Epoch: [0][8300/19336]\tTime 6.128 (6.134)\tData 0.046 (0.033)\tLoss 3.2527 (3.2306)\tPrec@1 41.016 (41.430)\tPrec@5 58.398 (61.113)\n",
      "Epoch: [0][8310/19336]\tTime 6.128 (6.134)\tData 0.042 (0.033)\tLoss 2.9479 (3.2303)\tPrec@1 44.531 (41.434)\tPrec@5 63.867 (61.116)\n",
      "Epoch: [0][8320/19336]\tTime 6.163 (6.134)\tData 0.049 (0.033)\tLoss 3.0219 (3.2300)\tPrec@1 42.578 (41.438)\tPrec@5 61.523 (61.120)\n",
      "Epoch: [0][8330/19336]\tTime 6.075 (6.134)\tData 0.040 (0.033)\tLoss 2.9365 (3.2297)\tPrec@1 44.727 (41.442)\tPrec@5 66.211 (61.124)\n",
      "Epoch: [0][8340/19336]\tTime 6.122 (6.134)\tData 0.047 (0.033)\tLoss 3.0636 (3.2293)\tPrec@1 44.531 (41.447)\tPrec@5 60.938 (61.128)\n",
      "Epoch: [0][8350/19336]\tTime 6.091 (6.134)\tData 0.037 (0.033)\tLoss 2.9618 (3.2290)\tPrec@1 43.945 (41.451)\tPrec@5 64.844 (61.133)\n",
      "Epoch: [0][8360/19336]\tTime 6.120 (6.134)\tData 0.047 (0.033)\tLoss 2.8872 (3.2287)\tPrec@1 47.461 (41.455)\tPrec@5 66.016 (61.136)\n",
      "Epoch: [0][8370/19336]\tTime 6.126 (6.134)\tData 0.037 (0.033)\tLoss 3.0041 (3.2284)\tPrec@1 45.117 (41.457)\tPrec@5 65.234 (61.140)\n",
      "Epoch: [0][8380/19336]\tTime 6.130 (6.134)\tData 0.048 (0.033)\tLoss 2.8855 (3.2282)\tPrec@1 44.336 (41.460)\tPrec@5 65.625 (61.143)\n",
      "Epoch: [0][8390/19336]\tTime 6.119 (6.134)\tData 0.037 (0.033)\tLoss 3.0795 (3.2280)\tPrec@1 41.797 (41.462)\tPrec@5 61.523 (61.144)\n",
      "Epoch: [0][8400/19336]\tTime 6.107 (6.134)\tData 0.037 (0.033)\tLoss 2.8511 (3.2278)\tPrec@1 44.531 (41.464)\tPrec@5 65.234 (61.148)\n",
      "Epoch: [0][8410/19336]\tTime 6.113 (6.134)\tData 0.042 (0.033)\tLoss 3.0113 (3.2275)\tPrec@1 42.773 (41.466)\tPrec@5 61.523 (61.150)\n",
      "Epoch: [0][8420/19336]\tTime 6.117 (6.134)\tData 0.043 (0.033)\tLoss 3.2116 (3.2273)\tPrec@1 41.406 (41.468)\tPrec@5 63.281 (61.152)\n",
      "Epoch: [0][8430/19336]\tTime 6.114 (6.134)\tData 0.041 (0.033)\tLoss 3.1923 (3.2270)\tPrec@1 43.945 (41.471)\tPrec@5 61.914 (61.157)\n",
      "Epoch: [0][8440/19336]\tTime 6.118 (6.134)\tData 0.041 (0.033)\tLoss 2.9133 (3.2267)\tPrec@1 46.484 (41.476)\tPrec@5 64.648 (61.162)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0][8450/19336]\tTime 6.130 (6.134)\tData 0.040 (0.033)\tLoss 2.9521 (3.2264)\tPrec@1 45.898 (41.479)\tPrec@5 66.016 (61.165)\n",
      "Epoch: [0][8460/19336]\tTime 6.109 (6.134)\tData 0.037 (0.033)\tLoss 2.6834 (3.2262)\tPrec@1 47.266 (41.481)\tPrec@5 67.578 (61.168)\n",
      "Epoch: [0][8470/19336]\tTime 6.073 (6.134)\tData 0.042 (0.033)\tLoss 3.2190 (3.2260)\tPrec@1 43.750 (41.483)\tPrec@5 61.523 (61.171)\n",
      "Epoch: [0][8480/19336]\tTime 6.135 (6.134)\tData 0.046 (0.033)\tLoss 3.0057 (3.2257)\tPrec@1 45.312 (41.487)\tPrec@5 64.844 (61.174)\n",
      "Epoch: [0][8490/19336]\tTime 6.126 (6.134)\tData 0.050 (0.033)\tLoss 3.1520 (3.2255)\tPrec@1 41.602 (41.488)\tPrec@5 63.086 (61.175)\n",
      "Epoch: [0][8500/19336]\tTime 6.138 (6.134)\tData 0.050 (0.033)\tLoss 3.0634 (3.2255)\tPrec@1 45.703 (41.490)\tPrec@5 65.625 (61.177)\n",
      "Epoch: [0][8510/19336]\tTime 6.160 (6.134)\tData 0.039 (0.033)\tLoss 3.0261 (3.2253)\tPrec@1 45.312 (41.492)\tPrec@5 63.867 (61.179)\n",
      "Epoch: [0][8520/19336]\tTime 6.142 (6.133)\tData 0.042 (0.033)\tLoss 2.8712 (3.2249)\tPrec@1 46.875 (41.497)\tPrec@5 68.164 (61.184)\n",
      "Epoch: [0][8530/19336]\tTime 6.101 (6.133)\tData 0.040 (0.033)\tLoss 3.1534 (3.2247)\tPrec@1 41.211 (41.499)\tPrec@5 61.523 (61.187)\n",
      "Epoch: [0][8540/19336]\tTime 6.128 (6.133)\tData 0.048 (0.033)\tLoss 2.9916 (3.2244)\tPrec@1 43.555 (41.503)\tPrec@5 64.453 (61.192)\n",
      "Epoch: [0][8550/19336]\tTime 6.091 (6.133)\tData 0.038 (0.033)\tLoss 3.1356 (3.2241)\tPrec@1 41.016 (41.505)\tPrec@5 63.867 (61.195)\n",
      "Epoch: [0][8560/19336]\tTime 6.115 (6.133)\tData 0.048 (0.033)\tLoss 3.1673 (3.2238)\tPrec@1 41.992 (41.509)\tPrec@5 62.305 (61.199)\n",
      "Epoch: [0][8570/19336]\tTime 6.125 (6.133)\tData 0.038 (0.033)\tLoss 3.0167 (3.2235)\tPrec@1 42.969 (41.513)\tPrec@5 65.039 (61.204)\n",
      "Epoch: [0][8580/19336]\tTime 6.132 (6.133)\tData 0.039 (0.033)\tLoss 3.0637 (3.2233)\tPrec@1 43.164 (41.515)\tPrec@5 62.500 (61.208)\n",
      "Epoch: [0][8590/19336]\tTime 6.105 (6.133)\tData 0.041 (0.033)\tLoss 3.2779 (3.2230)\tPrec@1 40.039 (41.519)\tPrec@5 61.523 (61.212)\n",
      "Epoch: [0][8600/19336]\tTime 6.122 (6.133)\tData 0.038 (0.033)\tLoss 3.0740 (3.2228)\tPrec@1 43.750 (41.521)\tPrec@5 63.867 (61.214)\n",
      "Epoch: [0][8610/19336]\tTime 6.123 (6.133)\tData 0.039 (0.033)\tLoss 3.1056 (3.2226)\tPrec@1 42.773 (41.523)\tPrec@5 62.500 (61.217)\n",
      "Epoch: [0][8620/19336]\tTime 6.136 (6.133)\tData 0.040 (0.033)\tLoss 3.1451 (3.2222)\tPrec@1 42.383 (41.527)\tPrec@5 62.500 (61.221)\n",
      "Epoch: [0][8630/19336]\tTime 6.091 (6.133)\tData 0.039 (0.033)\tLoss 3.0749 (3.2220)\tPrec@1 43.359 (41.530)\tPrec@5 60.742 (61.224)\n",
      "Epoch: [0][8640/19336]\tTime 6.117 (6.133)\tData 0.040 (0.033)\tLoss 2.9284 (3.2216)\tPrec@1 45.898 (41.535)\tPrec@5 64.453 (61.228)\n",
      "Epoch: [0][8650/19336]\tTime 6.115 (6.133)\tData 0.043 (0.033)\tLoss 2.9445 (3.2213)\tPrec@1 46.680 (41.539)\tPrec@5 66.016 (61.232)\n",
      "Epoch: [0][8660/19336]\tTime 6.190 (6.133)\tData 0.048 (0.033)\tLoss 3.0550 (3.2210)\tPrec@1 42.578 (41.542)\tPrec@5 62.109 (61.235)\n",
      "Epoch: [0][8670/19336]\tTime 6.236 (6.133)\tData 0.038 (0.033)\tLoss 2.9292 (3.2208)\tPrec@1 43.164 (41.543)\tPrec@5 61.914 (61.238)\n",
      "Epoch: [0][8680/19336]\tTime 6.095 (6.133)\tData 0.052 (0.033)\tLoss 2.9048 (3.2206)\tPrec@1 44.141 (41.546)\tPrec@5 64.453 (61.241)\n",
      "Epoch: [0][8690/19336]\tTime 6.100 (6.133)\tData 0.042 (0.033)\tLoss 3.1422 (3.2204)\tPrec@1 43.945 (41.548)\tPrec@5 60.938 (61.243)\n",
      "Epoch: [0][8700/19336]\tTime 6.111 (6.133)\tData 0.049 (0.033)\tLoss 2.6442 (3.2201)\tPrec@1 49.023 (41.551)\tPrec@5 68.945 (61.246)\n",
      "Epoch: [0][8710/19336]\tTime 6.134 (6.133)\tData 0.042 (0.033)\tLoss 3.1646 (3.2199)\tPrec@1 39.648 (41.553)\tPrec@5 62.891 (61.248)\n",
      "Epoch: [0][8720/19336]\tTime 6.114 (6.133)\tData 0.039 (0.033)\tLoss 3.0787 (3.2196)\tPrec@1 45.703 (41.555)\tPrec@5 63.867 (61.252)\n",
      "Epoch: [0][8730/19336]\tTime 6.132 (6.133)\tData 0.039 (0.033)\tLoss 3.0423 (3.2195)\tPrec@1 44.141 (41.557)\tPrec@5 61.523 (61.253)\n",
      "Epoch: [0][8740/19336]\tTime 6.125 (6.133)\tData 0.045 (0.033)\tLoss 2.9193 (3.2192)\tPrec@1 43.945 (41.559)\tPrec@5 63.281 (61.257)\n",
      "Epoch: [0][8750/19336]\tTime 6.113 (6.133)\tData 0.040 (0.033)\tLoss 3.0156 (3.2188)\tPrec@1 43.164 (41.563)\tPrec@5 63.867 (61.260)\n",
      "Epoch: [0][8760/19336]\tTime 6.099 (6.133)\tData 0.039 (0.033)\tLoss 2.9507 (3.2186)\tPrec@1 45.508 (41.567)\tPrec@5 65.234 (61.264)\n",
      "Epoch: [0][8770/19336]\tTime 6.131 (6.133)\tData 0.039 (0.033)\tLoss 2.8880 (3.2182)\tPrec@1 45.703 (41.571)\tPrec@5 66.406 (61.267)\n",
      "Epoch: [0][8780/19336]\tTime 6.117 (6.133)\tData 0.039 (0.033)\tLoss 2.9514 (3.2180)\tPrec@1 45.508 (41.574)\tPrec@5 63.477 (61.270)\n",
      "Epoch: [0][8790/19336]\tTime 6.136 (6.133)\tData 0.039 (0.033)\tLoss 2.8880 (3.2177)\tPrec@1 43.945 (41.576)\tPrec@5 66.016 (61.274)\n",
      "Epoch: [0][8800/19336]\tTime 6.099 (6.133)\tData 0.041 (0.034)\tLoss 2.6888 (3.2175)\tPrec@1 49.219 (41.580)\tPrec@5 68.164 (61.277)\n",
      "Epoch: [0][8810/19336]\tTime 6.134 (6.133)\tData 0.038 (0.034)\tLoss 2.9476 (3.2171)\tPrec@1 45.117 (41.583)\tPrec@5 65.430 (61.280)\n",
      "Epoch: [0][8820/19336]\tTime 6.134 (6.133)\tData 0.048 (0.034)\tLoss 3.0023 (3.2168)\tPrec@1 43.750 (41.587)\tPrec@5 65.430 (61.285)\n",
      "Epoch: [0][8830/19336]\tTime 6.100 (6.133)\tData 0.041 (0.034)\tLoss 2.8574 (3.2165)\tPrec@1 42.969 (41.589)\tPrec@5 64.648 (61.287)\n",
      "Epoch: [0][8840/19336]\tTime 6.120 (6.133)\tData 0.040 (0.034)\tLoss 3.0496 (3.2163)\tPrec@1 40.625 (41.591)\tPrec@5 64.258 (61.291)\n",
      "Epoch: [0][8850/19336]\tTime 6.101 (6.133)\tData 0.050 (0.034)\tLoss 2.9052 (3.2160)\tPrec@1 43.555 (41.594)\tPrec@5 66.016 (61.295)\n",
      "Epoch: [0][8860/19336]\tTime 6.075 (6.133)\tData 0.038 (0.034)\tLoss 2.9693 (3.2158)\tPrec@1 43.750 (41.597)\tPrec@5 64.844 (61.297)\n",
      "Epoch: [0][8870/19336]\tTime 6.102 (6.133)\tData 0.039 (0.034)\tLoss 3.0233 (3.2155)\tPrec@1 42.969 (41.601)\tPrec@5 60.156 (61.300)\n",
      "Epoch: [0][8880/19336]\tTime 6.107 (6.133)\tData 0.039 (0.034)\tLoss 2.9630 (3.2153)\tPrec@1 44.336 (41.605)\tPrec@5 63.281 (61.304)\n",
      "Epoch: [0][8890/19336]\tTime 6.134 (6.133)\tData 0.039 (0.034)\tLoss 3.0030 (3.2150)\tPrec@1 41.602 (41.608)\tPrec@5 64.844 (61.307)\n",
      "Epoch: [0][8900/19336]\tTime 6.131 (6.133)\tData 0.051 (0.034)\tLoss 3.1615 (3.2147)\tPrec@1 41.016 (41.610)\tPrec@5 59.570 (61.311)\n",
      "Epoch: [0][8910/19336]\tTime 6.137 (6.133)\tData 0.040 (0.034)\tLoss 2.9135 (3.2144)\tPrec@1 43.750 (41.614)\tPrec@5 65.039 (61.315)\n",
      "Epoch: [0][8920/19336]\tTime 6.124 (6.133)\tData 0.040 (0.034)\tLoss 2.9368 (3.2142)\tPrec@1 44.141 (41.616)\tPrec@5 66.602 (61.318)\n",
      "Epoch: [0][8930/19336]\tTime 6.081 (6.133)\tData 0.040 (0.034)\tLoss 3.2558 (3.2140)\tPrec@1 39.258 (41.619)\tPrec@5 58.008 (61.320)\n",
      "Epoch: [0][8940/19336]\tTime 6.120 (6.133)\tData 0.044 (0.034)\tLoss 2.8690 (3.2138)\tPrec@1 43.164 (41.621)\tPrec@5 66.406 (61.324)\n",
      "Epoch: [0][8950/19336]\tTime 6.126 (6.133)\tData 0.045 (0.034)\tLoss 2.9829 (3.2135)\tPrec@1 46.094 (41.624)\tPrec@5 63.086 (61.326)\n",
      "Epoch: [0][8960/19336]\tTime 6.070 (6.133)\tData 0.044 (0.034)\tLoss 2.8728 (3.2133)\tPrec@1 45.312 (41.628)\tPrec@5 64.062 (61.329)\n",
      "Epoch: [0][8970/19336]\tTime 6.115 (6.133)\tData 0.053 (0.034)\tLoss 3.0019 (3.2131)\tPrec@1 45.508 (41.630)\tPrec@5 65.430 (61.332)\n",
      "Epoch: [0][8980/19336]\tTime 6.141 (6.133)\tData 0.044 (0.034)\tLoss 3.1766 (3.2129)\tPrec@1 40.820 (41.632)\tPrec@5 62.500 (61.335)\n",
      "Epoch: [0][8990/19336]\tTime 6.165 (6.133)\tData 0.046 (0.034)\tLoss 2.9035 (3.2126)\tPrec@1 44.141 (41.635)\tPrec@5 65.625 (61.339)\n",
      "Epoch: [0][9000/19336]\tTime 6.115 (6.133)\tData 0.040 (0.034)\tLoss 3.0662 (3.2124)\tPrec@1 44.531 (41.637)\tPrec@5 61.719 (61.342)\n",
      "Epoch: [0][9010/19336]\tTime 6.145 (6.133)\tData 0.040 (0.034)\tLoss 2.8954 (3.2121)\tPrec@1 46.289 (41.640)\tPrec@5 65.234 (61.346)\n",
      "Epoch: [0][9020/19336]\tTime 6.117 (6.133)\tData 0.046 (0.034)\tLoss 2.7721 (3.2118)\tPrec@1 48.438 (41.644)\tPrec@5 70.312 (61.351)\n",
      "Epoch: [0][9030/19336]\tTime 6.111 (6.133)\tData 0.041 (0.034)\tLoss 2.8061 (3.2116)\tPrec@1 45.898 (41.646)\tPrec@5 68.164 (61.354)\n",
      "Epoch: [0][9040/19336]\tTime 6.122 (6.133)\tData 0.042 (0.034)\tLoss 3.0053 (3.2113)\tPrec@1 45.117 (41.649)\tPrec@5 63.867 (61.357)\n",
      "Epoch: [0][9050/19336]\tTime 6.113 (6.133)\tData 0.051 (0.034)\tLoss 3.1528 (3.2112)\tPrec@1 46.680 (41.651)\tPrec@5 64.258 (61.359)\n",
      "Epoch: [0][9060/19336]\tTime 6.113 (6.133)\tData 0.040 (0.034)\tLoss 2.8505 (3.2109)\tPrec@1 47.266 (41.654)\tPrec@5 66.992 (61.363)\n",
      "Epoch: [0][9070/19336]\tTime 6.093 (6.133)\tData 0.042 (0.034)\tLoss 3.2454 (3.2107)\tPrec@1 41.211 (41.657)\tPrec@5 62.109 (61.366)\n",
      "Epoch: [0][9080/19336]\tTime 6.113 (6.133)\tData 0.047 (0.034)\tLoss 3.0945 (3.2104)\tPrec@1 41.211 (41.660)\tPrec@5 62.305 (61.370)\n",
      "Epoch: [0][9090/19336]\tTime 6.124 (6.133)\tData 0.051 (0.034)\tLoss 3.0505 (3.2101)\tPrec@1 43.945 (41.662)\tPrec@5 61.914 (61.373)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0][9100/19336]\tTime 6.134 (6.133)\tData 0.040 (0.034)\tLoss 3.1555 (3.2098)\tPrec@1 45.508 (41.666)\tPrec@5 61.914 (61.376)\n",
      "Epoch: [0][9110/19336]\tTime 6.098 (6.133)\tData 0.045 (0.034)\tLoss 2.9649 (3.2096)\tPrec@1 43.945 (41.670)\tPrec@5 64.453 (61.379)\n",
      "Epoch: [0][9120/19336]\tTime 6.104 (6.133)\tData 0.039 (0.034)\tLoss 3.0235 (3.2094)\tPrec@1 43.164 (41.673)\tPrec@5 63.672 (61.383)\n",
      "Epoch: [0][9130/19336]\tTime 6.127 (6.133)\tData 0.044 (0.034)\tLoss 3.1484 (3.2091)\tPrec@1 41.211 (41.676)\tPrec@5 62.305 (61.387)\n",
      "Epoch: [0][9140/19336]\tTime 6.124 (6.133)\tData 0.047 (0.034)\tLoss 3.0527 (3.2089)\tPrec@1 42.383 (41.679)\tPrec@5 61.914 (61.389)\n",
      "Epoch: [0][9150/19336]\tTime 6.130 (6.133)\tData 0.041 (0.034)\tLoss 2.9817 (3.2086)\tPrec@1 44.922 (41.682)\tPrec@5 66.016 (61.392)\n",
      "Epoch: [0][9160/19336]\tTime 6.158 (6.133)\tData 0.053 (0.034)\tLoss 3.1546 (3.2084)\tPrec@1 41.992 (41.685)\tPrec@5 62.891 (61.396)\n",
      "Epoch: [0][9170/19336]\tTime 6.110 (6.133)\tData 0.050 (0.034)\tLoss 3.0556 (3.2082)\tPrec@1 45.508 (41.688)\tPrec@5 62.500 (61.398)\n",
      "Epoch: [0][9180/19336]\tTime 6.135 (6.133)\tData 0.053 (0.034)\tLoss 2.8362 (3.2079)\tPrec@1 46.094 (41.691)\tPrec@5 66.016 (61.401)\n",
      "Epoch: [0][9190/19336]\tTime 6.141 (6.133)\tData 0.040 (0.034)\tLoss 2.9630 (3.2077)\tPrec@1 44.922 (41.694)\tPrec@5 66.406 (61.405)\n",
      "Epoch: [0][9200/19336]\tTime 6.112 (6.133)\tData 0.041 (0.034)\tLoss 2.9382 (3.2073)\tPrec@1 45.312 (41.700)\tPrec@5 64.844 (61.409)\n",
      "Epoch: [0][9210/19336]\tTime 6.113 (6.133)\tData 0.052 (0.034)\tLoss 2.9493 (3.2071)\tPrec@1 44.336 (41.703)\tPrec@5 66.016 (61.413)\n",
      "Epoch: [0][9220/19336]\tTime 6.116 (6.133)\tData 0.049 (0.034)\tLoss 3.0432 (3.2068)\tPrec@1 44.531 (41.705)\tPrec@5 64.844 (61.415)\n",
      "Epoch: [0][9230/19336]\tTime 6.084 (6.133)\tData 0.066 (0.034)\tLoss 2.8072 (3.2066)\tPrec@1 45.898 (41.707)\tPrec@5 67.383 (61.418)\n",
      "Epoch: [0][9240/19336]\tTime 6.169 (6.133)\tData 0.072 (0.034)\tLoss 3.0082 (3.2064)\tPrec@1 43.359 (41.709)\tPrec@5 63.086 (61.420)\n",
      "Epoch: [0][9250/19336]\tTime 6.106 (6.133)\tData 0.068 (0.034)\tLoss 2.9902 (3.2061)\tPrec@1 44.922 (41.713)\tPrec@5 64.844 (61.423)\n",
      "Epoch: [0][9260/19336]\tTime 6.110 (6.133)\tData 0.066 (0.034)\tLoss 3.0580 (3.2060)\tPrec@1 45.898 (41.715)\tPrec@5 61.719 (61.424)\n",
      "Epoch: [0][9270/19336]\tTime 6.139 (6.133)\tData 0.077 (0.034)\tLoss 3.0711 (3.2058)\tPrec@1 43.945 (41.718)\tPrec@5 64.453 (61.427)\n",
      "Epoch: [0][9280/19336]\tTime 6.163 (6.133)\tData 0.072 (0.034)\tLoss 3.0701 (3.2055)\tPrec@1 40.625 (41.720)\tPrec@5 62.305 (61.429)\n",
      "Epoch: [0][9290/19336]\tTime 6.141 (6.133)\tData 0.069 (0.034)\tLoss 2.7698 (3.2053)\tPrec@1 48.438 (41.721)\tPrec@5 65.430 (61.431)\n",
      "Epoch: [0][9300/19336]\tTime 6.124 (6.133)\tData 0.067 (0.034)\tLoss 2.7843 (3.2050)\tPrec@1 47.266 (41.725)\tPrec@5 67.773 (61.435)\n",
      "Epoch: [0][9310/19336]\tTime 6.125 (6.133)\tData 0.077 (0.034)\tLoss 3.0856 (3.2047)\tPrec@1 45.703 (41.729)\tPrec@5 62.695 (61.439)\n",
      "Epoch: [0][9320/19336]\tTime 6.122 (6.133)\tData 0.067 (0.034)\tLoss 2.8901 (3.2044)\tPrec@1 46.289 (41.733)\tPrec@5 65.234 (61.443)\n",
      "Epoch: [0][9330/19336]\tTime 6.112 (6.133)\tData 0.074 (0.035)\tLoss 2.9596 (3.2042)\tPrec@1 45.508 (41.735)\tPrec@5 63.477 (61.446)\n",
      "Epoch: [0][9340/19336]\tTime 6.146 (6.133)\tData 0.069 (0.035)\tLoss 2.9491 (3.2039)\tPrec@1 44.336 (41.739)\tPrec@5 65.430 (61.450)\n",
      "Epoch: [0][9350/19336]\tTime 6.082 (6.133)\tData 0.068 (0.035)\tLoss 2.8839 (3.2037)\tPrec@1 48.242 (41.742)\tPrec@5 65.820 (61.452)\n",
      "Epoch: [0][9360/19336]\tTime 6.106 (6.133)\tData 0.068 (0.035)\tLoss 2.7788 (3.2035)\tPrec@1 49.023 (41.745)\tPrec@5 67.969 (61.456)\n",
      "Epoch: [0][9370/19336]\tTime 6.128 (6.133)\tData 0.076 (0.035)\tLoss 2.9562 (3.2032)\tPrec@1 44.336 (41.748)\tPrec@5 66.016 (61.460)\n",
      "Epoch: [0][9380/19336]\tTime 6.140 (6.133)\tData 0.071 (0.035)\tLoss 2.5611 (3.2030)\tPrec@1 49.414 (41.750)\tPrec@5 70.312 (61.462)\n",
      "Epoch: [0][9390/19336]\tTime 6.117 (6.133)\tData 0.068 (0.035)\tLoss 2.9873 (3.2027)\tPrec@1 41.797 (41.753)\tPrec@5 63.477 (61.466)\n",
      "Epoch: [0][9400/19336]\tTime 6.144 (6.133)\tData 0.078 (0.035)\tLoss 2.9278 (3.2025)\tPrec@1 42.383 (41.755)\tPrec@5 65.234 (61.469)\n",
      "Epoch: [0][9410/19336]\tTime 6.141 (6.133)\tData 0.068 (0.035)\tLoss 3.0135 (3.2024)\tPrec@1 43.359 (41.757)\tPrec@5 64.453 (61.470)\n",
      "Epoch: [0][9420/19336]\tTime 6.149 (6.133)\tData 0.073 (0.035)\tLoss 2.9881 (3.2021)\tPrec@1 43.359 (41.760)\tPrec@5 63.281 (61.474)\n",
      "Epoch: [0][9430/19336]\tTime 6.153 (6.133)\tData 0.080 (0.035)\tLoss 3.0582 (3.2019)\tPrec@1 46.680 (41.764)\tPrec@5 65.234 (61.477)\n",
      "Epoch: [0][9440/19336]\tTime 6.136 (6.133)\tData 0.069 (0.035)\tLoss 2.8697 (3.2016)\tPrec@1 45.898 (41.767)\tPrec@5 66.797 (61.481)\n",
      "Epoch: [0][9450/19336]\tTime 6.167 (6.133)\tData 0.084 (0.035)\tLoss 3.0406 (3.2014)\tPrec@1 44.141 (41.770)\tPrec@5 64.844 (61.484)\n",
      "Epoch: [0][9460/19336]\tTime 6.146 (6.133)\tData 0.068 (0.035)\tLoss 3.0974 (3.2011)\tPrec@1 45.117 (41.772)\tPrec@5 62.891 (61.487)\n",
      "Epoch: [0][9470/19336]\tTime 6.178 (6.133)\tData 0.078 (0.035)\tLoss 2.8235 (3.2009)\tPrec@1 47.070 (41.775)\tPrec@5 68.164 (61.491)\n",
      "Epoch: [0][9480/19336]\tTime 6.122 (6.133)\tData 0.067 (0.035)\tLoss 2.9374 (3.2006)\tPrec@1 43.750 (41.778)\tPrec@5 64.258 (61.494)\n",
      "Epoch: [0][9490/19336]\tTime 6.177 (6.133)\tData 0.085 (0.035)\tLoss 3.1343 (3.2004)\tPrec@1 41.797 (41.781)\tPrec@5 61.523 (61.497)\n",
      "Epoch: [0][9500/19336]\tTime 6.101 (6.133)\tData 0.051 (0.035)\tLoss 2.7177 (3.2002)\tPrec@1 47.656 (41.783)\tPrec@5 66.406 (61.499)\n",
      "Epoch: [0][9510/19336]\tTime 6.116 (6.133)\tData 0.045 (0.035)\tLoss 2.9260 (3.2001)\tPrec@1 43.359 (41.785)\tPrec@5 66.797 (61.502)\n",
      "Epoch: [0][9520/19336]\tTime 6.124 (6.133)\tData 0.051 (0.035)\tLoss 3.0305 (3.1999)\tPrec@1 43.750 (41.787)\tPrec@5 64.258 (61.504)\n",
      "Epoch: [0][9530/19336]\tTime 6.075 (6.133)\tData 0.042 (0.035)\tLoss 2.9176 (3.1996)\tPrec@1 45.312 (41.791)\tPrec@5 66.211 (61.507)\n",
      "Epoch: [0][9540/19336]\tTime 6.083 (6.133)\tData 0.042 (0.035)\tLoss 3.0141 (3.1994)\tPrec@1 41.211 (41.793)\tPrec@5 65.234 (61.510)\n",
      "Epoch: [0][9550/19336]\tTime 6.112 (6.133)\tData 0.054 (0.035)\tLoss 2.7358 (3.1992)\tPrec@1 48.242 (41.797)\tPrec@5 67.578 (61.513)\n",
      "Epoch: [0][9560/19336]\tTime 6.130 (6.133)\tData 0.044 (0.035)\tLoss 3.0435 (3.1989)\tPrec@1 42.969 (41.800)\tPrec@5 63.281 (61.516)\n",
      "Epoch: [0][9570/19336]\tTime 6.121 (6.133)\tData 0.051 (0.035)\tLoss 2.8574 (3.1987)\tPrec@1 46.680 (41.803)\tPrec@5 66.016 (61.520)\n",
      "Epoch: [0][9580/19336]\tTime 6.113 (6.133)\tData 0.045 (0.035)\tLoss 3.1635 (3.1985)\tPrec@1 44.922 (41.806)\tPrec@5 63.867 (61.522)\n",
      "Epoch: [0][9590/19336]\tTime 6.107 (6.133)\tData 0.054 (0.035)\tLoss 2.8062 (3.1983)\tPrec@1 46.289 (41.809)\tPrec@5 66.602 (61.525)\n",
      "Epoch: [0][9600/19336]\tTime 6.067 (6.133)\tData 0.044 (0.035)\tLoss 3.0628 (3.1981)\tPrec@1 44.141 (41.812)\tPrec@5 62.305 (61.528)\n",
      "Epoch: [0][9610/19336]\tTime 6.147 (6.133)\tData 0.041 (0.035)\tLoss 2.8656 (3.1978)\tPrec@1 44.141 (41.815)\tPrec@5 66.797 (61.531)\n",
      "Epoch: [0][9620/19336]\tTime 6.115 (6.133)\tData 0.044 (0.035)\tLoss 3.2893 (3.1975)\tPrec@1 41.797 (41.819)\tPrec@5 58.984 (61.535)\n",
      "Epoch: [0][9630/19336]\tTime 6.080 (6.133)\tData 0.041 (0.035)\tLoss 3.1965 (3.1973)\tPrec@1 41.992 (41.822)\tPrec@5 61.133 (61.538)\n",
      "Epoch: [0][9640/19336]\tTime 6.112 (6.133)\tData 0.050 (0.035)\tLoss 2.9146 (3.1970)\tPrec@1 45.898 (41.825)\tPrec@5 63.281 (61.542)\n",
      "Epoch: [0][9650/19336]\tTime 6.126 (6.133)\tData 0.042 (0.035)\tLoss 3.3570 (3.1968)\tPrec@1 38.867 (41.827)\tPrec@5 60.156 (61.546)\n",
      "Epoch: [0][9660/19336]\tTime 6.122 (6.133)\tData 0.050 (0.035)\tLoss 3.0503 (3.1965)\tPrec@1 42.188 (41.830)\tPrec@5 62.500 (61.549)\n",
      "Epoch: [0][9670/19336]\tTime 6.108 (6.133)\tData 0.041 (0.035)\tLoss 3.2148 (3.1963)\tPrec@1 41.992 (41.833)\tPrec@5 60.938 (61.551)\n",
      "Epoch: [0][9680/19336]\tTime 6.165 (6.133)\tData 0.045 (0.035)\tLoss 2.7445 (3.1961)\tPrec@1 47.852 (41.835)\tPrec@5 65.234 (61.554)\n",
      "Epoch: [0][9690/19336]\tTime 6.110 (6.133)\tData 0.045 (0.035)\tLoss 3.1234 (3.1958)\tPrec@1 42.969 (41.839)\tPrec@5 62.891 (61.558)\n",
      "Epoch: [0][9700/19336]\tTime 6.124 (6.133)\tData 0.042 (0.035)\tLoss 2.9268 (3.1956)\tPrec@1 45.312 (41.843)\tPrec@5 65.625 (61.562)\n",
      "Epoch: [0][9710/19336]\tTime 6.110 (6.133)\tData 0.041 (0.035)\tLoss 2.8694 (3.1953)\tPrec@1 45.703 (41.847)\tPrec@5 64.258 (61.566)\n",
      "Epoch: [0][9720/19336]\tTime 6.090 (6.133)\tData 0.051 (0.035)\tLoss 2.7853 (3.1950)\tPrec@1 48.047 (41.851)\tPrec@5 66.992 (61.569)\n",
      "Epoch: [0][9730/19336]\tTime 6.136 (6.133)\tData 0.052 (0.036)\tLoss 2.9283 (3.1948)\tPrec@1 44.727 (41.854)\tPrec@5 64.062 (61.572)\n",
      "Epoch: [0][9740/19336]\tTime 6.143 (6.133)\tData 0.049 (0.036)\tLoss 2.9777 (3.1947)\tPrec@1 42.578 (41.857)\tPrec@5 63.281 (61.574)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [0][9750/19336]\tTime 6.152 (6.133)\tData 0.054 (0.036)\tLoss 2.7047 (3.1944)\tPrec@1 50.195 (41.860)\tPrec@5 68.945 (61.578)\n",
      "Epoch: [0][9760/19336]\tTime 6.132 (6.133)\tData 0.041 (0.036)\tLoss 2.8782 (3.1941)\tPrec@1 44.336 (41.863)\tPrec@5 65.039 (61.581)\n",
      "Epoch: [0][9770/19336]\tTime 6.093 (6.133)\tData 0.047 (0.036)\tLoss 3.3107 (3.1939)\tPrec@1 40.234 (41.866)\tPrec@5 58.594 (61.583)\n",
      "Epoch: [0][9780/19336]\tTime 6.107 (6.133)\tData 0.041 (0.036)\tLoss 2.9199 (3.1937)\tPrec@1 44.727 (41.869)\tPrec@5 65.234 (61.586)\n",
      "Epoch: [0][9790/19336]\tTime 6.120 (6.133)\tData 0.046 (0.036)\tLoss 3.1759 (3.1934)\tPrec@1 42.969 (41.872)\tPrec@5 61.719 (61.589)\n",
      "Epoch: [0][9800/19336]\tTime 6.101 (6.132)\tData 0.048 (0.036)\tLoss 2.9525 (3.1932)\tPrec@1 43.164 (41.875)\tPrec@5 63.867 (61.591)\n",
      "Epoch: [0][9810/19336]\tTime 6.079 (6.132)\tData 0.052 (0.036)\tLoss 3.1312 (3.1930)\tPrec@1 42.188 (41.876)\tPrec@5 63.867 (61.594)\n",
      "Epoch: [0][9820/19336]\tTime 6.101 (6.132)\tData 0.041 (0.036)\tLoss 3.0398 (3.1928)\tPrec@1 41.992 (41.878)\tPrec@5 65.039 (61.597)\n",
      "Epoch: [0][9830/19336]\tTime 6.078 (6.132)\tData 0.051 (0.036)\tLoss 2.7868 (3.1925)\tPrec@1 46.875 (41.881)\tPrec@5 66.211 (61.600)\n",
      "Epoch: [0][9840/19336]\tTime 6.104 (6.132)\tData 0.046 (0.036)\tLoss 3.0306 (3.1923)\tPrec@1 40.625 (41.883)\tPrec@5 61.719 (61.602)\n",
      "Epoch: [0][9850/19336]\tTime 6.108 (6.132)\tData 0.041 (0.036)\tLoss 2.7733 (3.1921)\tPrec@1 47.656 (41.885)\tPrec@5 68.555 (61.605)\n",
      "Epoch: [0][9860/19336]\tTime 6.120 (6.132)\tData 0.047 (0.036)\tLoss 2.9745 (3.1919)\tPrec@1 42.188 (41.887)\tPrec@5 62.305 (61.607)\n",
      "Epoch: [0][9870/19336]\tTime 6.133 (6.132)\tData 0.054 (0.036)\tLoss 2.8107 (3.1917)\tPrec@1 46.875 (41.890)\tPrec@5 69.336 (61.611)\n",
      "Epoch: [0][9880/19336]\tTime 6.119 (6.132)\tData 0.058 (0.036)\tLoss 2.9942 (3.1915)\tPrec@1 44.531 (41.892)\tPrec@5 63.086 (61.614)\n",
      "Epoch: [0][9890/19336]\tTime 6.140 (6.132)\tData 0.042 (0.036)\tLoss 3.0845 (3.1912)\tPrec@1 44.336 (41.895)\tPrec@5 63.281 (61.617)\n",
      "Epoch: [0][9900/19336]\tTime 6.118 (6.132)\tData 0.054 (0.036)\tLoss 3.1496 (3.1910)\tPrec@1 41.211 (41.899)\tPrec@5 59.766 (61.620)\n",
      "Epoch: [0][9910/19336]\tTime 6.101 (6.132)\tData 0.042 (0.036)\tLoss 3.0429 (3.1908)\tPrec@1 43.555 (41.901)\tPrec@5 63.867 (61.623)\n",
      "Epoch: [0][9920/19336]\tTime 6.113 (6.132)\tData 0.042 (0.036)\tLoss 2.7637 (3.1905)\tPrec@1 46.875 (41.903)\tPrec@5 66.797 (61.625)\n"
     ]
    }
   ],
   "source": [
    "# define loss function (criterion) and optimizer\n",
    "criterion = torch.nn.CrossEntropyLoss().cuda()\n",
    "\n",
    "# get trainer\n",
    "Trainer = get_trainer(train_dataloader, val_dataloader, model, criterion, config)\n",
    "\n",
    "# Run!\n",
    "Trainer.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
