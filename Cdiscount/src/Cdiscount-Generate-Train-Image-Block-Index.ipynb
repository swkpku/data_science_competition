{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "category_names.csv\n",
      "sample_submission.csv\n",
      "test.bson\n",
      "train_block_0.bson\n",
      "train_block_10.bson\n",
      "train_block_11.bson\n",
      "train_block_12.bson\n",
      "train_block_13.bson\n",
      "train_block_14.bson\n",
      "train_block_15.bson\n",
      "train_block_16.bson\n",
      "train_block_17.bson\n",
      "train_block_18.bson\n",
      "train_block_19.bson\n",
      "train_block_1.bson\n",
      "train_block_20.bson\n",
      "train_block_21.bson\n",
      "train_block_22.bson\n",
      "train_block_23.bson\n",
      "train_block_24.bson\n",
      "train_block_2.bson\n",
      "train_block_3.bson\n",
      "train_block_4.bson\n",
      "train_block_5.bson\n",
      "train_block_6.bson\n",
      "train_block_7.bson\n",
      "train_block_8.bson\n",
      "train_block_9.bson\n",
      "train.bson\n",
      "train_example.bson\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os, sys, math, io\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import multiprocessing as mp\n",
    "import bson\n",
    "import struct\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from collections import defaultdict\n",
    "from tqdm import *\n",
    "\n",
    "data_dir = '/mnt/data/cdiscount/'\n",
    "\n",
    "from subprocess import check_output\n",
    "print(check_output([\"ls\", data_dir]).decode(\"utf8\"))\n",
    "\n",
    "num_blocks = 25\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_block_bson_path_list = []\n",
    "for i in range(num_blocks):\n",
    "    train_block_bson_filename = 'train_block_' + str(i) + '.bson'\n",
    "    train_block_bson_path_list.append(os.path.join(data_dir, train_block_bson_filename))\n",
    "\n",
    "num_train_block_products = 1413980"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 1000012755)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categories_df = pd.read_csv(\"categories.csv\", index_col=0)\n",
    "def make_category_tables():\n",
    "    cat2idx = {}\n",
    "    idx2cat = {}\n",
    "    for ir in categories_df.itertuples():\n",
    "        category_id = ir[0]\n",
    "        category_idx = ir[4]\n",
    "        cat2idx[category_id] = category_idx\n",
    "        idx2cat[category_idx] = category_id\n",
    "    return cat2idx, idx2cat\n",
    "\n",
    "cat2idx, idx2cat = make_category_tables()\n",
    "cat2idx[1000012755], idx2cat[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 282796/1413980 [00:03<00:12, 88928.70it/s]\n",
      " 20%|██        | 282796/1413980 [00:03<00:12, 88294.08it/s]\n",
      " 20%|██        | 282796/1413980 [00:03<00:12, 87710.66it/s]\n",
      " 20%|██        | 282796/1413980 [00:03<00:12, 88971.44it/s]\n",
      " 20%|██        | 282796/1413980 [00:05<00:23, 47458.41it/s]\n",
      " 20%|██        | 282796/1413980 [00:07<00:31, 36313.07it/s]\n",
      " 20%|██        | 282796/1413980 [00:08<00:33, 33969.63it/s]\n",
      " 20%|██        | 282796/1413980 [00:07<00:31, 35577.18it/s]\n",
      " 20%|██        | 282796/1413980 [00:08<00:32, 35156.46it/s]\n",
      " 20%|██        | 282796/1413980 [00:08<00:32, 34362.44it/s]\n",
      " 20%|██        | 282796/1413980 [00:08<00:33, 33480.92it/s]\n",
      " 20%|██        | 282796/1413980 [00:08<00:32, 35171.43it/s]\n",
      " 20%|██        | 282796/1413980 [00:08<00:32, 34990.88it/s]\n",
      " 20%|██        | 282796/1413980 [00:08<00:32, 35101.36it/s]\n",
      " 20%|██        | 282796/1413980 [00:08<00:32, 34601.54it/s]\n",
      " 20%|█▉        | 282795/1413980 [00:08<00:32, 34679.51it/s]\n",
      " 20%|██        | 282796/1413980 [00:08<00:32, 34309.71it/s]\n",
      " 20%|██        | 282796/1413980 [00:08<00:32, 34343.79it/s]\n",
      " 20%|██        | 282796/1413980 [00:08<00:34, 33020.42it/s]\n",
      " 20%|██        | 282796/1413980 [00:08<00:33, 33320.46it/s]\n",
      " 20%|██        | 282796/1413980 [00:08<00:34, 32319.94it/s]\n",
      " 20%|█▉        | 282795/1413980 [00:08<00:34, 32546.08it/s]\n",
      " 20%|█▉        | 282795/1413980 [00:08<00:34, 32736.82it/s]\n",
      " 20%|█▉        | 282795/1413980 [00:08<00:33, 33459.08it/s]\n",
      " 20%|█▉        | 282794/1413980 [00:08<00:32, 34279.57it/s]\n"
     ]
    }
   ],
   "source": [
    "def read_bson(bson_path, num_records, with_categories):\n",
    "    rows = {}\n",
    "    with open(bson_path, \"rb\") as f, tqdm(total=num_records) as pbar:\n",
    "        offset = 0\n",
    "        while True:\n",
    "            item_length_bytes = f.read(4)\n",
    "            if len(item_length_bytes) == 0:\n",
    "                break\n",
    "\n",
    "            length = struct.unpack(\"<i\", item_length_bytes)[0]\n",
    "\n",
    "            f.seek(offset)\n",
    "            item_data = f.read(length)\n",
    "            assert len(item_data) == length\n",
    "\n",
    "            item = bson.BSON.decode(item_data)\n",
    "            product_id = item[\"_id\"]\n",
    "            num_imgs = len(item[\"imgs\"])\n",
    "\n",
    "            row = [num_imgs, offset, length]\n",
    "            if with_categories:\n",
    "                row += [item[\"category_id\"]]\n",
    "            rows[product_id] = row\n",
    "\n",
    "            offset += length\n",
    "            f.seek(offset)\n",
    "            pbar.update()\n",
    "\n",
    "    columns = [\"num_imgs\", \"offset\", \"length\"]\n",
    "    if with_categories:\n",
    "        columns += [\"category_id\"]\n",
    "\n",
    "    df = pd.DataFrame.from_dict(rows, orient=\"index\")\n",
    "    df.index.name = \"product_id\"\n",
    "    df.columns = columns\n",
    "    df.sort_index(inplace=True)\n",
    "    return df\n",
    "\n",
    "train_block_offsets_df_list = []\n",
    "for i in range(num_blocks):\n",
    "    train_block_bson_filename = 'train_block_' + str(i) + '.bson'\n",
    "    train_block_bson_path = os.path.join(data_dir, train_block_bson_filename)\n",
    "    train_block_offsets_df_list.append(read_bson(train_block_bson_path, num_records=num_train_block_products, with_categories=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i, df in enumerate(train_block_offsets_df_list):\n",
    "    offset_csv_filename = 'train_block_' + str(i) + '_offsets.csv'\n",
    "    df.to_csv(offset_csv_filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "block 0\n",
      "number of products = 282796\n",
      "number of catagories = 4882\n",
      "number of images = 495389\n",
      "block 1\n",
      "number of products = 282796\n",
      "number of catagories = 4901\n",
      "number of images = 494121\n",
      "block 2\n",
      "number of products = 282796\n",
      "number of catagories = 4880\n",
      "number of images = 494809\n",
      "block 3\n",
      "number of products = 282796\n",
      "number of catagories = 4872\n",
      "number of images = 494692\n",
      "block 4\n",
      "number of products = 282796\n",
      "number of catagories = 4915\n",
      "number of images = 494378\n",
      "block 5\n",
      "number of products = 282796\n",
      "number of catagories = 4877\n",
      "number of images = 495252\n",
      "block 6\n",
      "number of products = 282796\n",
      "number of catagories = 4900\n",
      "number of images = 494569\n",
      "block 7\n",
      "number of products = 282796\n",
      "number of catagories = 4879\n",
      "number of images = 494409\n",
      "block 8\n",
      "number of products = 282796\n",
      "number of catagories = 4873\n",
      "number of images = 494617\n",
      "block 9\n",
      "number of products = 282796\n",
      "number of catagories = 4907\n",
      "number of images = 495297\n",
      "block 10\n",
      "number of products = 282796\n",
      "number of catagories = 4886\n",
      "number of images = 495392\n",
      "block 11\n",
      "number of products = 282796\n",
      "number of catagories = 4887\n",
      "number of images = 494978\n",
      "block 12\n",
      "number of products = 282796\n",
      "number of catagories = 4879\n",
      "number of images = 495346\n",
      "block 13\n",
      "number of products = 282796\n",
      "number of catagories = 4902\n",
      "number of images = 495471\n",
      "block 14\n",
      "number of products = 282796\n",
      "number of catagories = 4906\n",
      "number of images = 494425\n",
      "block 15\n",
      "number of products = 282795\n",
      "number of catagories = 4892\n",
      "number of images = 494911\n",
      "block 16\n",
      "number of products = 282796\n",
      "number of catagories = 4897\n",
      "number of images = 494872\n",
      "block 17\n",
      "number of products = 282796\n",
      "number of catagories = 4885\n",
      "number of images = 495398\n",
      "block 18\n",
      "number of products = 282796\n",
      "number of catagories = 4876\n",
      "number of images = 495082\n",
      "block 19\n",
      "number of products = 282796\n",
      "number of catagories = 4868\n",
      "number of images = 495701\n",
      "block 20\n",
      "number of products = 282796\n",
      "number of catagories = 4877\n",
      "number of images = 493180\n",
      "block 21\n",
      "number of products = 282795\n",
      "number of catagories = 4866\n",
      "number of images = 494425\n",
      "block 22\n",
      "number of products = 282795\n",
      "number of catagories = 4890\n",
      "number of images = 494748\n",
      "block 23\n",
      "number of products = 282795\n",
      "number of catagories = 4874\n",
      "number of images = 495105\n",
      "block 24\n",
      "number of products = 282794\n",
      "number of catagories = 4871\n",
      "number of images = 494724\n"
     ]
    }
   ],
   "source": [
    "for i, df in enumerate(train_block_offsets_df_list):\n",
    "    print('block %d' % i)\n",
    "    print('number of products = %d' % len(df))\n",
    "    print('number of catagories = %d' % len(df[\"category_id\"].unique()))\n",
    "    print('number of images = %d' % df[\"num_imgs\"].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "282796it [00:00, 667440.79it/s]\n",
      "100%|██████████| 282796/282796 [00:05<00:00, 55831.64it/s]\n",
      "282796it [00:00, 776933.29it/s]\n",
      "100%|██████████| 282796/282796 [00:05<00:00, 55463.11it/s]\n",
      "282796it [00:00, 773074.21it/s]\n",
      "100%|██████████| 282796/282796 [00:05<00:00, 55144.97it/s]\n",
      "282796it [00:00, 768736.27it/s]\n",
      "100%|██████████| 282796/282796 [00:05<00:00, 55267.37it/s]\n",
      "282796it [00:00, 777024.39it/s]\n",
      "100%|██████████| 282796/282796 [00:05<00:00, 55815.43it/s]\n",
      "282796it [00:00, 770582.62it/s]\n",
      "100%|██████████| 282796/282796 [00:05<00:00, 54878.67it/s]\n",
      "282796it [00:00, 774627.70it/s]\n",
      "100%|██████████| 282796/282796 [00:05<00:00, 55975.11it/s]\n",
      "282796it [00:00, 774993.12it/s]\n",
      "100%|██████████| 282796/282796 [00:05<00:00, 56435.18it/s]\n",
      "282796it [00:00, 774627.19it/s]\n",
      "100%|██████████| 282796/282796 [00:04<00:00, 56842.06it/s]\n",
      "282796it [00:00, 773907.48it/s]\n",
      "100%|██████████| 282796/282796 [00:05<00:00, 56022.03it/s]\n",
      "282796it [00:00, 769783.96it/s]\n",
      "100%|██████████| 282796/282796 [00:05<00:00, 55471.96it/s]\n",
      "282796it [00:00, 768407.59it/s]\n",
      "100%|██████████| 282796/282796 [00:05<00:00, 55733.58it/s]\n",
      "282796it [00:00, 769289.20it/s]\n",
      "100%|██████████| 282796/282796 [00:04<00:00, 56715.99it/s]\n",
      "282796it [00:00, 768829.95it/s]\n",
      "100%|██████████| 282796/282796 [00:05<00:00, 55397.72it/s]\n",
      "282796it [00:00, 774424.89it/s]\n",
      "100%|██████████| 282796/282796 [00:05<00:00, 55749.48it/s]\n",
      "282795it [00:00, 772904.73it/s]\n",
      "100%|██████████| 282795/282795 [00:05<00:00, 56221.46it/s]\n",
      "282796it [00:00, 744186.24it/s]\n",
      "100%|██████████| 282796/282796 [00:05<00:00, 55939.61it/s]\n",
      "282796it [00:00, 763988.34it/s]\n",
      "100%|██████████| 282796/282796 [00:04<00:00, 56953.58it/s]\n",
      "282796it [00:00, 776154.94it/s]\n",
      "100%|██████████| 282796/282796 [00:05<00:00, 56147.53it/s]\n",
      "282796it [00:00, 779480.52it/s]\n",
      "100%|██████████| 282796/282796 [00:05<00:00, 56139.33it/s]\n",
      "282796it [00:00, 774941.47it/s]\n",
      "100%|██████████| 282796/282796 [00:04<00:00, 57011.60it/s]\n",
      "282795it [00:00, 776784.51it/s]\n",
      "100%|██████████| 282795/282795 [00:05<00:00, 55877.51it/s]\n",
      "282795it [00:00, 779674.52it/s]\n",
      "100%|██████████| 282795/282795 [00:05<00:00, 55630.57it/s]\n",
      "282795it [00:00, 731924.89it/s]\n",
      "100%|██████████| 282795/282795 [00:05<00:00, 55597.07it/s]\n",
      "282794it [00:00, 773820.72it/s]\n",
      "100%|██████████| 282794/282794 [00:05<00:00, 56050.98it/s]\n"
     ]
    }
   ],
   "source": [
    "def make_val_set(df, split_percentage=0.2, drop_percentage=0.):\n",
    "    # Find the product_ids for each category.\n",
    "    category_dict = defaultdict(list)\n",
    "    for ir in tqdm(df.itertuples()):\n",
    "        category_dict[ir[4]].append(ir[0])\n",
    "\n",
    "    train_list = []\n",
    "    val_list = []\n",
    "    with tqdm(total=len(df)) as pbar:\n",
    "        for category_id, product_ids in category_dict.items():\n",
    "            category_idx = cat2idx[category_id]\n",
    "\n",
    "            # Randomly remove products to make the dataset smaller.\n",
    "            keep_size = int(len(product_ids) * (1. - drop_percentage))\n",
    "            if keep_size < len(product_ids):\n",
    "                product_ids = np.random.choice(product_ids, keep_size, replace=False)\n",
    "\n",
    "            # Randomly choose the products that become part of the validation set.\n",
    "            val_size = int(len(product_ids) * split_percentage)\n",
    "            if val_size > 0:\n",
    "                val_ids = np.random.choice(product_ids, val_size, replace=False)\n",
    "            else:\n",
    "                val_ids = []\n",
    "\n",
    "            # Create a new row for each image.\n",
    "            for product_id in product_ids:\n",
    "                row = [product_id, category_idx]\n",
    "                for img_idx in range(df.loc[product_id, \"num_imgs\"]):\n",
    "                    if product_id in val_ids:\n",
    "                        val_list.append(row + [img_idx])\n",
    "                    else:\n",
    "                        train_list.append(row + [img_idx])\n",
    "                pbar.update()\n",
    "                \n",
    "    columns = [\"product_id\", \"category_idx\", \"img_idx\"]\n",
    "    train_df = pd.DataFrame(train_list, columns=columns)\n",
    "    val_df = pd.DataFrame(val_list, columns=columns)   \n",
    "    return train_df, val_df\n",
    "\n",
    "train_block_images_df_list = []\n",
    "val_block_images_df_list = []\n",
    "\n",
    "for offsets_df in train_block_offsets_df_list:\n",
    "    train_block_images_df, val_block_images_df = make_val_set(offsets_df, split_percentage=0.2, drop_percentage=0.)\n",
    "    train_block_images_df_list.append(train_block_images_df)\n",
    "    val_block_images_df_list.append(val_block_images_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "for train_block_images_df, val_block_images_df in zip(train_block_images_df_list, val_block_images_df_list):\n",
    "    train_block_image_csv_filename = 'train_block_' + str(i) + '_images.csv'\n",
    "    val_block_image_csv_filename = 'val_block_' + str(i) + '_images.csv'\n",
    "    train_block_images_df.to_csv(train_block_image_csv_filename)\n",
    "    val_block_images_df.to_csv(val_block_image_csv_filename)\n",
    "    i = i + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
