{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/weiso/tensorflow/lib/python3.5/site-packages/numpy/lib/arraysetops.py:463: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n",
      "7069896it [00:09, 708798.63it/s]\n",
      "  0%|          | 0/7069896 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "1000005633",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-060ef9415e13>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0mtrain_offsets_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"train_offsets.csv\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_col\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdev_images_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_val_set\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_offsets_df\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplit_percentage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.05\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdrop_percentage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Number of dev images:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdev_images_df\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0mdev_images_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"dev_images.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-060ef9415e13>\u001b[0m in \u001b[0;36mmake_val_set\u001b[0;34m(df, split_percentage, drop_percentage)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpbar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcategory_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mproduct_ids\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcategory_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m             \u001b[0mcategory_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcat2idx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcategory_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;31m# Randomly remove products to make the dataset smaller.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 1000005633"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from tqdm import *\n",
    "\n",
    "def make_category_tables():\n",
    "    cat2idx = {}\n",
    "    idx2cat = {}\n",
    "    for ir in categories_df.itertuples():\n",
    "        category_id = ir[0]\n",
    "        category_idx = ir[4]\n",
    "        cat2idx[category_id] = category_idx\n",
    "        idx2cat[category_idx] = category_id\n",
    "    return cat2idx, idx2cat\n",
    "\n",
    "def make_val_set(df, split_percentage=0.2, drop_percentage=0.):\n",
    "    # Find the product_ids for each category.\n",
    "    category_dict = defaultdict(list)\n",
    "    for ir in tqdm(df.itertuples()):\n",
    "        category_dict[ir[4]].append(ir[0])\n",
    "\n",
    "    train_list = []\n",
    "    val_list = []\n",
    "    with tqdm(total=len(df)) as pbar:\n",
    "        for category_id, product_ids in category_dict.items():\n",
    "            category_idx = cat2idx[category_id]\n",
    "\n",
    "            # Randomly remove products to make the dataset smaller.\n",
    "            keep_size = int(len(product_ids) * (1. - drop_percentage))\n",
    "            if keep_size < len(product_ids):\n",
    "                product_ids = np.random.choice(product_ids, keep_size, replace=False)\n",
    "\n",
    "            # Randomly choose the products that become part of the validation set.\n",
    "            val_size = int(len(product_ids) * split_percentage)\n",
    "            if val_size > 0:\n",
    "                val_ids = np.random.choice(product_ids, val_size, replace=False)\n",
    "            else:\n",
    "                val_ids = []\n",
    "\n",
    "            # Create a new row for each image.\n",
    "            for product_id in product_ids:\n",
    "                row = [product_id, category_idx]\n",
    "                for img_idx in range(df.loc[product_id, \"num_imgs\"]):\n",
    "                    if product_id in val_ids:\n",
    "                        val_list.append(row + [img_idx])\n",
    "                    #else:\n",
    "                        #train_list.append(row + [img_idx])\n",
    "                pbar.update()\n",
    "                \n",
    "    columns = [\"product_id\", \"category_idx\", \"img_idx\"]\n",
    "    train_df = pd.DataFrame(train_list, columns=columns)\n",
    "    val_df = pd.DataFrame(val_list, columns=columns)   \n",
    "    return train_df, val_df\n",
    "\n",
    "categories_df = pd.read_csv(\"categories.csv\")\n",
    "cat2idx, idx2cat = make_category_tables()\n",
    "\n",
    "train_offsets_df = pd.read_csv(\"train_offsets.csv\", index_col=0)\n",
    "_, dev_images_df = make_val_set(train_offsets_df, split_percentage=0.05, drop_percentage=0.)\n",
    "print(\"Number of dev images:\", len(dev_images_df))\n",
    "dev_images_df.to_csv(\"dev_images.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
