{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image shape:  (42000, 784)\n",
      "label shape:  (42000,)\n",
      "test image shape:  (28000, 784)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import csv\n",
    "\n",
    "data_path = '../data/'\n",
    "\n",
    "filename = data_path + 'train.csv'\n",
    "#data, label = read_data(filename)\n",
    "data = pd.read_csv(filename)\n",
    "\n",
    "images = data.as_matrix(columns=[data.columns[1:]])\n",
    "print('image shape: ', images.shape)\n",
    "\n",
    "labels = data.as_matrix(columns=[data.columns[0]])\n",
    "labels = labels.reshape(labels.shape[0])\n",
    "print('label shape: ', labels.shape)\n",
    "\n",
    "test_filename = data_path + 'test.csv'\n",
    "test_data = pd.read_csv(test_filename)\n",
    "test_images = test_data.as_matrix(columns=[test_data.columns[:]])\n",
    "print('test image shape: ', test_images.shape)\n",
    "\n",
    "num_data = images.shape[0]\n",
    "data_dim = images.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f84c52de208>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "\n",
    "def display_img(dataset, idx):\n",
    "    image = dataset[idx]\n",
    "    image_width = image_height = np.ceil(np.sqrt(data_dim)).astype(np.uint8)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(image.reshape(image_width, image_height), cmap=cm.binary)\n",
    "    plt.show()\n",
    "    \n",
    "display_img(images, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train image shape:  (41000, 784)\n",
      "train label shape:  (41000,)\n",
      "val image shape:  (1000, 784)\n",
      "val label shape:  (1000,)\n"
     ]
    }
   ],
   "source": [
    "num_train = 41000\n",
    "num_val   = 1000\n",
    "\n",
    "indicies = np.arange(num_data)\n",
    "np.random.shuffle(indicies)\n",
    "train_mask = indicies[range(num_train)]\n",
    "val_mask = indicies[range(num_train, num_train + num_val)]\n",
    "\n",
    "train_image = images[train_mask]\n",
    "train_label = labels[train_mask]\n",
    "val_image = images[val_mask]\n",
    "val_label = labels[val_mask]\n",
    "\n",
    "print('train image shape: ', train_image.shape)\n",
    "print('train label shape: ', train_label.shape)\n",
    "print('val image shape: ', val_image.shape)\n",
    "print('val label shape: ', val_label.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training with batch_size: 16, learning_rate: 0.001000\n",
      "best_val_acc: 0.994000\n",
      "training with batch_size: 16, learning_rate: 0.005000\n",
      "best_val_acc: 0.991000\n",
      "training with batch_size: 16, learning_rate: 0.008000\n",
      "best_val_acc: 0.988000\n",
      "training with batch_size: 16, learning_rate: 0.000100\n",
      "best_val_acc: 0.995000\n",
      "training with batch_size: 16, learning_rate: 0.000500\n",
      "best_val_acc: 0.991000\n",
      "training with batch_size: 16, learning_rate: 0.000080\n",
      "best_val_acc: 0.997000\n",
      "training with batch_size: 32, learning_rate: 0.001000\n",
      "best_val_acc: 0.992000\n",
      "training with batch_size: 32, learning_rate: 0.005000\n",
      "best_val_acc: 0.992000\n",
      "training with batch_size: 32, learning_rate: 0.008000\n",
      "best_val_acc: 0.990000\n",
      "training with batch_size: 32, learning_rate: 0.000100\n",
      "best_val_acc: 0.992000\n",
      "training with batch_size: 32, learning_rate: 0.000500\n",
      "best_val_acc: 0.995000\n",
      "training with batch_size: 32, learning_rate: 0.000800\n",
      "best_val_acc: 0.995000\n",
      "training with batch_size: 32, learning_rate: 0.000010\n",
      "best_val_acc: 0.880000\n",
      "training with batch_size: 32, learning_rate: 0.000050\n",
      "best_val_acc: 0.993000\n",
      "training with batch_size: 32, learning_rate: 0.000080\n",
      "best_val_acc: 0.990000\n",
      "training with batch_size: 64, learning_rate: 0.001000\n",
      "best_val_acc: 0.994000\n",
      "training with batch_size: 64, learning_rate: 0.005000\n",
      "best_val_acc: 0.994000\n",
      "training with batch_size: 64, learning_rate: 0.008000\n",
      "best_val_acc: 0.975000\n",
      "training with batch_size: 64, learning_rate: 0.000100\n",
      "best_val_acc: 0.988000\n",
      "training with batch_size: 64, learning_rate: 0.000500\n",
      "best_val_acc: 0.994000\n",
      "training with batch_size: 64, learning_rate: 0.000800\n",
      "best_val_acc: 0.997000\n",
      "training with batch_size: 64, learning_rate: 0.000010\n",
      "best_val_acc: 0.769000\n",
      "training with batch_size: 64, learning_rate: 0.000050\n",
      "best_val_acc: 0.971000\n",
      "training with batch_size: 64, learning_rate: 0.000080\n",
      "best_val_acc: 0.991000\n",
      "training with batch_size: 128, learning_rate: 0.001000\n",
      "best_val_acc: 0.996000\n",
      "training with batch_size: 128, learning_rate: 0.005000\n",
      "best_val_acc: 0.991000\n",
      "training with batch_size: 128, learning_rate: 0.008000\n",
      "best_val_acc: 0.990000\n",
      "training with batch_size: 128, learning_rate: 0.000100\n",
      "best_val_acc: 0.974000\n",
      "training with batch_size: 128, learning_rate: 0.000500\n",
      "best_val_acc: 0.995000\n",
      "training with batch_size: 128, learning_rate: 0.000800\n",
      "best_val_acc: 0.995000\n",
      "training with batch_size: 128, learning_rate: 0.000010\n",
      "best_val_acc: 0.749000\n",
      "training with batch_size: 128, learning_rate: 0.000050\n",
      "best_val_acc: 0.955000\n",
      "training with batch_size: 128, learning_rate: 0.000080\n",
      "best_val_acc: 0.968000\n",
      "training with batch_size: 256, learning_rate: 0.001000\n",
      "best_val_acc: 0.998000\n",
      "training with batch_size: 256, learning_rate: 0.005000\n",
      "best_val_acc: 0.985000\n",
      "training with batch_size: 256, learning_rate: 0.008000\n",
      "best_val_acc: 0.983000\n",
      "training with batch_size: 256, learning_rate: 0.000100\n",
      "best_val_acc: 0.975000\n",
      "training with batch_size: 256, learning_rate: 0.000500\n",
      "best_val_acc: 0.997000\n",
      "training with batch_size: 256, learning_rate: 0.000800\n",
      "best_val_acc: 0.993000\n",
      "training with batch_size: 256, learning_rate: 0.000010\n",
      "best_val_acc: 0.755000\n",
      "training with batch_size: 256, learning_rate: 0.000050\n",
      "best_val_acc: 0.930000\n",
      "training with batch_size: 256, learning_rate: 0.000080\n",
      "best_val_acc: 0.958000\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 10\n",
    "batch_size = 128\n",
    "val_batch_size = 1000\n",
    "learning_rate = 1e-4\n",
    "\n",
    "num_test = test_images.shape[0]\n",
    "\n",
    "def model(x):\n",
    "    x = tf.reshape(x, [-1, 28, 28, 1])\n",
    "    x = tf.layers.conv2d(x, filters=32, kernel_size=[3, 3], strides=[1, 1], padding='same', activation=tf.nn.relu)\n",
    "    x = tf.layers.batch_normalization(x, training=True)\n",
    "    x = tf.layers.conv2d(x, filters=32, kernel_size=[3, 3], strides=[1, 1], padding='same', activation=tf.nn.relu)\n",
    "    x = tf.layers.batch_normalization(x, training=True)\n",
    "    x = tf.layers.conv2d(x, filters=32, kernel_size=[3, 3], strides=[1, 1], padding='same', activation=tf.nn.relu)\n",
    "    x = tf.layers.batch_normalization(x, training=True)\n",
    "    x = tf.layers.conv2d(x, filters=32, kernel_size=[3, 3], strides=[1, 1], padding='same', activation=tf.nn.relu)\n",
    "    x = tf.layers.batch_normalization(x, training=True)\n",
    "    x = tf.layers.conv2d(x, filters=32, kernel_size=[3, 3], strides=[1, 1], padding='same', activation=tf.nn.relu)\n",
    "    x = tf.layers.batch_normalization(x, training=True)\n",
    "    x = tf.layers.conv2d(x, filters=32, kernel_size=[3, 3], strides=[1, 1], padding='same', activation=tf.nn.relu)\n",
    "    x = tf.layers.batch_normalization(x, training=True)\n",
    "    \n",
    "    x = tf.layers.conv2d(x, filters=64, kernel_size=[3, 3], strides=[2, 2], padding='same', activation=tf.nn.relu)\n",
    "    x = tf.layers.batch_normalization(x, training=True)    \n",
    "    \n",
    "    x = tf.layers.conv2d(x, filters=64, kernel_size=[3, 3], strides=[1, 1], padding='same', activation=tf.nn.relu)\n",
    "    x = tf.layers.batch_normalization(x, training=True)\n",
    "    x = tf.layers.conv2d(x, filters=64, kernel_size=[3, 3], strides=[1, 1], padding='same', activation=tf.nn.relu)\n",
    "    x = tf.layers.batch_normalization(x, training=True)\n",
    "    x = tf.layers.conv2d(x, filters=64, kernel_size=[3, 3], strides=[1, 1], padding='same', activation=tf.nn.relu)\n",
    "    x = tf.layers.batch_normalization(x, training=True)\n",
    "    x = tf.layers.conv2d(x, filters=64, kernel_size=[3, 3], strides=[1, 1], padding='same', activation=tf.nn.relu)\n",
    "    x = tf.layers.batch_normalization(x, training=True)\n",
    "    x = tf.layers.conv2d(x, filters=64, kernel_size=[3, 3], strides=[1, 1], padding='same', activation=tf.nn.relu)\n",
    "    x = tf.layers.batch_normalization(x, training=True)\n",
    "    \n",
    "    x = tf.layers.conv2d(x, filters=128, kernel_size=[3, 3], strides=[2, 2], padding='same', activation=tf.nn.relu)\n",
    "    x = tf.layers.batch_normalization(x, training=True)\n",
    "    \n",
    "    x = tf.layers.conv2d(x, filters=128, kernel_size=[3, 3], strides=[1, 1], padding='same', activation=tf.nn.relu)\n",
    "    x = tf.layers.batch_normalization(x, training=True)\n",
    "    x = tf.layers.conv2d(x, filters=128, kernel_size=[3, 3], strides=[1, 1], padding='same', activation=tf.nn.relu)\n",
    "    x = tf.layers.batch_normalization(x, training=True) \n",
    "    x = tf.layers.conv2d(x, filters=128, kernel_size=[3, 3], strides=[1, 1], padding='same', activation=tf.nn.relu)\n",
    "    x = tf.layers.batch_normalization(x, training=True)\n",
    "    x = tf.layers.conv2d(x, filters=128, kernel_size=[3, 3], strides=[1, 1], padding='same', activation=tf.nn.relu)\n",
    "    x = tf.layers.batch_normalization(x, training=True) \n",
    "    x = tf.layers.conv2d(x, filters=128, kernel_size=[3, 3], strides=[1, 1], padding='same', activation=tf.nn.relu)\n",
    "    x = tf.layers.batch_normalization(x, training=True) \n",
    "\n",
    "    x = tf.contrib.layers.flatten(x)\n",
    "    x = tf.layers.dense(inputs=x, units=4*4*64, activation=tf.nn.relu)\n",
    "    x = tf.layers.dense(inputs=x, units=4*4*64, activation=tf.nn.relu)\n",
    "    logits = tf.layers.dense(x, 10)\n",
    "    return logits\n",
    "\n",
    "tf.reset_default_graph()\n",
    "X = tf.placeholder(tf.float32, [None, 784])\n",
    "y = tf.placeholder(tf.int64, [None])\n",
    "\n",
    "logits = model(X)\n",
    "\n",
    "loss = tf.reduce_mean(tf.losses.softmax_cross_entropy(tf.one_hot(y, 10), logits=logits))\n",
    "\n",
    "predictions = tf.equal(tf.argmax(logits, 1), y)\n",
    "acc = tf.reduce_mean(tf.cast(predictions, tf.float32))\n",
    "\n",
    "train_indicies = np.arange(num_train)\n",
    "np.random.shuffle(train_indicies)\n",
    "\n",
    "val_indicies = np.arange(num_val)\n",
    "test_indicies = np.arange(num_test)\n",
    "\n",
    "test_predictions = tf.argmax(logits, 1)\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "def run_model(sess, train_step, batch_size, learning_rate, show_every):\n",
    "    print('training with batch_size: %d, learning_rate: %f'%(batch_size, learning_rate))\n",
    "    max_val_acc = .0\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        #print('epoch %d:' % epoch)\n",
    "        for iter_i in range(num_train//batch_size):  \n",
    "            start_idx = (iter_i*batch_size)%num_train\n",
    "            idx = train_indicies[start_idx:start_idx+batch_size]\n",
    "            \n",
    "            _loss, _acc, _ = sess.run([loss, acc, train_step], feed_dict={\n",
    "                X: train_image[idx, :], y: train_label[idx]\n",
    "            })\n",
    "            #if iter_i % show_every == 0:\n",
    "                #print('iter: %d, loss: %f, acc: %f' % (iter_i, _loss, _acc))\n",
    "\n",
    "        #total_correct = []\n",
    "        for iter_i in range(num_val//val_batch_size):\n",
    "            start_idx = (iter_i * val_batch_size) % num_val\n",
    "            idx = val_indicies[start_idx:start_idx+val_batch_size]\n",
    "            val_acc = sess.run(acc, feed_dict={\n",
    "                X:val_image[idx, :], y: val_label[idx]\n",
    "            })\n",
    "            if (val_acc > max_val_acc):\n",
    "                max_val_acc = val_acc\n",
    "            #total_correct = total_correct + val_correct_predictions\n",
    "            #print('val set acc: %f'% val_acc)\n",
    "            \n",
    "#         if (val_acc >= 0.995):\n",
    "#             print('bingo!')\n",
    "#             total_test_predictions = []\n",
    "#             test_batch_size = 7000\n",
    "#             for test_iter in range(num_test//test_batch_size):\n",
    "#                 start_test_idx = (test_iter * test_batch_size)%num_test\n",
    "#                 test_idx = test_indicies[start_test_idx:start_test_idx+test_batch_size]\n",
    "#                 _test_predictions = sess.run(test_predictions, feed_dict={X:test_images[test_idx, :]})\n",
    "#                 total_test_predictions = np.concatenate((total_test_predictions,_test_predictions))\n",
    "            \n",
    "#             result = total_test_predictions\n",
    "#             with open('submission_%d.csv'%(epoch), 'w', newline='') as csvfile:\n",
    "#                 datawriter = csv.writer(csvfile, delimiter=',')\n",
    "#                 datawriter.writerow(['ImageId', 'Label'])\n",
    "#                 for i, predict_label in enumerate(result):\n",
    "#                     datawriter.writerow([i+1, predict_label.astype(np.uint8)])\n",
    "        \n",
    "#         save_path = \"../ckpt/epoch%d/model.ckpt\"%epoch\n",
    "#         saver.save(sess, save_path)\n",
    "    return max_val_acc\n",
    "#     print('predicting:')\n",
    "#     total_test_predictions = []\n",
    "#     test_batch_size = 7000\n",
    "#     for test_iter in range(num_test//test_batch_size):\n",
    "#         start_test_idx = (test_iter * test_batch_size)%num_test\n",
    "#         test_idx = test_indicies[start_test_idx:start_test_idx+test_batch_size]\n",
    "#         _test_predictions = sess.run(test_predictions, feed_dict={X:test_images[test_idx, :]})\n",
    "#         total_test_predictions = np.concatenate((total_test_predictions,_test_predictions))\n",
    "#     return total_test_predictions\n",
    "\n",
    "batch_size_arr = [16, 32, 64, 128, 256]\n",
    "learning_rate_arr = [1e-3, 5e-3, 8e-3, 1e-4, 5e-4, 8e-4, 1e-5, 5e-5, 8e-5]\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    for batch_size in batch_size_arr:\n",
    "        for learning_rate in learning_rate_arr:\n",
    "            train_step = tf.train.AdamOptimizer(learning_rate).minimize(loss)\n",
    "            \n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            val_acc = run_model(sess, train_step, batch_size, learning_rate, show_every=100)\n",
    "            print('best_val_acc: %f'%val_acc)\n",
    "    \n",
    "print('done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('submission.csv', 'w', newline='') as csvfile:\n",
    "    datawriter = csv.writer(csvfile, delimiter=',')\n",
    "    datawriter.writerow(['ImageId', 'Label'])\n",
    "    for i, predict_label in enumerate(result):\n",
    "        datawriter.writerow([i+1, predict_label.astype(np.uint8)])\n",
    "\n",
    "print('done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for i in range(7000, 7010):\n",
    "    display_img(test_images, i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "1e-3 256\n",
    "5e-3 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training with batch_size: 8, learning_rate: 0.001000\n",
    "best_val_acc: 0.989000\n",
    "training with batch_size: 8, learning_rate: 0.005000\n",
    "best_val_acc: 0.978000\n",
    "training with batch_size: 8, learning_rate: 0.008000\n",
    "best_val_acc: 0.116000\n",
    "training with batch_size: 8, learning_rate: 0.000100\n",
    "best_val_acc: 0.992000\n",
    "training with batch_size: 8, learning_rate: 0.000500\n",
    "best_val_acc: 0.990000\n",
    "training with batch_size: 8, learning_rate: 0.000800\n",
    "best_val_acc: 0.992000\n",
    "training with batch_size: 8, learning_rate: 0.000010\n",
    "best_val_acc: 0.966000\n",
    "training with batch_size: 8, learning_rate: 0.000050\n",
    "best_val_acc: 0.992000\n",
    "training with batch_size: 8, learning_rate: 0.000080\n",
    "best_val_acc: 0.993000\n",
    "training with batch_size: 16, learning_rate: 0.001000\n",
    "best_val_acc: 0.994000\n",
    "training with batch_size: 16, learning_rate: 0.005000\n",
    "best_val_acc: 0.991000\n",
    "training with batch_size: 16, learning_rate: 0.008000\n",
    "best_val_acc: 0.988000\n",
    "training with batch_size: 16, learning_rate: 0.000100\n",
    "best_val_acc: 0.995000\n",
    "training with batch_size: 16, learning_rate: 0.000500\n",
    "best_val_acc: 0.991000\n",
    "training with batch_size: 16, learning_rate: 0.000080\n",
    "best_val_acc: 0.997000\n",
    "training with batch_size: 32, learning_rate: 0.001000\n",
    "best_val_acc: 0.992000\n",
    "training with batch_size: 32, learning_rate: 0.005000\n",
    "best_val_acc: 0.992000\n",
    "training with batch_size: 32, learning_rate: 0.008000\n",
    "best_val_acc: 0.990000\n",
    "training with batch_size: 32, learning_rate: 0.000100\n",
    "best_val_acc: 0.992000\n",
    "training with batch_size: 32, learning_rate: 0.000500\n",
    "best_val_acc: 0.995000\n",
    "training with batch_size: 32, learning_rate: 0.000800\n",
    "best_val_acc: 0.995000\n",
    "training with batch_size: 32, learning_rate: 0.000010\n",
    "best_val_acc: 0.880000\n",
    "training with batch_size: 32, learning_rate: 0.000050\n",
    "best_val_acc: 0.993000\n",
    "training with batch_size: 32, learning_rate: 0.000080\n",
    "best_val_acc: 0.990000\n",
    "training with batch_size: 64, learning_rate: 0.001000\n",
    "best_val_acc: 0.994000\n",
    "training with batch_size: 64, learning_rate: 0.005000\n",
    "best_val_acc: 0.994000\n",
    "training with batch_size: 64, learning_rate: 0.008000\n",
    "best_val_acc: 0.975000\n",
    "training with batch_size: 64, learning_rate: 0.000100\n",
    "best_val_acc: 0.988000\n",
    "training with batch_size: 64, learning_rate: 0.000500\n",
    "best_val_acc: 0.994000\n",
    "training with batch_size: 64, learning_rate: 0.000800\n",
    "best_val_acc: 0.997000\n",
    "training with batch_size: 64, learning_rate: 0.000010\n",
    "best_val_acc: 0.769000\n",
    "training with batch_size: 64, learning_rate: 0.000050\n",
    "best_val_acc: 0.971000\n",
    "training with batch_size: 64, learning_rate: 0.000080\n",
    "best_val_acc: 0.991000\n",
    "training with batch_size: 128, learning_rate: 0.001000\n",
    "best_val_acc: 0.996000\n",
    "training with batch_size: 128, learning_rate: 0.005000\n",
    "best_val_acc: 0.991000\n",
    "training with batch_size: 128, learning_rate: 0.008000\n",
    "best_val_acc: 0.990000\n",
    "training with batch_size: 128, learning_rate: 0.000100\n",
    "best_val_acc: 0.974000\n",
    "training with batch_size: 128, learning_rate: 0.000500\n",
    "best_val_acc: 0.995000\n",
    "training with batch_size: 128, learning_rate: 0.000800\n",
    "best_val_acc: 0.995000\n",
    "training with batch_size: 128, learning_rate: 0.000010\n",
    "best_val_acc: 0.749000\n",
    "training with batch_size: 128, learning_rate: 0.000050\n",
    "best_val_acc: 0.955000\n",
    "training with batch_size: 128, learning_rate: 0.000080\n",
    "best_val_acc: 0.968000\n",
    "training with batch_size: 256, learning_rate: 0.001000\n",
    "best_val_acc: 0.998000\n",
    "training with batch_size: 256, learning_rate: 0.005000\n",
    "best_val_acc: 0.985000\n",
    "training with batch_size: 256, learning_rate: 0.008000\n",
    "best_val_acc: 0.983000\n",
    "training with batch_size: 256, learning_rate: 0.000100\n",
    "best_val_acc: 0.975000\n",
    "training with batch_size: 256, learning_rate: 0.000500\n",
    "best_val_acc: 0.997000\n",
    "training with batch_size: 256, learning_rate: 0.000800\n",
    "best_val_acc: 0.993000\n",
    "training with batch_size: 256, learning_rate: 0.000010\n",
    "best_val_acc: 0.755000\n",
    "training with batch_size: 256, learning_rate: 0.000050\n",
    "best_val_acc: 0.930000\n",
    "training with batch_size: 256, learning_rate: 0.000080\n",
    "best_val_acc: 0.958000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
